{
  "paper": {
    "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
    "title": "Deep Learning for Generic Object Detection: A Survey",
    "authors": [
      "Li Liu",
      "Wanli Ouyang",
      "Xiaogang Wang",
      "Paul Fieguth",
      "Jie Chen",
      "Xinwang Liu",
      "Matti Pietik\u00e4inen"
    ],
    "year": 2019,
    "venue": "International Journal of Computer Vision",
    "source": "openalex",
    "abstract": "Abstract Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.",
    "pdf_path": "data/papers_cv/doi_https___doi.org_10.1007_s11263-019-01247-4.pdf",
    "url": "https://link.springer.com/content/pdf/10.1007/s11263-019-01247-4.pdf",
    "doi": "https://doi.org/10.1007/s11263-019-01247-4",
    "arxiv_id": null,
    "openalex_id": "https://openalex.org/W2988916019",
    "citation_count": 2673,
    "is_open_access": true,
    "sync_timestamp": "2026-02-19 20:15:05.635003+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S1",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "body",
      "text": "International Journal of Computer Vision https://doi.org/10.1007/s11263-019-01247-4 Deep Learning for Generic Object Detection: A Survey Li Liu 1,2 \u00b7 Wanli Ouyang 3 \u00b7 Xiaogang Wang 4 \u00b7 Paul Fieguth 5 \u00b7 Jie Chen 2 \u00b7 Xinwang Liu 1 \u00b7 Matti Pietik\u00e4inen 2 Received: 6 September 2018 / Accepted: 26 September 2019 \u00a9 The Author(s) 2019",
      "page_hint": null,
      "token_count": 54,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S2",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of prede\ufb01ned categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the \ufb01eld of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this \ufb01eld brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S3",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "\ufb01nish the survey by identifying promising directions for future research. Keywords Object detection \u00b7 Deep learning \u00b7 Convolutional neural networks \u00b7 Object recognition",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S4",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "introduction",
      "text": "As a longstanding, fundamental and challenging problem in computer vision, object detection (illustrated in Fig. 1) has been an active area of research for several decades (Fis- Communicated by Bernt Schiele. B Li Liu li.liu@oulu.\ufb01 Wanli Ouyang wanli.ouyang@sydney.edu.au Xiaogang Wang xgwang@ee.cuhk.edu.hk Paul Fieguth p\ufb01eguth@uwaterloo.ca Jie Chen jie.chen@oulu.\ufb01 Xinwang Liu xinwangliu@nudt.edu.cn Matti Pietik\u00e4inen matti.pietikainen@oulu.\ufb01 1 National University of Defense Technology, Changsha, China 2 University of Oulu, Oulu, Finland 3 University of Sydney, Camperdown, Australia 4 Chinese University of Hong Kong, Sha Tin, China 5 University of Waterloo, Waterloo, Canada chler and Elschlager 1973). The goal of object detection is to determine whether there are any instances of objects from given categories (such as humans, cars, bicycles, dogs or cats) in an",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S5",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "introduction",
      "text": "image and, if present, to return the spatial loca- tion and extent of each object instance (e.g., via a bounding box Everingham et al.2010; Russakovsky et al. 2015). As the cornerstone of image understanding and computer vision, object detection forms the basis for solving complex or high level vision tasks such as segmentation, scene understand- ing, object tracking, image captioning, event detection, and activity recognition. Object detection supports a wide range of applications, including robot vision, consumer electronics, security, autonomous driving, human computer interaction, content based image retrieval, intelligent video surveillance, and augmented reality. Recently, deep learning techniques (Hinton and Salakhut- dinov 2006; LeCun et al. 2015) have emerged as powerful",
      "page_hint": null,
      "token_count": 111,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S6",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "from data. In particular, these techniques have provided major improvements in object detection, as illustrated in Fig. 3. As illustrated in Fig. 2, object detection can be grouped into one of two types (Grauman and Leibe 2011; Zhang et al. 2013): detection of speci\ufb01c instances versus the detection of broad categories. The \ufb01rst type aims to detect instances of a particular object (such as Donald Trump\u2019s face, the Eiffel Tower, or a neighbor\u2019s dog), essentially a matching problem. 123 International Journal of Computer Vision Fig. 1 Most frequent keywords in ICCV and CVPR conference papers from 2016 to 2018. The size of each word is proportional to the fre- quency of that keyword. We can see that object detection has",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S7",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "received signi\ufb01cant attention in recent years Fig. 2 Object detection includes localizing instances of a particular object (top), as well as generalizing to detecting object categories in general (bottom). This survey focuses on recent advances for the latter problem of generic object detection The goal of the second type is to detect (usually previ- ously unseen) instances of some prede\ufb01ned object categories (for example humans, cars, bicycles, and dogs). Historically, much of the effort in the \ufb01eld of object detection has focused on the detection of a single category (typically faces and pedestrians) or a few speci\ufb01c categories. In contrast, over the past several years, the research community has started moving towards the more challenging goal of building gen- eral",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S8",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "purpose object detection systems where the breadth of object detection ability rivals that of humans. Krizhevsky et al. ( 2012a) proposed a Deep Convo- lutional Neural Network (DCNN) called AlexNet which achieved record breaking image classi\ufb01cation accuracy in the Large Scale Visual Recognition Challenge (ILSVRC) (Rus- sakovsky et al.2015). Since that time, the research focus in most aspects of computer vision has been speci\ufb01cally on deep learning methods, indeed including the domain of generic object detection (Girshick et al.2014;H ee ta l . 2014;G i r - shick 2015; Sermanet et al. 2014; Ren et al. 2017). Although tremendous progress has been achieved, illustrated in Fig. 3, we are unaware of comprehensive surveys of this subject (a) (b) Fig. 3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S9",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "An overview of recent object detection performance: we can observe a signi\ufb01cant improvement in performance (measured as mean average precision) since the arrival of deep learning in 2012.a Detection",
      "page_hint": null,
      "token_count": 29,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S10",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "top object detection competition results in ILSVRC2013-2017 (results in both panels use only the provided training data) over the past 5 years. Given the exceptionally rapid rate of progress, this article attempts to track recent advances and summarize their achievements in order to gain a clearer pic- ture of the current panorama in generic object detection. 1.1 Comparison with Previous Reviews Many notable object detection surveys have been published, as summarized in Table 1. These include many excellent sur- veys on the problem of speci\ufb01c object detection, such as pedestrian detection (Enzweiler and Gavrila 2009; Geron- imo et al. 2010; Dollar et al. 2012), face detection (Yang et al. 2002; Zafeiriou et al. 2015), vehicle detection (Sun et al. 2006)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S11",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "and text detection (Ye and Doermann 2015). There are comparatively few recent surveys focusing directly on the problem of generic object detection, except for the work by Zhang et al. (2013) who conducted a survey on the topic of object class detection. However, the research reviewed in Grauman and Leibe ( 2011), Andreopoulos and Tsotsos (2013) and Zhang et al. ( 2013) is mostly pre-2012, and there- fore prior to the recent striking success and dominance of deep learning and related methods. Deep learning allows computational models to learn fantastically complex, subtle, and abstract representations, driving signi\ufb01cant progress in a broad range of problems such as visual recognition, object detection, speech recognition, natural language processing, medical image analysis, drug discovery",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S12",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "and genomics. Among different types of deep neu- ral networks, DCNNs (LeCun et al. 1998, 2015; Krizhevsky et al. 2012a) have brought about breakthroughs in processing images, video, speech and audio. To be sure, there have been many published surveys on deep learning, including that of Bengio et al. (2013), LeCun et al. ( 2015), Litjens et al. ( 2017), Gu et al. ( 2018), and more recently in tutorials at ICCV and CVPR. In contrast, although many deep learning based methods have been proposed for object detection, we are unaware of 123 International Journal of Computer Vision Table 1 Summary of related object detection surveys since 2000 No. Survey title References Year V enue Content 1 Monocular pedestrian detection:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S13",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "survey and experiments Enzweiler and Gavrila ( 2009) 2009 PAMI An evaluation of three pedestrian detectors 2 Survey of pedestrian detection for advanced driver assistance systems Geronimo et al. ( 2010) 2010 PAMI A survey of pedestrian detection for advanced driver assistance systems 3 Pedestrian detection: an evaluation of the state of the art Dollar et al. ( 2012) 2012 PAMI A thorough and detailed evaluation of detectors in monocular images 4 Detecting faces in images: a survey Yang et al. ( 2002) 2002 PAMI First survey of face detection from a single image 5 A survey on face detection in the wild: past, present and future Zafeiriou et al. ( 2015) 2015 CVIU A survey of face detection in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S14",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "the wild since 2000 6 On road vehicle detection: a review Sun et al. ( 2006) 2006 PAMI A review of vision based on-road vehicle detection systems 7 Text detection and recognition in imagery: a survey Ye and Doermann ( 2015) 2015 PAMI A survey of text detection and recognition in color imagery 8 Toward category level object recognition Ponce et al. ( 2007) 2007 Book Representative papers on object categorization, detection, and segmentation 9 The evolution of object categorization and the challenge of image abstraction Dickinson et al. ( 2009) 2009 Book A trace of the evolution of object categorization over 4 decades 10 Context based object categorization: a critical survey Galleguillos and Belongie ( 2010) 2010 CVIU A",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S15",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "results",
      "text": "review of contextual information for object categorization 11 50 years of object recognition: directions forward Andreopoulos and Tsotsos ( 2013) 2013 CVIU A review of the evolution of object recognition systems over 5 decades 12 Visual object recognition Grauman and Leibe ( 2011) 2011 Tutorial Instance and category object recognition techniques 13 Object class detection: a survey Zhang et al. ( 2013) 2013 ACM CS Survey of generic object detection",
      "page_hint": null,
      "token_count": 70,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S16",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "14 Feature representation for statistical learning based object detection: a review Li et al. ( 2015b) 2015 PR Feature representation methods in statistical learning based object detection, including handcrafted and deep learning based features 15 Salient object detection: a survey Borji et al. ( 2014) 2014 arXiv A survey for salient object detection 16 Representation learning: a review and new perspectives Bengio et al. ( 2013) 2013 PAMI Unsupervised feature learning and deep learning, probabilistic models, autoencoders, manifold learning, and deep networks 17 Deep learning LeCun et al. ( 2015) 2015 Nature An introduction to deep learning and applications 18 A survey on deep learning in medical image analysis Litjens et al. ( 2017) 2017 MIA A survey of deep",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S17",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "learning for image classi\ufb01cation, object detection, segmentation and registration in medical image analysis 19 Recent advances in convolutional neural networks Gu et al. ( 2018) 2017 PR A broad survey of the recent advances in CNN and its applications in computer vision, speech and natural language processing 20 Tutorial: tools for ef\ufb01cient object detection \u2212 2015 ICCV15 A short course for object detection only covering recent milestones 123 International Journal of Computer Vision Table 1 continued No. Survey title References Year V enue Content 21 Tutorial: deep learning for objects and scenes \u2212 2017 CVPR17 A high level summary of recent work on deep learning for visual recognition of objects and scenes 22 Tutorial: instance level recognition \u2212 2017 ICCV17",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S18",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "A short course of recent advances on instance level recognition, including object detection, instance segmentation and human pose prediction 23 Tutorial: visual recognition and beyond \u2212 2018 CVPR18 A tutorial on methods and principles behind image classi\ufb01cation, object detection, instance segmentation, and semantic segmentation 24 Deep learning for generic object detection Ours 2019 VISI A comprehensive survey of deep learning for generic object detection any comprehensive recent survey. A thorough review and summary of existing work is essential for further progress in object detection, particularly for researchers wishing to enter the \ufb01eld. Since our focus is ongeneric object detection, the extensive work on DCNNs for speci\ufb01c object detection, such as face detection (Li et al. 2015a; Zhang et al. 2016a;H",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S19",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "ue ta l . 2017), pedestrian detection (Zhang et al. 2016b; Hosang et al. 2015), vehicle detection (Zhou et al. 2016b) and traf\ufb01c sign detection (Zhu et al. 2016b) will not be considered. 1.2 Scope The number of papers on generic object detection based on deep learning is breathtaking. There are so many, in fact, that compiling any comprehensive review of the state of the art is beyond the scope of any reasonable length paper. As a result, it is necessary to establish selection criteria, in such a way that we have limited our focus to top journal and conference papers. Due to these limitations, we sincerely apologize to those authors whose works are not included in this paper. For",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S20",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "surveys of work on related topics, readers are referred to the articles in Table 1. This survey focuses on major progress of the last 5 years, and we restrict our attention to still pictures, leaving the important subject of video object detection as a topic for separate consideration in the future. The main goal of this paper is to offer a comprehensive survey of deep learning based generic object detection tech- niques, and to present some degree of taxonomy, a high level perspective and organization, primarily on the basis of popular datasets, evaluation metrics, context modeling, and detection proposal methods. The intention is that our categorization be helpful for readers to have an accessi- ble understanding of similarities and differences",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S21",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "between a wide variety of strategies. The proposed taxonomy gives researchers a framework to understand current research and to identify open challenges for future research. The remainder of this paper is organized as follows. Related background and the progress made during the last 2 decades are summarized in Sect. 2. A brief introduction to deep learning is given in Sect. 3. Popular datasets and evaluation criteria are summarized in Sect. 4. We describe the milestone object detection frameworks in Sect. 5.F r o m Sects. 6 to 9, fundamental sub-problems and the relevant issues involved in designing object detectors are discussed. Finally, in Sect.10, we conclude the paper with an overall discussion of object detection, state-of-the- art performance, and future",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S22",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "research directions. 2 Generic Object Detection 2.1 The Problem Generic object detection , also called generic object category detection, object class detection, or object category detec- tion (Zhang et al. 2013), is de\ufb01ned as follows. Given an image, determine whether or not there are instances of objects from prede\ufb01ned categories (usuallymany categories, e.g., 200 categories in the ILSVRC object detection challenge) and, if present, to return the spatial location and extent of each instance. A greater emphasis is placed on detecting a broad range of natural categories, as opposed to speci\ufb01c object category detection where only a narrower prede\ufb01ned category of interest (e.g., faces, pedestrians, or cars) may be present. Although thousands of objects occupy the visual world in which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S23",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "we live, currently the research community is primarily interested in the localization of highly structured objects (e.g., cars, faces, bicycles and airplanes) and artic- 123 International Journal of Computer Vision (a) (b) (d)(c) Fig. 4 Recognition problems related to generic object detection: a image level object classi\ufb01cation, b bounding box level generic object detection, c pixel-wise semantic segmentation, d instance level semantic segmentation ulated objects (e.g., humans, cows and horses) rather than unstructured scenes (such as sky, grass and cloud). The spatial location and extent of an object can be de\ufb01ned coarsely using a bounding box (an axis-aligned rectangle tightly bounding the object) (Everingham et al. 2010;R u s - sakovsky et al. 2015), a precise pixelwise segmentation mask (Zhang",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S24",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "et al. 2013), or a closed boundary (Lin et al. 2014; Russell et al. 2008), as illustrated in Fig. 4. To the best of our knowledge, for the evaluation of generic object detec- tion algorithms, it is bounding boxes which are most widely used in the current literature (Everingham et al. 2010;R u s - sakovsky et al. 2015), and therefore this is also the approach we adopt in this survey. However, as the research community moves towards deeper scene understanding (from image level object classi\ufb01cation to single object localization, to generic object detection, and to pixelwise object segmentation), it is anticipated that future challenges will be at the pixel level (Lin et al. 2014). There are many problems closely",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S25",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "related to that of generic object detection 1. The goal of object classi\ufb01cation or object categorization (Fig. 4a) is to assess the presence of objects from a given set of object classes in an image; i.e., assigning one or more object class labels to a given image, determin- ing the presence without the need of location. The additional requirement to locate the instances in an image makes detec- tion a more challenging task than classi\ufb01cation. Theobject recognition problem denotes the more general problem of identifying/localizing all the objects present in an image, subsuming the problems of object detection and classi\ufb01ca- tion (Everingham et al. 2010; Russakovsky et al. 2015; Opelt 1 To the best of our knowledge, there is no",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S26",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "universal agreement in the literature on the de\ufb01nitions of various vision subtasks. Terms such as detection, localization, recognition, classi\ufb01cation, categorization, veri- \ufb01cation, identi\ufb01cation, annotation, labeling, and understanding are often differently de\ufb01ned (Andreopoulos and Tsotsos2013). Fig. 5 Taxonomy of challenges in generic object detection et al. 2006; Andreopoulos and Tsotsos 2013). Generic object detection is closely related to semantic image segmentation (Fig. 4c), which aims to assign each pixel in an image to a semantic class label. Object instance segmentation (Fig. 4d) aims to distinguish different instances of the same object class, as opposed to semantic segmentation which does not. 2.2 Main Challenges The ideal of generic object detection is to develop a general- purpose algorithm that achieves two competing goals",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S27",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "ofhigh quality/accuracy and high ef\ufb01ciency (Fig. 5). As illustrated in Fig. 6, high quality detection must accurately local- ize and recognize objects in images or video frames, such that the large variety of object categories in the real world can be distinguished (i.e., high distinctiveness), and that object instances from the same category, subject to intra- class appearance variations, can be localized and recognized (i.e., high robustness). High ef\ufb01ciency requires that the entire detection task runs in real time with acceptable memory and storage demands. 2.2.1 Accuracy Related Challenges Challenges in detection accuracy stem from (1) the vast range of intra-class variations and (2) the huge number of object categories. Intra-class variations can be divided into two types: intrin- sic",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S28",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "factors and imaging conditions. In terms of intrinsic factors, each object category can have many different object instances, possibly varying in one or more of color, tex- ture, material, shape, and size, such as the \u201cchair\u201d category shown in Fig.6i. Even in a more narrowly de\ufb01ned class, such as human or horse, object instances can appear in different poses, subject to nonrigid deformations or with the addition of clothing. 123 International Journal of Computer Vision (a) (e) (f) (g) (h) (i) (j) (b) (c) (d) Fig. 6 Changes in appearance of the same class with variations in imag- ing conditions ( a\u2013h). There is an astonishing variation in what is meant to be a single object class ( i). In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S29",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "contrast, the four images in j appear very similar, but in fact are from four different object classes. Most images are from ImageNet (Russakovsky et al.2015) and MS COCO (Lin et al. 2014) Imaging condition variations are caused by the dra- matic impacts unconstrained environments can have on object appearance, such as lighting (dawn, day, dusk, indoors), physical location, weather conditions, cameras,",
      "page_hint": null,
      "token_count": 62,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S30",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "tances. All of these conditions produce signi\ufb01cant variations in object appearance, such as illumination, pose, scale, occlusion, clutter, shading, blur and motion, with examples illustrated in Fig.6a\u2013h. Further challenges may be added by digitization artifacts, noise corruption, poor resolution, and \ufb01ltering distortions. In addition to intraclass variations, the large number of object categories, on the order of 10 4\u2013105, demands great dis- crimination power from the detector to distinguish between subtly different interclass variations, as illustrated in Fig. 6j. In practice, current detectors focus mainly on structured object categories, such as the 20, 200 and 91 object classes in PASCAL VOC (Everingham et al. 2010), ILSVRC (Rus- sakovsky et al. 2015) and MS COCO (Lin et al. 2014) respectively. Clearly,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S31",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "the number of object categories under consideration in existing benchmark datasets is much smaller than can be recognized by humans. 2.2.2 Ef\ufb01ciency and Scalability Related Challenges The prevalence of social media networks and mobile/wearable devices has led to increasing demands for analyzing visual data. However, mobile/wearable devices have limited com- putational capabilities and storage space, making ef\ufb01cient object detection critical. The ef\ufb01ciency challenges stem from the need to localize and recognize, computational complexity growing with the (possibly large) number of object categories, and with the (possibly very large) number of locations and scales within a single image, such as the examples in Fig. 6c, d. A further challenge is that of scalability: A detector should be able to handle previously",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S32",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "unseen objects, unknown situ- Fig. 7 Milestones of object detection and recognition, including feature representations (Csurka et al. 2004; Dalal and Triggs 2005;H ee ta l . 2016;K r i z h e v s k ye ta l .2012a; Lazebnik et al. 2006;L o w e1999, 2004; Perronnin et al. 2010; Simonyan and Zisserman 2015; Sivic and Zisser- man 2003; Szegedy et al. 2015; Viola and Jones 2001;W a n ge ta l .2009), detection frameworks (Felzenszwalb et al. 2010b; Girshick et al. 2014; Sermanet et al. 2014; Uijlings et al. 2013; Viola and Jones 2001), and datasets (Everingham et al. 2010; Lin et al. 2014; Russakovsky et al. 2015). The time period up to 2012 is dominated by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S33",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "handcrafted fea- tures, a transition took place in 2012 with the development of DCNNs for image classi\ufb01cation by Krizhevsky et al. (2012a), with methods after 2012 dominated by related deep networks. Most of the listed methods are highly cited and won a major ICCV or CVPR prize. See Sect.2.3 for details 123 International Journal of Computer Vision ations, and high data rates. As the number of images and the number of categories continue to grow, it may become impossible to annotate them manually, forcing a reliance on weakly supervised strategies. 2.3 Progress in the Past 2 Decades Early research on object recognition was based on template matching techniques and simple part-based models (Fischler and Elschlager1973), focusing on speci\ufb01c objects whose",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S34",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "spatial layouts are roughly rigid, such as faces. Before 1990 the leading paradigm of object recognition was based on geo- metric representations (Mundy2006; Ponce et al. 2007), with the focus later moving away from geometry and prior mod- els towards the use of statistical classi\ufb01ers [such as Neural Networks (Rowley et al. 1998), SVM (Osuna et al. 1997) and Adaboost (Viola and Jones 2001; Xiao et al. 2003)] based on appearance features (Murase and Nayar 1995a; Schmid and Mohr 1997). This successful family of object detectors set the stage for most subsequent research in this \ufb01eld. The milestones of object detection in more recent years are presented in Fig. 7, in which two main eras (SIFT vs. DCNN) are highlighted.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S35",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "The appearance features moved from global representations (Murase and Nayar1995b; Swain and Bal- lard 1991; Turk and Pentland 1991) to local representations that are designed to be invariant to changes in translation, scale, rotation, illumination, viewpoint and occlusion. Hand- crafted local invariant features gained tremendous popularity, starting from the Scale Invariant Feature Transform (SIFT) feature (Lowe1999), and the progress on various visual recognition tasks was based substantially on the use of local descriptors (Mikolajczyk and Schmid 2005) such as Haar- like features (Viola and Jones 2001), SIFT (Lowe 2004), Shape Contexts (Belongie et al. 2002), Histogram of Gradi- ents (HOG) (Dalal and Triggs 2005) Local Binary Patterns (LBP) (Ojala et al. 2002), and region covariances (Tuzel et al. 2006). These",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S36",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "local features are usually aggregated by simple concatenation or feature pooling encoders such as the Bag of Visual Words approach, introduced by Sivic and Zisserman (2003) and Csurka et al. ( 2004), Spatial Pyramid Matching (SPM) of BoW models (Lazebnik et al. 2006), and Fisher V ectors (Perronnin et al. 2010). For years, the multistage hand tuned pipelines of hand- crafted local descriptors and discriminative classi\ufb01ers dom- inated a variety of domains in computer vision, including object detection, until the signi\ufb01cant turning point in 2012 when DCNNs (Krizhevsky et al.2012a) achieved their record-breaking results in image classi\ufb01cation. The use of CNNs for detection and localization (Row- ley et al. 1998) can be traced back to the 1990s, with a modest",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S37",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "number of hidden layers used for object detection (V aillant et al. 1994; Rowley et al. 1998; Sermanet et al. 2013), successful in restricted domains such as face detec- tion. However, more recently, deeper CNNs have led to record-breaking improvements in the detection of more gen- eral object categories, a shift which came about when the successful application of DCNNs in image classi\ufb01cation (Krizhevsky et al. 2012a) was transferred to object detec- tion, resulting in the milestone Region-based CNN (RCNN) detector of Girshick et al. (2014). The successes of deep detectors rely heavily on vast train- ing data and large networks with millions or even billions of parameters. The availability of GPUs with very high compu- tational capability and large-scale",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S38",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "detection datasets [such as ImageNet (Deng et al. 2009; Russakovsky et al. 2015) and MS COCO (Lin et al. 2014)] play a key role in their suc- cess. Large datasets have allowed researchers to target more realistic and complex problems from images with large intra- class variations and inter-class similarities (Lin et al.2014; Russakovsky et al. 2015). However, accurate annotations are labor intensive to obtain, so detectors must consider meth- ods that can relieve annotation dif\ufb01culties or can learn with smaller training datasets. The research community has started moving towards the challenging goal of building general purpose object detec- tion systems whose ability to detect many object categories matches that of humans. This is a major challenge: accord- ing to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S39",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "cognitive scientists, human beings can identify around 3000 entry level categories and 30,000 visual categories over- all, and the number of categories distinguishable with domain expertise may be to the order of 10 5 (Biederman 1987a). Despite the remarkable progress of the past years, designing an accurate, robust, ef\ufb01cient detection and recognition sys- tem that approaches human-level performance on 10 4\u2013105 categories is undoubtedly an unresolved problem. 3 A Brief Introduction to Deep Learning Deep learning has revolutionized a wide range of machine learning tasks, from image classi\ufb01cation and video process- ing to speech recognition and natural language understand- ing. Given this tremendously rapid evolution, there exist many recent survey papers on deep learning (Bengio et al. 2013; Goodfellow et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S40",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "al. 2016;G ue ta l . 2018; LeCun et al. 2015; Litjens et al. 2017; Pouyanfar et al. 2018;W ue ta l . 2019; Y oung et al. 2018; Zhang et al. 2018d; Zhou et al. 2018a; Zhu et al. 2017). These surveys have reviewed deep learning techniques from different perspectives (Bengio et al. 2013; Goodfellow et al. 2016;G ue ta l . 2018; LeCun et al. 2015; Pouyanfar et al. 2018;W ue ta l . 2019; Zhou et al. 2018a), or with applications to medical image analysis (Lit- jens et al. 2017), natural language processing (Y oung et al. 2018), speech recognition systems (Zhang et al. 2018d), and remote sensing (Zhu et al. 2017). 123 International Journal of Computer",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S41",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "Vision (a) (b) Fig. 8 a Illustration of three operations that are repeatedly applied by a typical CNN: convolution with a number of linear \ufb01lters; Nonlinearities (e.g. ReLU); and local pooling (e.g. max pooling). TheM feature maps from a previous layer are convolved with N different \ufb01lters (here shown as size 3 \u00d7 3 \u00d7 M), using a stride of 1. The resulting N feature maps are then passed through a nonlinear function (e.g. ReLU), and pooled (e.g. taking a maximum over 2\u00d7 2 regions) to give N feature maps at a reduced resolution. b Illustration of the architecture of VGGNet (Simonyan and Zisserman 2015), a typical CNN with 11 weight layers. An image with 3 color channels is presented",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S42",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "as the input. The network has 8 convolutional layers, 3 fully connected layers, 5 max pooling lay- ers and a softmax classi\ufb01cation layer. The last three fully connected layers take features from the top convolutional layer as input in vector form. The \ufb01nal layer is aC-way softmax function, C being the number of classes. The whole network can be learned from labeled training data by optimizing an objective function (e.g. mean squared error or cross entropy loss) via stochastic gradient descent (Color \ufb01gure online) Convolutional Neural Networks (CNNs), the most repre- sentative models of deep learning, are able to exploit the basic properties underlying natural signals: translation invariance, local connectivity, and compositional hierarchies (LeCun et al. 2015). A typical CNN,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S43",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "illustrated in Fig. 8, has a hier- archical structure and is composed of a number of layers to learn representations of data with multiple levels of abstrac- tion (LeCun et al. 2015). We begin with a convolution xl\u22121 \u2217 wl (1) between an input feature map xl\u22121 at a feature map from previous layer l\u22121, convolved with a 2D convolutional kernel (or \ufb01lter or weights) wl . This convolution appears over a sequence of layers, subject to a nonlinear operation \u03c3, such that xl j = \u03c3 \u239b \u239d Nl\u22121 \u2211 i=1 xl\u22121 i \u2217 wl i,j + bl j \u239e \u23a0 , (2) with a convolution now between the Nl\u22121 input feature maps xl\u22121 i and the corresponding kernel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S44",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "wl i,j , plus a bias term bl j . The elementwise nonlinear function \u03c3(\u00b7) is typically a recti- \ufb01ed linear unit (ReLU) for each element, \u03c3(x) = max{x,0}. (3) Finally, pooling corresponds to the downsampling/upsampl- ing of feature maps. These three operations (convolution, nonlinearity, pooling) are illustrated in Fig.8a; CNNs having a large number of layers, a \u201cdeep\u201d network, are referred to as Deep CNNs (DCNNs), with a typical DCNN architecture illustrated in Fig.8b. Most layers of a CNN consist of a number of feature maps, within which each pixel acts like a neuron. Each neuron in a convolutional layer is connected to feature maps of the pre- vious layer through a set of weightswi,j (essentially a set of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S45",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "2D \ufb01lters). As can be seen in Fig. 8b, where the early CNN layers are typically composed of convolutional and pooling layers, the later layers are normally fully connected. From earlier to later layers, the input image is repeatedly con- volved, and with each layer, the receptive \ufb01eld or region of support increases. In general, the initial CNN layers extract low-level features (e.g., edges), with later layers extracting more general features of increasing complexity (Zeiler and Fergus2014; Bengio et al. 2013; LeCun et al. 2015; Oquab et al. 2014). DCNNs have a number of outstanding advantages: a hierarchical structure to learn representations of data with multiple levels of abstraction, the capacity to learn very com- plex functions, and learning feature",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S46",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "representations directly and automatically from data with minimal domain knowl- edge. What has particularly made DCNNs successful has 123 International Journal of Computer Vision been the availability of large scale labeled datasets and of GPUs with very high computational capability. Despite the great successes, known de\ufb01ciencies remain. In particular, there is an extreme need for labeled training data and a requirement of expensive computing resources, and considerable skill and experience are still needed to select appropriate learning parameters and network architectures. Trained networks are poorly interpretable, there is a lack of robustness to degradations, and many DCNNs have shown serious vulnerability to attacks (Goodfellow et al.2015), all of which currently limit the use of DCNNs in real-world applications. 4 Datasets",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S47",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "and Performance Evaluation 4.1 Datasets Datasets have played a key role throughout the history of object recognition research, not only as a common ground for measuring and comparing the performance of competing algorithms, but also pushing the \ufb01eld towards increasingly complex and challenging problems. In particular, recently, deep learning techniques have brought tremendous success to many visual recognition problems, and it is the large amounts of annotated data which play a key role in their success. Access to large numbers of images on the Internet makes it possible to build comprehensive datasets in order to capture a vast richness and diversity of objects, enabling unprece- dented performance in object recognition. For generic object detection, there are four famous datasets: PASCAL",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S48",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "VOC (Everingham et al. 2010, 2015), ImageNet (Deng et al. 2009), MS COCO (Lin et al. 2014) and Open Images (Kuznetsova et al. 2018). The attributes of these datasets are summarized in Table 2, and selected sample images are shown in Fig. 9. There are three steps to creating large-scale annotated datasets: determining the set of target object categories, collecting a diverse set of candidate images to represent the selected categories on the Internet, and annotating the collected images, typically by designing crowdsourcing strategies. Recognizing space limitations, we refer interested readers to the original papers (Everingham et al. 2010, 2015; Lin et al. 2014; Russakovsky et al. 2015; Kuznetsova et al. 2018) for detailed descriptions of these datasets in terms",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S49",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "of construction and properties. The four datasets form the backbone of their respective detection challenges. Each challenge consists of a publicly available dataset of images together with ground truth anno- tation and standardized evaluation software, and an annual competition and corresponding workshop. Statistics for the number of images and object instances in the training, vali- dation and testing datasets 2 for the detection challenges are given in Table 3. The most frequent object classes in VOC, COCO, ILSVRC and Open Images detection datasets are visualized in Table 4. PASCAL VOC Everingham et al. ( 2010, 2015) is a multi- year effort devoted to the creation and maintenance of a series of benchmark datasets for classi\ufb01cation and object detection, creating the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S50",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "precedent for standardized evaluation of recog- nition algorithms in the form of annual competitions. Starting from only four categories in 2005, the dataset has increased to 20 categories that are common in everyday life. Since 2009, the number of images has grown every year, but with all pre- vious images retained to allow test results to be compared from year to year. Due the availability of larger datasets like ImageNet, MS COCO and Open Images, PASCAL VOC has gradually fallen out of fashion. ILSVRC, the ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al. 2015), is derived from Ima- geNet (Deng et al. 2009), scaling up PASCAL VOC\u2019s goal of standardized training and evaluation of detection algorithms by more than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S51",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "an order of magnitude in the number of object classes and images. ImageNet1000, a subset of ImageNet images with 1000 different object categories and a total of 1.2 million images, has been \ufb01xed to provide a standardized benchmark for the ILSVRC image classi\ufb01cation challenge. MS COCO is a response to the criticism of ImageNet that objects in its dataset tend to be large and well centered, mak- ing the ImageNet dataset atypical of real-world scenarios. To push for richer image understanding, researchers created the MS COCO database (Lin et al.2014) containing com- plex everyday scenes with common objects in their natural context, closer to real life, where objects are labeled using fully-segmented instances to provide more accurate detec- tor evaluation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S52",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "The COCO object detection challenge (Lin et al. 2014) features two object detection tasks: using either bounding box output or object instance segmentation output. COCO introduced three new challenges: 1. It contains objects at a wide range of scales, including a high percentage of small objects (Singh and Davis 2018); 2. Objects are less iconic and amid clutter or heavy occlu- sion; 3. The evaluation metric (see Table 5) encourages more accurate object localization. Just like ImageNet in its time, MS COCO has become the standard for object detection today. OICOD (the Open Image Challenge Object Detection) is derived from Open Images V4 (now V5 in 2019) (Kuznetsova et al. 2018), currently the largest publicly available object 2 The annotations",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S53",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "on the test set are not publicly released, except for PASCAL VOC2007. 123 International Journal of Computer Vision Table 2 Popular databases for object recognition Dataset name Total images Categories Images per category Objects per image Image size Started year Highlights PASCAL VOC (2012) (Evering- ham et al. 2015) 11,540 20 303\u20134087 2 .4 470 \u00d7 380 2005 Covers only 20 categories that are common in everyday life; Large number of training images; Close to real-world applications; Signi\ufb01cantly larger intraclass variations; Objects in scene context; Multiple objects in one image; Contains many dif\ufb01cult samples ImageNet (Rus- sakovsky et al. 2015) 14 millions+ 21 ,841 \u2212 1.5 500 \u00d7 400 2009 Large number of object categories; More instances and more categories",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S54",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "of objects per image; More challenging than PASCAL VOC; Backbone of the ILSVRC challenge; Images are object-centric MS COCO (Lin et al. 2014) 328,000+ 91 \u2212 7.3 640 \u00d7 480 2014 Even closer to real world scenarios; Each image contains more instances of objects and richer object annotation information; Contains object segmentation notation data that is not available in the ImageNet dataset Places (Zhou et al. 2017a) 10 millions + 434 \u2212\u2212 256 \u00d7 256 2014 The largest labeled dataset for scene recognition; Four subsets Places365 Standard, Places365 Challenge, Places 205 and Places88 as benchmarks Open Images (Kuznetsova et al.2018) 9 millions + 6000+\u2212 8.3 V aried 2017 Annotated with image level labels, object bounding boxes and visual relationships; Open",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S55",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "Images V5 supports large scale object detection, object instance segmentation and visual relationship detection Example images from PASCAL VOC, ImageNet, MS COCO and Open Images are shown in Fig. 9 (a) (b) (c) (d) Fig. 9 Some example images with object annotations from PASCAL VOC, ILSVRC, MS COCO and Open Images. See Table 2 for a summary of these datasets detection dataset. OICOD is different from previous large scale object detection datasets like ILSVRC and MS COCO, not merely in terms of the signi\ufb01cantly increased number of classes, images, bounding box annotations and instance segmentation mask annotations, but also regarding the anno- tation process. In ILSVRC and MS COCO, instances of all 123 International Journal of Computer Vision Table 3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S56",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "Statistics of commonly used object detection datasets Challenge Object classes Number of images Number of annotated objects Summary (Train +Va l ) Train V al Test Train V al Images Boxes Boxes/Image PASCAL VOC object detection challenge VOC07 20 2501 2510 4952 6301 (7844) 6307(7818) 5011 12 ,608 2 .5 VOC08 20 2111 2221 4133 5082 (6337) 5281(6347) 4332 10 ,364 2 .4 VOC09 20 3473 3581 6650 8505 (9760) 8713(9779) 7054 17 ,218 2 .3 VOC10 20 4998 5105 9637 11 ,577(13,339) 11,797(13,352) 10,103 23 ,374 2 .4 VOC11 20 5717 5823 10 ,994 13 ,609(15,774) 13,841(15,787) 11,540 27 ,450 2 .4 VOC12 20 5717 5823 10 ,991 13 ,609(15,774) 13,841(15,787) 11,540 27 ,450 2 .4 ILSVRC object detection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S57",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "challenge ILSVRC13 200 395,909 20 ,121 40 ,152 345 ,854 55 ,502 416 ,030 401 ,356 1 .0 ILSVRC14 200 456 ,567 20 ,121 40 ,152 478 ,807 55 ,502 476 ,668 534 ,309 1 .1 ILSVRC15 200 456 ,567 20 ,121 51 ,294 478 ,807 55 ,502 476 ,668 534 ,309 1 .1 ILSVRC16 200 456 ,567 20 ,121 60 ,000 478 ,807 55 ,502 476 ,668 534 ,309 1 .1 ILSVRC17 200 456 ,567 20 ,121 65 ,500 478 ,807 55 ,502 476 ,668 534 ,309 1 .1 MS COCO object detection challenge MS COCO15 80 82,783 40 ,504 81 ,434 604 ,907 291 ,875 123 ,287 896 ,782 7 .3 MS COCO16 80 82 ,783 40 ,504",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S58",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "81 ,434 604 ,907 291 ,875 123 ,287 896 ,782 7 .3 MS COCO17 80 118 ,287 5000 40 ,670 860 ,001 36 ,781 123 ,287 896 ,782 7 .3 MS COCO18 80 118 ,287 5000 40 ,670 860 ,001 36 ,781 123 ,287 896 ,782 7 .3 Open images challenge object detection (OICOD)( B a s e do no p e ni m a g e sV 4Kuznetsova et al. 2018) OICOD18 500 1 ,643,042 100 ,000 99 ,999 11 ,498,734 696 ,410 1 ,743,042 12 ,195,144 7 .0 Object statistics for VOC challenges list the non-dif\ufb01cult objects used in the evaluation (all annotated objects). For the COCO challenge, prior to 2017, the test set had four splits (",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S59",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "Dev, Standard, Reserve,a n d Challenge), with each having about 20K images. Starting in 2017, the test set has only the Dev and Challenge splits, with the other two splits removed. Starting in 2017, the train and val sets are arranged differently, and the test set is divided into two roughly equally sized splits of about 20 ,000 images each: Test Dev and Test Challenge. Note that the 2017 Test Dev/Challenge splits contain the same images as the 2015 Test Dev/Challenge splits, so results across the years are directly comparable classes in the dataset are exhaustively annotated, whereas for Open Images V4 a classi\ufb01er was applied to each image and only those labels with suf\ufb01ciently high scores were sent for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S60",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "human veri\ufb01cation. Therefore in OICOD only the object instances of human-con\ufb01rmed positive labels are annotated. 4.2 Evaluation Criteria There are three criteria for evaluating the performance of detection algorithms: detection speed in Frames Per Second (FPS), precision, and recall. The most commonly used met- ric is Average Precision (AP), derived from precision and recall. AP is usually evaluated in a category speci\ufb01c manner, i.e., computed for each object category separately. To com- pare performance over all object categories, the mean AP (mAP) averaged over all object categories is adopted as the \ufb01nal measure of performance 3. More details on these metrics 3 In object detection challenges, such as PASCAL VOC and ILSVRC, the winning entry of each object category is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S61",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "that with the highest AP score, and the winner of the challenge is the team that wins on the most object categories. The mAP is also used as the measure of a team\u2019s can be found in Everingham et al. ( 2010), Everingham et al. (2015), Russakovsky et al. ( 2015), Hoiem et al. ( 2012). The standard outputs of a detector applied to a testing image I are the predicted detections {(b j ,c j , p j )}j , indexed by object j, of Bounding Box (BB) b j , predicted category c j , and con\ufb01dence p j . A predicted detection (b,c, p)is regarded as a True Positive (TP) if \u2022 The predicted category c equals",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S62",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "the ground truth label cg. \u2022 The overlap ratio IOU (Intersection Over Union) (Ever- ingham et al. 2010; Russakovsky et al. 2015) IOU(b,bg) = area (b \u2229 bg) area (b \u222a bg), (4) between the predicted BB b and the ground truth bg is not smaller than a prede\ufb01ned threshold \u03b5, where \u2229 and Footnote 3 continued performance, and is justi\ufb01ed since the ranking of teams by mAP was always the same as the ranking by the number of object categories won (Russakovsky et al.2015). 123 International Journal of Computer Vision Table 4 Most frequent object classes for each detection challenge (a) (b) (c) (d) The size of each word is proportional to the frequency of that class in the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S63",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "training dataset cup denote intersection and union, respectively. A typical value of \u03b5 is 0.5. Otherwise, it is considered as a False Positive (FP). The con- \ufb01dence level p is usually compared with some threshold \u03b2 to determine whether the predicted class label c is accepted. AP is computed separately for each of the object classes, based on Precision and Recall. For a given object class c and a testing image Ii ,l e t {(bij , pij )}M j=1 denote the detections returned by a detector, ranked by con\ufb01dence pij in decreasing order. Each detection (bij , pij ) is either a TP or an FP , which can be determined via the algorithm 4 in Fig. 10. Based",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S64",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "on the TP and FP detections, the precision P(\u03b2) and recall R(\u03b2) (Everingham et al. 2010) can be computed as a function of the con\ufb01dence threshold \u03b2, so by varying the con\ufb01dence 4 It is worth noting that for a given threshold \u03b2, multiple detections of the same object in an image are not considered as all correct detections, and only the detection with the highest con\ufb01dence level is considered as a TP and the rest as FPs. threshold different pairs (P, R) can be obtained, in principle allowing precision to be regarded as a function of recall, i.e. P(R), from which the Average Precision (AP) (Everingham et al. 2010; Russakovsky et al. 2015) can be found. Since the introduction",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S65",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "of MS COCO, more attention has been placed on the accuracy of the bounding box location. Instead of using a \ufb01xed IOU threshold, MS COCO introduces a few metrics (summarized in Table 5) for characterizing the performance of an object detector. For instance, in contrast to the traditional mAP computed at a single IoU of 0.5, APcoco is averaged across all object categories and multiple IOU val- ues from 0 .5t o0 .95 in steps of 0 .05. Because 41% of the objects in MS COCO are small and 24% are large, metrics AP small coco , AP medium coco and AP large coco are also introduced. Finally, Table 5 summarizes the main metrics used in the PASCAL, ILSVRC and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S66",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "MS COCO object detection challenges, with metric modi\ufb01cations for the Open Images challenges pro- posed in Kuznetsova et al. (2018). 5 Detection Frameworks There has been steady progress in object feature represen- tations and classi\ufb01ers for recognition, as evidenced by the dramatic change from handcrafted features (Viola and Jones 2001; Dalal and Triggs 2005; Felzenszwalb et al. 2008; Harzallah et al. 2009; V edaldi et al. 2009) to learned DCNN features (Girshick et al. 2014; Ouyang et al. 2015; Girshick 2015; Ren et al. 2015; Dai et al. 2016c). In contrast, in terms of localization, the basic \u201csliding window\u201d strategy (Dalal and Triggs 2005; Felzenszwalb et al. 2010b, 2008) remains mainstream, although with some efforts to avoid exhaustive search (Lampert",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S67",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "et al. 2008; Uijlings et al. 2013). However, the number of windows is large and grows quadratically with the number of image pixels, and the need to search over multiple scales and aspect ratios further increases the search space. Therefore, the design of ef\ufb01cient and effec- tive detection frameworks plays a key role in reducing this computational cost. Commonly adopted strategies include cascading, sharing feature computation, and reducing per- window computation. This section reviews detection frameworks, listed in Fig. 11 and Table 11, the milestone approaches appearing since deep learning entered the \ufb01eld, organized into two main categories: (a) Two stage detection frameworks, which include a pre- processing step for generating object proposals; (b) One stage detection frameworks, or region",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S68",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "proposal free frameworks, having a single proposed method which does not separate the process of the detection proposal. 123 International Journal of Computer Vision Table 5 Summary of commonly used metrics for evaluating object detectors Metric Meaning De\ufb01nition and description TP True positive A true positive detection, per Fig. 10 FP False positive A false positive detection, per Fig. 10 \u03b2 Con\ufb01dence threshold A con\ufb01dence threshold for computing P(\u03b2) and R(\u03b2) \u03b5 IOU threshold VOC Typically around 0 .5 ILSVRC min (0.5, wh (w+10)(h+10) ); w \u00d7 h is the size of a GT box MS COCO Ten IOU thresholds \u03b5 \u2208{ 0.5 : 0.05 : 0.95} P(\u03b2) Precision The fraction of correct detections out of the total detections returned",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S69",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "by the detector with con\ufb01dence of at least \u03b2 R(\u03b2) Recall The fraction of all Nc objects detected by the detector having a con\ufb01dence of at least \u03b2 AP Average Precision Computed over the different levels of recall achieved by varying the con\ufb01dence \u03b2 mAP mean Average Precision VOC AP at a single IOU and averaged over all classes ILSVRC AP at a modi\ufb01ed IOU and averaged over all classes MS COCOAPcoco: mAP averaged over ten IOUs: {0.5 : 0.05 : 0.95}; AP IOU=0.5 coco : mAP at IOU = 0.50 (PASCAL VOC metric); AP IOU=0.75 coco :m A Pa tI O U = 0.75 (strict metric); AP small coco : mAP for small objects of area smaller than 32",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S70",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "2; AP medium coco : mAP for objects of area between 32 2 and 96 2; AP large coco : mAP for large objects of area bigger than 96 2; AR Average Recall The maximum recall given a \ufb01xed number of detections per image, averaged over all categories and IOU thresholds AR Average Recall MS COCO AR max=1 coco : AR given 1 detection per image; AR max=10 coco : AR given 10 detection per image; AR max=100 coco : AR given 100 detection per image; AR small coco : AR for small objects of area smaller than 32 2; AR medium coco : AR for objects of area between 32 2 and 96 2; AR large coco : AR",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S71",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "for large objects of area bigger than 96 2; Fig. 10 The algorithm for determining TPs and FPs by greedily match- ing object detection results to ground truth boxes Sections 6\u20139 will discuss fundamental sub-problems involved in detection frameworks in greater detail, including DCNN features, detection proposals, and context modeling. 5.1 Region Based (Two Stage) Frameworks In a region-based framework, category-independent region proposals5 are generated from an image, CNN (Krizhevsky et al. 2012a) features are extracted from these regions, and then category-speci\ufb01c classi\ufb01ers are used to determine the category labels of the proposals. As can be observed from Fig. 11, DetectorNet (Szegedy et al. 2013), OverFeat (Ser- manet et al. 2014), MultiBox (Erhan et al. 2014) and RCNN (Girshick et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S72",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "al. 2014) independently and almost simultane- ously proposed using CNNs for generic object detection. RCNN (Girshick et al. 2014): Inspired by the break- through image classi\ufb01cation results obtained by CNNs and the success of the selective search in region proposal for hand- crafted features (Uijlings et al. 2013), Girshick et al. ( 2014, 2016) were among the \ufb01rst to explore CNNs for generic object detection and developed RCNN, which integrates 5 Object proposals, also called region proposals or detection proposals, are a set of candidate regions or bounding boxes in an image that may potentially contain an object (Chavali et al.2016;H o s a n ge ta l .2016). 123 International Journal of Computer Vision RCNN (Girshick et al.) DetectorNet",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S73",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "(Szegedy et al.) Faster RCNN (Ren et al.) RFCN (Dai et al.) YOLO (Redmon et al.) Mask RCNN (He et al.) MultiBox (Erhan et al.) OverFeat (Sermanet et al.) MSC Multibox (Szegedy et al.) Fast RCNN (Girshick) SSD (Liu et al.) NIN (Lin et al.) VGGNet (Simonyan and Zisserman) GoogLeNet (Szegedy et al.) ResNet (He et al.) SPPNet (He et al.) YOLO9000 (Redmon and Farhadi) RetinaNet (Lin et al.) DenseNet (Huang et al.) Feature Pyramid Network (FPN) (Lin et al.) CornerNet (Law and Deng) Fig. 11 Milestones in generic object detection Fig. 12 Illustration of the RCNN detection framework (Girshick et al. 2014, 2016) AlexNet (Krizhevsky et al. 2012a) with a region proposal selective search (Uijlings et al. 2013). As",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S74",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "illustrated in detail in Fig. 12, training an RCNN framework consists of multi- stage pipelines: 1. Region proposal computation Class agnostic region pro- posals, which are candidate regions that might contain objects, are obtained via a selective search (Uijlings et al. 2013). 2. CNN model \ufb01netuning Region proposals, which are cropped from the image and warped into the same size, are used as the input for \ufb01ne-tuning a CNN model pre- trained using a large-scale dataset such as ImageNet. At this stage, all region proposals with \u2265 0.5I O U 6 overlap with a ground truth box are de\ufb01ned as positives for that ground truth box\u2019s class and the rest as negatives. 3. Class speci\ufb01c SVM classi\ufb01ers training A set",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S75",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "of class- speci\ufb01c linear SVM classi\ufb01ers are trained using \ufb01xed length features extracted with CNN, replacing the soft- max classi\ufb01er learned by \ufb01ne-tuning. For training SVM classi\ufb01ers, positive examples are de\ufb01ned to be the ground truth boxes for each class. A region proposal with less than 0.3 IOU overlap with all ground truth instances of a class is negative for that class. Note that the positive and negative examples de\ufb01ned for training the SVM classi- \ufb01ers are different from those for \ufb01ne-tuning the CNN. 4. Class speci\ufb01c bounding box regressor training Bounding box regression is learned for each object class with CNN features. In spite of achieving high object detection quality, RCNN has notable drawbacks (Girshick 2015): 1. Training is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S76",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "a multistage pipeline, slow and hard to opti- mize because each individual stage must be trained separately. 2. For SVM classi\ufb01er and bounding box regressor training, it is expensive in both disk space and time, because CNN features need to be extracted from each object proposal in each image, posing great challenges for large scale detection, particularly with very deep networks, such as VGG16 (Simonyan and Zisserman 2015). 3. Testing is slow, since CNN features are extracted per object proposal in each test image, without shared com- putation. All of these drawbacks have motivated successive innova- tions, leading to a number of improved detection frameworks such as SPPNet, Fast RCNN, Faster RCNN etc., as follows. 6 Please refer to Sect.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S77",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "4.2 for the de\ufb01nition of IOU. 123 International Journal of Computer Vision SPPNet (He et al. 2014) During testing, CNN feature extraction is the main bottleneck of the RCNN detection pipeline, which requires the extraction of CNN features from thousands of warped region proposals per image. As a result, He et al. ( 2014) introduced traditional spatial pyramid pooling (SPP) (Grauman and Darrell 2005; Lazebnik et al. 2006) into CNN architectures. Since convolutional layers accept inputs of arbitrary sizes, the requirement of \ufb01xed- sized images in CNNs is due only to the Fully Connected (FC) layers, therefore He et al. added an SPP layer on top of the last convolutional (CONV) layer to obtain features of \ufb01xed length for the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S78",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "FC layers. With this SPPNet, RCNN obtains a signi\ufb01cant speedup without sacri\ufb01cing any detec- tion quality, because it only needs to run the convolutional layers once on the entire test image to generate \ufb01xed-length features for region proposals of arbitrary size. While SPPNet accelerates RCNN evaluation by orders of magnitude, it does not result in a comparable speedup of the detector training. Moreover, \ufb01ne-tuning in SPPNet (He et al. 2014) is unable to update the convolutional layers before the SPP layer, which limits the accuracy of very deep networks. Fast RCNN (Girshick 2015) Girshick proposed Fast RCNN (Girshick 2015) that addresses some of the dis- advantages of RCNN and SPPNet, while improving on their detection speed and quality. As illustrated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S79",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "in Fig. 13, Fast RCNN enables end-to-end detector training by devel- oping a streamlined training process that simultaneously learns a softmax classi\ufb01er and class-speci\ufb01c bounding box regression, rather than separately training a softmax clas- si\ufb01er, SVMs, and Bounding Box Regressors (BBRs) as in RCNN/SPPNet. Fast RCNN employs the idea of sharing the computation of convolution across region proposals, and adds a Region of Interest (RoI) pooling layer between the last CONV layer and the \ufb01rst FC layer to extract a \ufb01xed-length feature for each region proposal. Essentially, RoI pooling uses warping at the feature level to approx- imate warping at the image level. The features after the RoI pooling layer are fed into a sequence of FC layers that \ufb01nally",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S80",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "branch into two sibling output layers: softmax prob- abilities for object category prediction, and class-speci\ufb01c bounding box regression offsets for proposal re\ufb01nement. Compared to RCNN/SPPNet, Fast RCNN improves the ef\ufb01- ciency considerably\u2014typically 3 times faster in training and 10 times faster in testing. Thus there is higher detection qual- ity, a single training process that updates all network layers, and no storage required for feature caching. Faster RCNN (Ren et al. 2015, 2017) Although Fast RCNN signi\ufb01cantly sped up the detection process, it still relies on external region proposals, whose computation is exposed as the new speed bottleneck in Fast RCNN. Recent work has shown that CNNs have a remarkable ability to local- ize objects in CONV layers (Zhou et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S81",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "al. 2015, 2016a; Cinbis et al. 2017; Oquab et al. 2015; Hariharan et al. 2016), an Fig. 13 High level diagrams of the leading frameworks for generic object detection. The properties of these methods are summarized in Table11 123 International Journal of Computer Vision ability which is weakened in the FC layers. Therefore, the selective search can be replaced by a CNN in producing region proposals. The Faster RCNN framework proposed by Ren et al. ( 2015, 2017) offered an ef\ufb01cient and accu- rate Region Proposal Network (RPN) for generating region proposals. They utilize the same backbone network, using features from the last shared convolutional layer to accom- plish the task of RPN for region proposal and Fast RCNN for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S82",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "region classi\ufb01cation, as shown in Fig. 13. RPN \ufb01rst initializes k reference boxes (i.e. the so called anchors) of different scales and aspect ratios at each CONV feature map location. The anchor positions are image content independent, but the feature vectors themselves, extracted from anchors, are image content dependent. Each anchor is mapped to a lower dimensional vector, which is fed into two sibling FC layers\u2014an object category classi\ufb01cation layer and a box regression layer. In contrast to detection in Fast RCNN, the features used for regression in RPN are of the same shape as the anchor box, thus k anchors lead to k regressors. RPN shares CONV features with Fast RCNN, thus enabling highly ef\ufb01cient region proposal computation. RPN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S83",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "is, in fact, a kind of Fully Convolutional Network (FCN) (Long et al. 2015; Shelhamer et al. 2017); Faster RCNN is thus a purely CNN based framework without using handcrafted features. For the VGG16 model (Simonyan and Zisserman 2015), Faster RCNN can test at 5 FPS (including all stages) on a GPU, while achieving state-of-the-art object detection accu- racy on PASCAL VOC 2007 using 300 proposals per image. The initial Faster RCNN in Ren et al. ( 2015) contains sev- eral alternating training stages, later simpli\ufb01ed in Ren et al. (2017). Concurrent with the development of Faster RCNN, Lenc and V edaldi ( 2015) challenged the role of region proposal generation methods such as selective search, studied the role of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S84",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "region proposal generation in CNN based detectors, and found that CNNs contain suf\ufb01cient geometric information for accurate object detection in the CONV rather than FC layers. They showed the possibility of building integrated, simpler, and faster object detectors that rely exclusively on CNNs, removing region proposal generation methods such as selective search. RFCN (Region based Fully Convolutional Network ) While Faster RCNN is an order of magnitude faster than Fast RCNN, the fact that the region-wise sub-network still needs to be applied per RoI (several hundred RoIs per image) led Dai et al. (2016c) to propose the RFCN detector which is fully convolutional (no hidden FC layers) with almost all computations shared over the entire image. As shown in Fig.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S85",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "13, RFCN differs from Faster RCNN only in the RoI sub-network. In Faster RCNN, the computation after the RoI pooling layer cannot be shared, so Dai et al. ( 2016c) proposed using all CONV layers to construct a shared RoI sub-network, and RoI crops are taken from the last layer of CONV features prior to prediction. However, Dai et al. ( 2016c) found that this naive design turns out to have considerably inferior detection accuracy, conjectured to be that deeper CONV layers are more sensitive to category semantics, and less sensitive to translation, whereas object detection needs localization rep- resentations that respect translation invariance. Based on this observation, Dai et al. (2016c) constructed a set of position- sensitive score maps",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S86",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "by using a bank of specialized CONV layers as the FCN output, on top of which a position-sensitive RoI pooling layer is added. They showed that RFCN with ResNet101 (He et al. 2016) could achieve comparable accu- racy to Faster RCNN, often at faster running times. Mask RCNN He et al. ( 2017) proposed Mask RCNN to tackle pixelwise object instance segmentation by extend- ing Faster RCNN. Mask RCNN adopts the same two stage pipeline, with an identical \ufb01rst stage (RPN), but in the sec- ond stage, in parallel to predicting the class and box offset, Mask RCNN adds a branch which outputs a binary mask for each RoI. The new branch is a Fully Convolutional Network (FCN) (Long et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S87",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "al. 2015; Shelhamer et al. 2017) on top of a CNN feature map. In order to avoid the misalignments caused by the original RoI pooling (RoIPool) layer, a RoIAlign layer was proposed to preserve the pixel level spatial cor- respondence. With a backbone network ResNeXt101-FPN (Xie et al. 2017; Lin et al. 2017a), Mask RCNN achieved top results for the COCO object instance segmentation and bounding box object detection. It is simple to train, general- izes well, and adds only a small overhead to Faster RCNN, running at 5 FPS (He et al. 2017). Chained Cascade Network and Cascade RCNN The essence of cascade (Felzenszwalb et al. 2010a; Bourdev and Brandt 2005; Li and Zhang 2004) is to learn more",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S88",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "dis- criminative classi\ufb01ers by using multistage classi\ufb01ers, such that early stages discard a large number of easy negative samples so that later stages can focus on handling more dif\ufb01- cult examples. Two-stage object detection can be considered as a cascade, the \ufb01rst detector removing large amounts of",
      "page_hint": null,
      "token_count": 47,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S89",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "regions. Recently, end-to-end learning of more than two cas- caded classi\ufb01ers and DCNNs for generic object detection were proposed in the Chained Cascade Network (Ouyang et al. 2017a), extended in Cascade RCNN (Cai and V asconce- los 2018), and more recently applied for simultaneous object detection and instance segmentation (Chen et al. 2019a), win- ning the COCO 2018 Detection Challenge. Light Head RCNN In order to further increase the detec- tion speed of RFCN (Dai et al. 2016c), Li et al. ( 2018c)p r o - posed Light Head RCNN, making the head of the detection network as light as possible to reduce the RoI computation. In particular, Li et al. (2018c) applied a convolution to pro- duce thin feature",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S90",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "maps with small channel numbers (e.g., 490 channels for COCO) and a cheap RCNN sub-network, leading to an excellent trade-off of speed and accuracy. 123 International Journal of Computer Vision 5.2 Unified (One Stage) Frameworks The region-based pipeline strategies of Sect. 5.1 have dom- inated since RCNN (Girshick et al. 2014), such that the leading results on popular benchmark datasets are all based on Faster RCNN (Ren et al.2015). Nevertheless, region- based approaches are computationally expensive for current mobile/wearable devices, which have limited storage and computational capability, therefore instead of trying to opti- mize the individual components of a complex region-based pipeline, researchers have begun to develop uni\ufb01ed detection strategies. Uni\ufb01ed pipelines refer to architectures that directly pre- dict class",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S91",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "probabilities and bounding box offsets from full images with a single feed-forward CNN in a monolithic set- ting that does not involve region proposal generation or post classi\ufb01cation / feature resampling, encapsulating all compu- tation in a single network. Since the whole pipeline is a single network, it can be optimized end-to-end directly on detection performance. DetectorNet (Szegedy et al. 2013) were among the \ufb01rst to explore CNNs for object detection. DetectorNet formulated object detection a regression problem to object bounding box masks. They use AlexNet (Krizhevsky et al. 2012a) and replace the \ufb01nal softmax classi\ufb01er layer with a regres- sion layer. Given an image window, they use one network to predict foreground pixels over a coarse grid, as well",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S92",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "as four additional networks to predict the object\u2019s top, bottom, left and right halves. A grouping process then converts the predicted masks into detected bounding boxes. The network needs to be trained per object type and mask type, and does not scale to multiple classes. DetectorNet must take many crops of the image, and run multiple networks for each part on every crop, thus making it slow. OverFeat, proposed by Sermanet et al. ( 2014) and illus- trated in Fig. 14, can be considered as one of the \ufb01rst single-stage object detectors based on fully convolutional (a) (b) Fig. 14 Illustration of the OverFeat (Sermanet et al. 2014) detection framework deep networks. It is one of the most in\ufb02uential object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S93",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "detec- tion frameworks, winning the ILSVRC2013 localization and detection competition. OverFeat performs object detection via a single forward pass through the fully convolutional layers in the network (i.e. the \u201cFeature Extractor\u201d, shown in Fig.14a). The key steps of object detection at test time can be summarized as follows: 1. Generate object candidates by performing object clas- si\ufb01cation via a sliding window fashion on multiscale images OverFeat uses a CNN like AlexNet (Krizhevsky et al. 2012a), which would require input images ofa \ufb01xed size due to its fully connected layers, in order to make the sliding window approach computationally ef\ufb01cient, OverFeat casts the network (as shown in Fig. 14a) into a fully convolutional network, taking inputs of any size, by viewing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S94",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "fully connected layers as convolutions with kernels of size 1 \u00d7 1. OverFeat leverages multiscale fea- tures to improve the overall performance by passing up to six enlarged scales of the original image through the net- work (as shown in Fig. 14b), resulting in a signi\ufb01cantly increased number of evaluated context views. For each of the multiscale inputs, the classi\ufb01er outputs a grid of predictions (class and con\ufb01dence). 2. Increase the number of predictions by offset max pooling In order to increase resolution, OverFeat applies offset max pooling after the last CONV layer, i.e. perform- ing a subsampling operation at every offset, yielding many more views for voting, increasing robustness while remaining ef\ufb01cient. 3. Bounding box regression Once an object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S95",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "is identi\ufb01ed, a single bounding box regressor is applied. The classi- \ufb01er and the regressor share the same feature extraction (CONV) layers, only the FC layers need to be recomputed after computing the classi\ufb01cation network. 4. Combine predictions OverFeat uses a greedy merge strat- egy to combine the individual bounding box predictions across all locations and scales. OverFeat has a signi\ufb01cant speed advantage, but is less accu- rate than RCNN (Girshick et al. 2014), because it was dif\ufb01cult to train fully convolutional networks at the time. The speed advantage derives from sharing the computation of convolu- tion between overlapping windows in the fully convolutional network. OverFeat is similar to later frameworks such as YOLO (Redmon et al. 2016) and SSD",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S96",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "(Liu et al. 2016), except that the classi\ufb01er and the regressors in OverFeat are trained sequentially. YOLO Redmon et al. ( 2016) proposed YOLO (Y ou Only Look Once), a uni\ufb01ed detector casting object detection as a regression problem from image pixels to spatially sep- arated bounding boxes and associated class probabilities, illustrated in Fig.13. Since the region proposal generation 123 International Journal of Computer Vision stage is completely dropped, YOLO directly predicts detec- tions using a small set of candidate regions 7. Unlike region based approaches (e.g. Faster RCNN) that predict detections based on features from a local region, YOLO uses features from an entire image globally. In particular, YOLO divides an image into anS \u00d7 S grid, each",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S97",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "predicting C class prob- abilities, B bounding box locations, and con\ufb01dence scores. By throwing out the region proposal generation step entirely, YOLO is fast by design, running in real time at 45 FPS and Fast YOLO (Redmon et al.2016) at 155 FPS. Since YOLO sees the entire image when making predictions, it implicitly encodes contextual information about object classes, and is less likely to predict false positives in the background. YOLO makes more localization errors than Fast RCNN, resulting from the coarse division of bounding box location, scale and aspect ratio. As discussed in Redmon et al. (2016), YOLO may fail to localize some objects, especially small ones, pos- sibly because of the coarse grid division, and because each grid",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S98",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "cell can only contain one object. It is unclear to what extent YOLO can translate to good performance on datasets with many objects per image, such as MS COCO. YOLOv2 and YOLO9000 Redmon and Farhadi ( 2017) proposed YOLOv2, an improved version of YOLO, in which the custom GoogLeNet (Szegedy et al. 2015) network is replaced with the simpler DarkNet19, plus batch normal- ization (He et al.2015), removing the fully connected layers, and using good anchor boxes 8 learned via kmeans and multi- scale training. YOLOv2 achieved state-of-the-art on standard detection tasks. Redmon and Farhadi ( 2017) also introduced YOLO9000, which can detect over 9000 object categories in real time by proposing a joint optimization method to train simultaneously on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S99",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "an ImageNet classi\ufb01cation dataset and a COCO detection dataset with WordTree to combine data from multiple sources. Such joint training allows YOLO9000 to perform weakly supervised detection, i.e. detecting object classes that do not have bounding box annotations. SSD In order to preserve real-time speed without sacri\ufb01c- ing too much detection accuracy, Liu et al. ( 2016) proposed SSD (Single Shot Detector), faster than YOLO (Redmon et al.2016) and with an accuracy competitive with region- based detectors such as Faster RCNN (Ren et al. 2015). SSD effectively combines ideas from RPN in Faster RCNN (Ren et al.2015), YOLO (Redmon et al. 2016) and multiscale CONV features (Hariharan et al. 2016) to achieve fast detec- tion speed, while still retaining high",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S100",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "detection quality. Like YOLO, SSD predicts a \ufb01xed number of bounding boxes and scores, followed by an NMS step to produce the \ufb01nal detection. The CNN network in SSD is fully convolutional, whose early layers are based on a standard architecture, such 7 YOLO uses far fewer bounding boxes, only 98 per image, compared to about 2000 from Selective Search. 8 Boxes of various sizes and aspect ratios that serve as object candidates. as VGG (Simonyan and Zisserman 2015), followed by sev- eral auxiliary CONV layers, progressively decreasing in size. The information in the last layer may be too coarse spa- tially to allow precise localization, so SSD performs detection over multiple scales by operating on multiple CONV feature maps,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S101",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "each of which predicts category scores and box off- sets for bounding boxes of appropriate sizes. For a 300\u00d7300 input, SSD achieves 74 .3% mAP on the VOC2007 test at 59 FPS versus Faster RCNN 7 FPS / mAP 73 .2% or YOLO 45 FPS / mAP 63 .4%. CornerNet Recently, Law and Deng (2018) questioned the dominant role that anchor boxes have come to play in SoA object detection frameworks (Girshick2015;H ee ta l . 2017; Redmon et al. 2016; Liu et al. 2016). Law and Deng ( 2018) argue that the use of anchor boxes, especially in one stage detectors (Fu et al.2017; Lin et al. 2017b; Liu et al. 2016; Redmon et al. 2016), has drawbacks (Law",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S102",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "and Deng 2018; Lin et al. 2017b) such as causing a huge imbalance between positive and negative examples, slowing down training and introducing extra hyperparameters. Borrowing ideas from the work on Associative Embedding in multiperson pose estima- tion (Newell et al. 2017), Law and Deng ( 2018) proposed CornerNet by formulating bounding box object detection as detecting paired top-left and bottom-right keypoints 9.I n CornerNet, the backbone network consists of two stacked Hourglass networks (Newell et al.2016), with a simple cor- ner pooling approach to better localize corners. CornerNet achieved a 42 .1% AP on MS COCO, outperforming all pre- vious one stage detectors; however, the average inference time is about 4FPS on a Titan X GPU, signi\ufb01cantly slower than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S103",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "SSD (Liu et al. 2016) and YOLO (Redmon et al. 2016). CornerNet generates incorrect bounding boxes because it is challenging to decide which pairs of keypoints should be grouped into the same objects. To further improve on Cor- nerNet, Duan et al. ( 2019) proposed CenterNet to detect each object as a triplet of keypoints, by introducing one extra key- point at the centre of a proposal, raising the MS COCO AP to 47.0%, but with an inference speed slower than CornerNet. 6 Object Representation As one of the main components in any detector, good feature representations are of primary importance in object detection (Dickinson et al. 2009; Girshick et al. 2014; Gidaris and Komodakis 2015; Zhu et al. 2016a).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S104",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "In the past, a great deal of effort was devoted to designing local descriptors [e.g., SIFT (Lowe 1999) and HOG (Dalal and Triggs 2005)] and to explore approaches [e.g., Bag of Words (Sivic and Zisserman 2003) and Fisher V ector (Perronnin et al. 2010)] to group and 9 The idea of using keypoints for object detection appeared previously in DeNet (TychsenSmith and Petersson 2017). 123 International Journal of Computer Vision Fig. 15 Performance of winning entries in the ILSVRC competitions from 2011 to 2017 in the image classi\ufb01cation task",
      "page_hint": null,
      "token_count": 89,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S105",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "to allow the discriminative parts to emerge; however, these feature representation methods required careful engineering and considerable domain expertise. In contrast, deep learning methods (especially deep CNNs) can learn powerful feature representations with mul- tiple levels of abstraction directly from raw images (Bengio et al.2013; LeCun et al. 2015). As the learning procedure reduces the dependency of speci\ufb01c domain knowledge and complex procedures needed in traditional feature engineer- ing (Bengio et al. 2013; LeCun et al. 2015), the burden for feature representation has been transferred to the design of better network architectures and training procedures. The leading frameworks reviewed in Sect. 5 [RCNN (Gir- shick et al. 2014), Fast RCNN (Girshick 2015), Faster RCNN (Ren et al. 2015), YOLO (Redmon",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S106",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "et al. 2016), SSD (Liu et al. 2016)] have persistently promoted detection accuracy and speed, in which it is generally accepted that the CNN archi- tecture (Sect.6.1 and Fig. 15) plays a crucial role. As a result, most of the recent improvements in detection accuracy have been via research into the development of novel networks. Therefore we begin by reviewing popular CNN architectures used in Generic Object Detection, followed by a review of the effort devoted to improving object feature representa- tions, such as developing invariant features to accommodate geometric variations in object scale, pose, viewpoint, part deformation and performing multiscale analysis to improve object detection over a wide range of scales. 6.1 Popular CNN Architectures CNN architectures (Sect. 3)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S107",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "serve as network backbones used in the detection frameworks of Sect. 5. Representative frame- works include AlexNet (Krizhevsky et al. 2012b), ZFNet (Zeiler and Fergus 2014) VGGNet (Simonyan and Zisserman 2015), GoogLeNet (Szegedy et al. 2015), Inception series (Ioffe and Szegedy 2015; Szegedy et al. 2016, 2017), ResNet (He et al. 2016), DenseNet (Huang et al. 2017a) and SENet (Hu et al. 2018b), summarized in Table 6, and where the improvement over time is seen in Fig. 15. A further review of recent CNN advances can be found in Gu et al. ( 2018). The trend in architecture evolution is for greater depth: AlexNet has 8 layers, VGGNet 16 layers, more recently ResNet and DenseNet both surpassed the 100 layer",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S108",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "mark, and it was VGGNet (Simonyan and Zisserman 2015) and GoogLeNet (Szegedy et al. 2015) which showed that increas- ing depth can improve the representational power. As can be observed from Table 6, networks such as AlexNet, OverFeat, ZFNet and VGGNet have an enormous number of param- eters, despite being only a few layers deep, since a large fraction of the parameters come from the FC layers. Newer networks like Inception, ResNet, and DenseNet, although having a great depth, actually have far fewer parameters by avoiding the use of FC layers. With the use of Inception modules (Szegedy et al. 2015)i n carefully designed topologies, the number of parameters of GoogLeNet is dramatically reduced, compared to AlexNet, ZFNet or VGGNet.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S109",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "Similarly, ResNet demonstrated the effectiveness of skip connections for learning extremely deep networks with hundreds of layers, winning the ILSVRC 2015 classi\ufb01cation task. Inspired by ResNet (He et al.2016), InceptionResNets (Szegedy et al. 2017) combined the Incep- tion networks with shortcut connections, on the basis that shortcut connections can signi\ufb01cantly accelerate network training. Extending ResNets, Huang et al. ( 2017a) proposed DenseNets, which are built from dense blocksconnecting each layer to every other layer in a feedforward fashion, lead- ing to compelling advantages such as parameter ef\ufb01ciency, implicit deep supervision 10, and feature reuse. Recently, He et al. ( 2016) proposed Squeeze and Excitation (SE) blocks, which can be combined with existing deep architectures to boost their performance at minimal",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S110",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "additional computational cost, adaptively recalibrating channel-wise feature responses by explicitly modeling the interdependencies between con- volutional feature channels, and which led to winning the ILSVRC 2017 classi\ufb01cation task. Research on CNN archi- tectures remains active, with emerging networks such as Hourglass (Law and Deng 2018), Dilated Residual Networks (Y u et al. 2017), Xception (Chollet 2017) ,D e t N e t( L ie ta l . 2018b), Dual Path Networks (DPN) (Chen et al. 2017b), Fish- Net (Sun et al. 2018), and GLoRe (Chen et al. 2019b). 10 DenseNets perform deep supervision in an implicit way, i.e. individ- ual layers receive additional supervision from other layers through the shorter connections. The bene\ufb01ts of deep supervision have previously been demonstrated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S111",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "in Deeply Supervised Nets (DSN) (Lee et al.2015). 123 International Journal of Computer Vision Table 6 DCNN architectures that were commonly used for generic object detection No. DCNN architecture #Paras ( \u00d7106) #Layers (CONV+FC) Test error (Top 5) First used in Highlights 1 AlexNet (Krizhevsky et al. 2012b)5 7 5 + 21 5 .3% Girshick et al. ( 2014) The \ufb01rst DCNN found effective for ImageNet classi\ufb01cation; thehistorical turning point fromhand-crafted features to CNN;Winning the ILSVRC2012Image classi\ufb01cation competition 2 ZFNet (fast) (Zeiler and Fergus 2014)5 8 5 + 21 4 .8% He et al. ( 2014) Similar to AlexNet, different in stride for convolution, \ufb01lter size,and number of \ufb01lters for somelayers 3 OverFeat (Sermanet et al. 2014) 140 6 +",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S112",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "21 3 .6% Sermanet et al. ( 2014) Similar to AlexNet, different in stride for convolution, \ufb01lter size,and number of \ufb01lters for somelayers 4 VGGNet (Simonyan and Zisserman 2015) 134 13 + 26 .8% Girshick ( 2015) Increasing network depth signi\ufb01cantly by stacking 3 \u00d7 3 convolution \ufb01lters and increasingthe network depth step by step 5 GoogLeNet (Szegedy et al. 2015)6 2 2 6 .7% Szegedy et al. ( 2015) Use Inception module, which uses multiple branches ofconvolutional layers withdifferent \ufb01lter sizes and thenconcatenates feature mapsproduced by these branches. The\ufb01rst inclusion of bottleneckstructure and global average pooling 6 Inception v2 (Ioffe and Szegedy 2015)1 2 3 1 4 .8% Howard et al. ( 2017) Faster training with the introduce of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S113",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "batch normalization 7 I n c e p t i o nv 3( S z e g e d ye ta l . 2016)2 2 4 7 3 .6% Inclusion of separable convolution and spatial resolution reduction 8 YOLONet (Redmon et al. 2016)6 4 2 4 + 1 \u2212 Redmon et al. ( 2016) A network inspired by GoogLeNet used in YOLO detector 9 ResNet50 (He et al. 2016)2 3 .44 9 3 .6% (ResNets) He et al. ( 2016) With identity mapping, substantially deeper networks can be learned 123 International Journal of Computer VisionTable 6 continued No. DCNN architecture #Paras ( \u00d7106) #Layers (CONV+FC) Test error (Top 5) First used in Highlights 10 ResNet101 (He et al. 2016) 42",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S114",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "100 He et al. ( 2016) Requires fewer parameters than VGG by using the global averagepooling and bottleneckintroduced in GoogLeNet 11 InceptionResNet v1 (Szegedy et al. 2017)2 1 8 7 3 .1% (Ensemble) Combination of identity mapping and Inception module, withsimilar computational cost ofInception v3, but faster trainingprocess 12 InceptionResNet v2 Szegedy et al. ( 2017) 30 95 (Huang et al. 2017b) A costlier residual version of Inception, with signi\ufb01cantlyimproved recognitionperformance 13 Inception v4 Szegedy et al. ( 2017) 41 75 An Inception variant without residual connections, withroughly the same recognitionperformance as InceptionResNetv2, but signi\ufb01cantly slower 14 ResNeXt (Xie et al. 2017)2 3 4 9 3 .0% Xie et al. ( 2017) Repeating a building block that aggregates a set",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S115",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "oftransformations with the sametopology 15 DenseNet201 (Huang et al. 2017a) 18 200 \u2212 Zhou et al. ( 2018b) Concatenate each layer with every other layer in a feed forwardfashion. Alleviate the vanishing gradient problem, encourage feature reuse, reduction innumber of parameters 16 DarkNet (Redmon and Farhadi 2017)2 0 1 9 \u2212 Redmon and Farhadi ( 2017) Similar to VGGNet, but with signi\ufb01cantly fewer parameters 17 MobileNet (Howard et al. 2017)3 .22 7 + 1 \u2212 Howard et al. ( 2017) Light weight deep CNNs using depth-wise separableconvolutions 18 SE ResNet (Hu et al. 2018b)2 6 5 0 2 .3% (SENets) Hu et al. ( 2018b) Channel-wise attention by a novel block called Squeeze and Excitation. Complementary to existing backbone CNNs",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S116",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "Regarding the statistics for \u201c#Paras\u201d and \u201c#Layers\u201d, the \ufb01nal FC prediction layer is not taken into consideration. \u201cTest Error\u201d column indicates the Top 5 classi\ufb01cation test error on ImageNet1000. When ambiguous, the \u201c#Paras\u201d, \u201c#Layers\u201d, and \u201cTest Error\u201d refer to: OverFeat (accurate model), VGGNet16, ResNet101 DenseNet201 (Growth Rate 32, De nseNet-BC), ResNeXt50 (32*4d), and SE ResNet50 123 International Journal of Computer Vision The training of a CNN requires a large-scale labeled dataset with intraclass diversity. Unlike image classi\ufb01cation, detection requires localizing (possibly many) objects from an image. It has been shown (Ouyang et al. 2017b) that pretrain- ing a deep model with a large scale dataset having object level annotations (such as ImageNet), instead of only the image level annotations, improves",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S117",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "the detection performance. How- ever, collecting bounding box labels is expensive, especially for hundreds of thousands of categories. A common scenario is for a CNN to be pretrained on a large dataset (usually with a large number of visual categories) with image-level labels; the pretrained CNN can then be applied to a small dataset, directly, as a generic feature extractor (Razavian et al.2014; Azizpour et al. 2016; Donahue et al. 2014;Y o s i n s k ie ta l . 2014), which can support a wider range of visual recogni- tion tasks. For detection, the pre-trained network is typically \ufb01ne-tuned11 on a given detection dataset (Donahue et al. 2014; Girshick et al. 2014, 2016). Several large scale image classi\ufb01cation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S118",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "datasets are used for CNN pre-training, among them ImageNet1000 (Deng et al.2009; Russakovsky et al. 2015) with 1.2 million images of 1000 object categories, Places (Zhou et al. 2017a), which is much larger than Ima- geNet1000 but with fewer classes, a recent Places-Imagenet hybrid (Zhou et al. 2017a), or JFT300M (Hinton et al. 2015; Sun et al. 2017). Pretrained CNNs without \ufb01ne-tuning were explored for object classi\ufb01cation and detection in Donahue et al. ( 2014), Girshick et al. ( 2016), Agrawal et al. ( 2014), where it was shown that detection accuracies are different for features extracted from different layers; for example, for AlexNet pre- trained on ImageNet, FC6 / FC7 / Pool5 are in descending order of detection accuracy",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S119",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "abstract",
      "text": "(Donahue et al. 2014; Girshick et al. 2016). Fine-tuning a pre-trained network can increase detection performance signi\ufb01cantly (Girshick et al. 2014, 2016), although in the case of AlexNet, the \ufb01ne-tuning perfor- mance boost was shown to be much larger for FC6 / FC7 than for Pool5, suggesting that Pool5 features are more general. Furthermore, the relationship between the source and target datasets plays a critical role, for example that ImageNet based CNN features show better performance for object detection than for human action (Zhou et al. 2015; Azizpour et al. 2016).",
      "page_hint": null,
      "token_count": 91,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S120",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Deep CNN based detectors such as RCNN (Girshick et al. 2014), Fast RCNN (Girshick 2015), Faster RCNN (Ren et al. 2015) and YOLO (Redmon et al. 2016), typically use the deep CNN architectures listed in Table 6 as the backbone network 11 Fine-tuning is done by initializing a network with weights optimized for a large labeled dataset like ImageNet. and then updating the net- work\u2019s weights using the target-task training set. and use features from the top layer of the CNN as object rep- resentations; however, detecting objects across a large range of scales is a fundamental challenge. A classical strategy to address this issue is to run the detector over a number of scaled input images (e.g., an image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S121",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "pyramid) (Felzenszwalb et al.2010b; Girshick et al. 2014;H ee ta l . 2014), which typically produces more accurate detection, with, however, obvious limitations of inference time and memory. 6.2.1 Handling of Object Scale Variations Since a CNN computes its feature hierarchy layer by layer, the sub-sampling layers in the feature hierarchy already lead to an inherent multiscale pyramid, producing feature maps at different spatial resolutions, but subject to challenges (Hari- haran et al. 2016; Long et al. 2015; Shrivastava et al. 2017). In particular, the higher layers have a large receptive \ufb01eld and strong semantics, and are the most robust to variations such as object pose, illumination and part deformation, but the res- olution is low and the geometric details",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S122",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "are lost. In contrast, lower layers have a small receptive \ufb01eld and rich geomet- ric details, but the resolution is high and much less sensitive to semantics. Intuitively, semantic concepts of objects can emerge in different layers, depending on the size of the objects. So if a target object is small it requires \ufb01ne detail information in earlier layers and may very well disappear at later layers, in principle making small object detection very challenging, for which tricks such as dilated or \u201catrous\u201d con- volution (Y u and Koltun 2015; Dai et al. 2016c; Chen et al. 2018b) have been proposed, increasing feature resolution, but increasing computational complexity. On the other hand, if the target object is large, then the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S123",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "semantic concept will emerge in much later layers. A number of methods (Shrivas- tava et al.2017; Zhang et al. 2018e; Lin et al. 2017a; Kong et al. 2017) have been proposed to improve detection accu- racy by exploiting multiple CNN layers, broadly falling into three types of multiscale object detection: 1. Detecting with combined features of multiple layers; 2. Detecting at multiple layers; 3. Combinations of the above two methods. (1) Detecting with combined features of multiple CNN lay- ers Many approaches, including Hypercolumns (Hariharan et al. 2016), HyperNet (Kong et al. 2016), and ION (Bell et al. 2016), combine features from multiple layers before making a prediction. Such feature combination is commonly accomplished via concatenation, a classic neural network",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S124",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "idea that concatenates features from different layers, archi- tectures which have recently become popular for semantic segmentation (Long et al. 2015; Shelhamer et al. 2017;H a r - iharan et al. 2016). As shown in Fig. 16a, ION (Bell et al. 2016) uses RoI pooling to extract RoI features from multiple 123 International Journal of Computer Vision (a) (b) Fig. 16 Comparison of HyperNet and ION. LRN is local response nor- malization, which performs a kind of \u201clateral inhibition\u201d by normalizing over local input regions (Jia et al.2014) layers, and then the object proposals generated by selective search and edgeboxes are classi\ufb01ed by using the concatenated features. HyperNet (Kong et al.2016), shown in Fig. 16b, follows a similar idea, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S125",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "integrates deep, intermediate and shallow features to generate object proposals and to predict objects via an end to end joint training strategy. The com- bined feature is more descriptive, and is more bene\ufb01cial for localization and classi\ufb01cation, but at increased computational complexity. (2) Detecting at multiple CNN layers A number of recent",
      "page_hint": null,
      "token_count": 52,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S126",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "ent resolutions at different layers and then combining these predictions: SSD (Liu et al.2016) and MSCNN (Cai et al. 2016), RBFNet (Liu et al. 2018b), and DSOD (Shen et al. 2017). SSD (Liu et al. 2016) spreads out default boxes of different scales to multiple layers within a CNN, and forces each layer to focus on predicting objects of a certain scale. RFBNet (Liu et al. 2018b) replaces the later convolution lay- ers of SSD with a Receptive Field Block (RFB) to enhance the discriminability and robustness of features. The RFB is a multibranch convolutional block, similar to the Inception block (Szegedy et al. 2015), but combining multiple branches with different kernels and convolution layers (Chen et al. 2018b). MSCNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S127",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(Cai et al. 2016) applies deconvolution on multiple layers of a CNN to increase feature map resolution before using the layers to learn region proposals and pool fea- tures. Similar to RFBNet (Liu et al.2018b), TridentNet (Li et al. 2019b) constructs a parallel multibranch architecture where each branch shares the same transformation param- eters but with different receptive \ufb01elds; dilated convolution with different dilation rates are used to adapt the receptive \ufb01elds for objects of different scales. (3) Combinations of the above two methods Features from different layers are complementary to each other and can improve detection accuracy, as shown by Hypercolumns (Hariharan et al.2016), HyperNet (Kong et al. 2016) and ION (Bell et al. 2016). On the other hand,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S128",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "however, it is natural to detect objects of different scales using features of approximately the same size, which can be achieved by detecting large objects from downscaled feature maps while detecting small objects from upscaled feature maps. There- fore, in order to combine the best of both worlds, some recent works propose to detect objects at multiple layers, and the resulting features obtained by combining features from dif- ferent layers. This approach has been found to be effective for segmentation (Long et al.2015; Shelhamer et al. 2017) and human pose estimation (Newell et al. 2016), has been widely exploited by both one-stage and two-stage detec- tors to alleviate problems of scale variation across object instances. Representative methods include SharpMask (Pin-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S129",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "heiro et al. 2016), Deconvolutional Single Shot Detector (DSSD) (Fu et al. 2017), Feature Pyramid Network (FPN) (Lin et al. 2017a), Top Down Modulation (TDM)(Shrivastava et al. 2017), Reverse connection with Objectness prior Net- work (RON) (Kong et al. 2017), ZIP (Li et al. 2018a), Scale Transfer Detection Network (STDN) (Zhou et al. 2018b), Re\ufb01neDet (Zhang et al. 2018a), StairNet (Woo et al. 2018), Path Aggregation Network (PANet) (Liu et al. 2018c), Fea- ture Pyramid Recon\ufb01guration (FPR) (Kong et al. 2018), D e t N e t( L ie ta l .2018b), Scale Aware Network (SAN) (Kim et al. 2018), Multiscale Location aware Kernel Representa- tion (MLKP) (Wang et al. 2018) and M2Det (Zhao et al. 2019), as shown in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S130",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Table 7 and contrasted in Fig. 17. Early works like FPN (Lin et al. 2017a), DSSD (Fu et al. 2017), TDM (Shrivastava et al. 2017), ZIP (Li et al. 2018a), RON (Kong et al. 2017) and Re\ufb01neDet (Zhang et al. 2018a) construct the feature pyramid according to the inherent multi- scale, pyramidal architecture of the backbone, and achieved encouraging results. As can be observed from Fig.17a1\u2013 f1, these methods have very similar detection architectures which incorporate a top-down network with lateral connec- tions to supplement the standard bottom-up, feed-forward network. Speci\ufb01cally, after a bottom-up pass the \ufb01nal high level semantic features are transmitted back by the top-down network to combine with the bottom-up features from inter- mediate layers after lateral",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S131",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "processing, and the combined features are then used for detection. As can be seen from Fig. 17a2\u2013e2, the main differences lie in the design of the simple Feature Fusion Block (FFB), which handles the selec- tion of features from different layers and the combination of multilayer features. FPN (Lin et al. 2017a) shows signi\ufb01cant improvement as a generic feature extractor in several applications including object detection (Lin et al. 2017a, b) and instance segmen- tation (He et al. 2017). Using FPN in a basic Faster RCNN system achieved state-of-the-art results on the COCO detec- tion dataset. STDN (Zhou et al. 2018b) used DenseNet (Huang et al. 2017a) to combine features of different layers and designed a scale transfer module to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S132",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "obtain feature maps 123 International Journal of Computer Vision Table 7 Summary of properties of representative methods in improving DCNN feature representations for generic object detection Group Detector name Region proposal Backbone DCNN Pipelined used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO COCO (1) Single detection withmultilayerfeatures ION (Bell et al. 2016) SS+EB MCG+RPN VGG16 Fast RCNN 79 .4 (07+12) 76 .4 (07+12) 55 .73 3 .1 CVPR16 Use features from multiple layers; usespatial recurrentneural networks formodeling contextualinformation; the BestStudent Entry and the3rd overall in theCOCO detectionchallenge 2015 HyperNet (Kong et al. 2016) RPN VGG16 Faster RCNN 76 .3 (07+12) 71 .4 (07T+12) \u2212\u2212 CVPR16 Use features from multiple layers forboth region proposaland regionclassi\ufb01cation PV ANet (Kim et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S133",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "al. 2016) RPN PV ANet Faster RCNN 84.9 (07+12+CO) 84.2 (07T+12+CO) \u2212\u2212 NIPSW16 Deep but lightweight; Combine ideas fromconcatenated ReLU(Shang et al.2016), Inception (Szegedyet al.2015), and HyperNet (Kong et al.2016) 123 International Journal of Computer VisionTable 7 continued Group Detector name Region proposal Backbone DCNN Pipelined used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO COCO (2) Detection at multiple layers SDP+CRC (Yang et al. 2016b) EB VGG16 Fast RCNN 69 .4 (07) \u2212\u2212 \u2212 CVPR16 Use features in multiple layers to reject easynegatives via CRC,and then classifyremaining proposalsusing SDP MSCNN (Cai et al. 2016) RPN VGG Faster RCNN Only Tested on KITTI ECCV16 Region proposal and classi\ufb01cation areperformed at multiplelayers; includesfeature upsampling;end to end learning MPN (Zagoruykoet al.2016)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S134",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "SharpMask (Pinheiroet al.2016) VGG16 Fast RCNN \u2212\u2212 51.93 3 .2 BMVC16 Concatenate features from differentconvolutional layersand features ofdifferent contextualregions; loss functionfor multiple overlapthresholds; ranked 2ndin both the COCO15 detection and segmentationchallenges DSOD (Shen et al. 2017) Free DenseNet SSD 77 .7 (07+12) 72 .2 (07T+12) 47 .32 9 .3 ICCV17 Concatenate feature sequentially, likeDenseNet. Train fromscratch on the targetdataset withoutpre-training RFBNet (Liu et al. 2018b) Free VGG16 SSD 82 .2 (07+12) 81 .2 (07T+12) 55 .73 4 .4 ECCV18 Propose a multi-branch convolutional blocksimilar to Inception(Szegedy et al.2015), but using dilatedconvolution 123 International Journal of Computer Vision Table 7 continued Group Detector name Region proposal Backbone DCNN Pipelined used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO COCO (3)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S135",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Combination of (1) and (2) DSSD (Fu et al. 2017) Free ResNet101 SSD 81 .5 (07+12) 80 .0 (07T+12) 53 .33 3 .2 2017 Use Conv-Deconv, as shown in Fig. 17c1, c2 FPN (Lin et al. 2017a) RPN ResNet101 Faster RCNN \u2212\u2212 59.13 6 .2 CVPR17 Use Conv-Deconv, as shown in Fig. 17a1, a2; Widely used indetectors TDM (Shrivastavaet al.2017) RPN ResNet101 VGG16 Faster RCNN \u2212\u2212 57.73 6 .8 CVPR17 Use Conv-Deconv, as shown in Fig. 17b2 RON (Kong et al. 2017) RPN VGG16 Faster RCNN 81 .3 (07+12+CO) 80.7 (07T+12+CO) 49 .52 7 .4 CVPR17 Use Conv-deconv, as shown in Fig. 17d2; Add the objectnessprior to signi\ufb01cantlyreduce object searchspace ZIP (Li et al. 2018a) RPN Inceptionv2 Faster RCNN 79",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S136",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": ".8 (07+12) \u2212\u2212 \u2212 IJCV18 Use Conv-Deconv, as shown in Fig. 17f1. Propose a mapattention decision(MAD) unit forfeatures from differentlayers 123 International Journal of Computer VisionTable 7 continued Group Detector name Region proposal Backbone DCNN Pipelined used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO COCO STDN (Zhou et al. 2018b) Free DenseNet169 SSD 80 .9 (07+12) \u2212 51.03 1 .8 CVPR18 A new scale transfer module, which resizesfeatures of differentscales to the samescale in parallel Re\ufb01neDet (Zhang et al.2018a) RPN VGG16 ResNet101 Faster RCNN 83 .8 (07+12) 83 .5 (07T+12) 62 .94 1 .8 CVPR18 Use cascade to obtain better and lessanchors. UseConv-deconv, asshown in Fig.17e2 to improve features PANet (Liu et al. 2018c) RPN ResNeXt101 +FPN Mask",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S137",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "RCNN \u2212\u2212 67.2 47.4 CVPR18 Shown in Fig. 17g. Based on FPN, addanother bottom-uppath to passinformation betweenlower and topmostlayers; adaptivefeature pooling.Ranked 1st and 2nd inCOCO 2017 tasks DetNet (Li et al. 2018b) RPN DetNet59+FPN Faster RCNN \u2212\u2212 61.74 0 .2 ECCV18 Introduces dilated convolution into the ResNet backbone to maintain highresolution in deeperlayers; Shown inFig.17i FPR (Kong et al. 2018) \u2212 VGG16 ResNet101 SSD 82 .4 (07+12) 81 .1 (07T+12) 54 .33 4 .6 ECCV18 Fuse task oriented features acrossdifferent spatiallocations and scales,globally and locally;Shown in Fig.17h M2Det (Zhao et al. 2019) \u2212 SSD VGG16 ResNet101 \u2212\u2212 64.64 4 .2 AAAI19 Shown in Fig. 17j, newly designed topdown path to learn aset of multilevelfeatures, recombinedto construct a featurepyramid for objectdetection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S138",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "123 International Journal of Computer Vision Table 7 continued Group Detector name Region proposal Backbone DCNN Pipelined used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO COCO (4) Model geometrictransforms DeepIDNet (Ouyang et al.2015) SS+ EB AlexNet ZFNet OverFeatGoogLeNet RCNN 69 .0 (07) \u2212\u2212 25.6 CVPR15 Introduce a deformation constrained poolinglayer, jointly learnedwith convolutionallayers in existingDCNNs. Utilize thefollowing modulesthat are not trained endto end: cascade,context modeling,model averaging, andbounding box locationre\ufb01nement in themultistage detectionpipeline DCN (Dai et al. 2017) RPN ResNet101 IRN RFCN 82 .6 (07+12) \u2212 58.03 7 .5 CVPR17 Design deformable convolution anddeformable RoIpooling modules that can replace plain convolution in existingDCNNs DPFCN (Mordan et al. 2018) AttractioNet (Gidaris andKomodakis2016) ResNet RFCN 83 .3 (07+12) 81 .2 (07T+12)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S139",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "59 .13 9 .1 IJCV18 Design a deformable part based RoI poolinglayer to explicitlyselect discriminativeregions around objectproposals Details for Groups (1), (2), and (3) are provided in Sect. 6.2. Abbreviations: Selective Search (SS), EdgeBoxes (EB), InceptionResNet (IRN). Conv-Deconv denotes the use of upsampling and convolutional layers with lateral connections to supplement the standard backbone network. Detection results on VOC07, VOC12 and COCO were reporte d with mAP@IoU =0.5, and the additional COCO results are computed as the average of mAP for IoU thresholds from 0.5 to 0.95. Training data: \u201c07\u201d \u2190VOC2007 trainval; \u201c07T\u201d \u2190VOC2007 trainval and test; \u201c12\u201d \u2190VOC2012 trainval; CO\u2190 COCO trainval. The COCO detection results were reported with COCO2015 Test-Dev, except for MPN (Zagoruyko et al. 2016) which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S140",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "reported with COCO2015 Test-Standard 123 International Journal of Computer Vision Fig. 17 Hourglass architectures: Conv1 to Conv5 are the main Conv blocks in backbone networks such as VGG or ResNet. The \ufb01gure com- pares a number of feature fusion blocks (FFB) commonly used in recent approaches: FPN (Lin et al.2017a), TDM (Shrivastava et al. 2017), DSSD (Fu et al. 2017), RON (Kong et al. 2017), Re\ufb01neDet (Zhang et al. 2018a), ZIP (Li et al. 2018a), PANet (Liu et al. 2018c), FPR (Kong et al. 2018), DetNet (Li et al. 2018b) and M2Det (Zhao et al. 2019). FFM feature fusion module, TUM thinned U-shaped module 123 International Journal of Computer Vision with different resolutions. The scale transfer module can be directly",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S141",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "embedded into DenseNet with little additional cost. More recent work, such as PANet (Liu et al. 2018c), FPR (Kong et al. 2018), DetNet (Li et al. 2018b), and M2Det (Zhao et al. 2019), as shown in Fig. 17g\u2013j, propose to further improve on the pyramid architectures like FPN in different ways. Based on FPN, Liu et al. designed PANet (Liu et al. 2018c) (Fig. 17g1) by adding another bottom-up path with clean lateral connections from low to top levels, in order to shorten the information path and to enhance the feature pyramid. Then, an adaptive feature pooling was proposed to aggregate features from all feature levels for each proposal. In addition, in the proposal sub-network, a complementary branch capturing different",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S142",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "views for each proposal is cre- ated to further improve mask prediction. These additional steps bring only slightly extra computational overhead, but are effective and allowed PANet to reach 1st place in the COCO 2017 Challenge Instance Segmentation task and 2nd place in the Object Detection task. Kong et al. proposed FPR (Kong et al.2018) by explicitly reformulating the feature pyramid construction process [e.g. FPN (Lin et al. 2017a)] as feature recon\ufb01guration functions in a highly nonlinear but ef\ufb01cient way. As shown in Fig.17h1, instead of using a top- down path to propagate strong semantic features from the topmost layer down as in FPN, FPR \ufb01rst extracts features from multiple layers in the backbone network by adaptive concatenation, and then",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S143",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "designs a more complex FFB module (Fig. 17h2) to spread strong semantics to all scales. Li et al. (2018b) proposed DetNet (Fig. 17i1) by introducing dilated convolutions to the later layers of the backbone network in order to maintain high spatial resolution in deeper layers. Zhao et al. ( 2019) proposed a MultiLevel Feature Pyramid Network (MLFPN) to build more effective feature pyramids for detecting objects of different scales. As can be seen from Fig. 17j1, features from two different layers of the backbone are \ufb01rst fused as the base feature, after which a top-down path with lateral connections from the base feature is created to build the feature pyramid. As shown in Fig. 17j2, j5, the FFB module is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S144",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "much more complex than those like FPN, in that FFB involves a Thinned U-shaped Module (TUM) to generate a second pyramid structure, after which the feature maps with equivalent sizes from multiple TUMs are com- bined for object detection. The authors proposed M2Det by integrating MLFPN into SSD, and achieved better detection performance than other one-stage detectors. 6.3 Handling of Other Intraclass Variations Powerful object representations should combine distinctive- ness and robustness. A large amount of recent work has been devoted to handling changes in object scale, as reviewed in Sect. 6.2.1. As discussed in Sect. 2.2 and summarized in Fig. 5, object detection still requires robustness to real-world variations other than just scale, which we group into three categories:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S145",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "\u2022 Geometric transformations, \u2022 Occlusions, and \u2022 Image degradations. To handle these intra-class variations, the most straightfor- ward approach is to augment the training datasets with a suf\ufb01cient amount of variations; for example, robustness to rotation could be achieved by adding rotated objects at many orientations to the training data. Robustness can frequently be learned this way, but usually at the cost of expensive train- ing and complex model parameters. Therefore, researchers have proposed alternative solutions to these problems. Handling of geometric transformations DCNNs are inher- ently limited by the lack of ability to be spatially invariant to geometric transformations of the input data (Lenc and V edaldi2018; Liu et al. 2017; Chellappa 2016). The intro- duction of local max",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S146",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "pooling layers has allowed DCNNs to enjoy some translation invariance, however the intermediate feature maps are not actually invariant to large geometric transformations of the input data (Lenc and V edaldi2018). Therefore, many approaches have been presented to enhance robustness, aiming at learning invariant CNN representations with respect to different types of transformations such as scale (Kim et al.2014; Bruna and Mallat 2013), rotation (Bruna and Mallat 2013; Cheng et al. 2016; Worrall et al. 2017; Zhou et al. 2017b), or both (Jaderberg et al. 2015). One representative work is Spatial Transformer Network (STN) (Jaderberg et al. 2015), which introduces a new learnable module to handle scaling, cropping, rotations, as well as non- rigid deformations via a global parametric transformation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S147",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "STN has now been used in rotated text detection (Jaderberg et al. 2015), rotated face detection and generic object detec- tion (Wang et al. 2017). Although rotation invariance may be attractive in certain applications, such as scene text detection (He et al. 2018; Ma et al. 2018), face detection (Shi et al. 2018), and aerial imagery (Ding et al. 2018; Xia et al. 2018), there is limited generic object detection work focusing on rotation invariance because popular benchmark detection datasets (e.g. PAS- CAL VOC, ImageNet, COCO) do not actually present rotated images. Before deep learning, Deformable Part based Models (DPMs) (Felzenszwalb et al. 2010b) were successful for generic object detection, representing objects by compo- nent parts arranged in a deformable",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S148",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "con\ufb01guration. Although DPMs have been signi\ufb01cantly outperformed by more recent object detectors, their spirit still deeply in\ufb02uences many recent detectors. DPM modeling is less sensitive to transfor- mations in object pose, viewpoint and nonrigid deformations, motivating researchers (Dai et al.2017; Girshick et al. 2015; 123 International Journal of Computer Vision Mordan et al. 2018; Ouyang et al. 2015; Wan et al. 2015)t o explicitly model object composition to improve CNN based detection. The \ufb01rst attempts (Girshick et al.2015; Wan et al. 2015) combined DPMs with CNNs by using deep features learned by AlexNet in DPM based detection, but without region proposals. To enable a CNN to bene\ufb01t from the built- in capability of modeling the deformations of object parts, a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S149",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "number of approaches were proposed, including DeepIDNet (Ouyang et al. 2015), DCN (Dai et al. 2017) and DPFCN (Mordan et al. 2018) (shown in Table 7). Although simi- lar in spirit, deformations are computed in different ways: DeepIDNet (Ouyang et al. 2017b) designed a deformation constrained pooling layer to replace regular max pooling, to learn the shared visual patterns and their deformation prop- erties across different object classes; DCN (Dai et al. 2017) designed a deformable convolution layer and a deformable RoI pooling layer, both of which are based on the idea of augmenting regular grid sampling locations in feature maps; and DPFCN (Mordan et al. 2018) proposed a deformable part-based RoI pooling layer which selects discriminative parts of objects",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S150",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "around object proposals by simultaneously optimizing latent displacements of all parts. Handling of occlusions In real-world images, occlu- sions are common, resulting in information loss from object instances. A deformable parts idea can be useful for occlu- sion handling, so deformable RoI Pooling (Dai et al.2017; Mordan et al. 2018; Ouyang and Wang 2013) and deformable convolution (Dai et al. 2017) have been proposed to allevi- ate occlusion by giving more \ufb02exibility to the typically \ufb01xed geometric structures. Wang et al. (2017) propose to learn an adversarial network that generates examples with occlusions and deformations, and context may be helpful in dealing with occlusions (Zhang et al.2018b). Despite these efforts, the occlusion problem is far from being solved; applying GANs",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S151",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "to this problem may be a promising research direction. Handling of image degradations Image noise is a com- mon problem in many real-world applications. It is frequently caused by insuf\ufb01cient lighting, low quality cameras, image compression, or the intentional low-cost sensors on edge devices and wearable devices. While low image quality may be expected to degrade the performance of visual recogni- tion, most current methods are evaluated in a degradation free and clean environment, evidenced by the fact that PASCAL VOC, ImageNet, MS COCO and Open Images all focus on relatively high quality images. To the best of our knowledge, there is so far very limited work to address this problem. 7 Context Modeling In the physical world, visual objects",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S152",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "occur in particular envi- ronments and usually coexist with other related objects. There is strong psychological evidence (Biederman1972; Bar 2004) that context plays an essential role in human object recognition, and it is recognized that a proper mod- eling of context helps object detection and recognition (Torralba 2003; Oliva and Torralba 2007; Chen et al. 2018b, 2015a; Divvala et al. 2009; Galleguillos and Belongie 2010), especially when object appearance features are insuf\ufb01cient because of small object size, object occlusion, or poor image quality. Many different types of context have been discussed (Divvala et al. 2009; Galleguillos and Belongie 2010), and can broadly be grouped into one of three categories: 1. Semantic context: The likelihood of an object to be found",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S153",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "in some scenes, but not in others; 2. Spatial context: The likelihood of \ufb01nding an object in some position and not others with respect to other objects in the scene; 3. Scale context: Objects have a limited set of sizes relative to other objects in the scene. A great deal of work (Chen et al. 2015b; Divvala et al. 2009; Galleguillos and Belongie 2010; Malisiewicz and Efros 2009; Murphy et al. 2003; Rabinovich et al. 2007; Parikh et al. 2012) preceded the prevalence of deep learning, and much of this work has yet to be explored in DCNN-based object detectors (Chen and Gupta 2017;H ue ta l . 2018a). The current state of the art in object detection (Ren et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S154",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "al. 2015; Liu et al. 2016;H ee ta l . 2017) detects objects with- out explicitly exploiting any contextual information. It is broadly agreed that DCNNs make use of contextual informa- tion implicitly (Zeiler and Fergus 2014; Zheng et al. 2015) since they learn hierarchical representations with multiple levels of abstraction. Nevertheless, there is value in exploring contextual information explicitly in DCNN based detectors (Hu et al. 2018a; Chen and Gupta 2017; Zeng et al. 2017), so the following reviews recent work in exploiting contextual cues in DCNN- based object detectors, organized into cate- gories of global and local contexts, motivated by earlier work in Zhang et al. ( 2013), Galleguillos and Belongie ( 2010). Representative approaches are summarized in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S155",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Table 8. 7.1 Global Context Global context (Zhang et al. 2013; Galleguillos and Belongie 2010) refers to image or scene level contexts, which can serve as cues for object detection (e.g., a bedroom will predict the presence of a bed). In DeepIDNet (Ouyang et al. 2015), the image classi\ufb01cation scores were used as contextual features, and concatenated with the object detection scores to improve detection results. In ION (Bell et al. 2016), Bell et al. pro- posed to use spatial Recurrent Neural Networks (RNNs) to explore contextual information across the entire image. In SegDeepM (Zhu et al. 2015), Zhu et al. proposed a Markov random \ufb01eld model that scores appearance as well as context 123 International Journal of Computer Vision",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S156",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Table 8 Summary of detectors that exploit context information, with labelling details as in Table 7 Group Detector name Region proposal Backbone DCNN Pipelined Used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO Global context SegDeepM (Zhu et al. 2015) SS+CMPC VGG16 RCNN VOC10 VOC12 \u2212 CVPR15 Additional features extracted from an enlarged objectproposal as contextinformation DeepIDNet (Ouyang et al.2015) SS+EB AlexNet ZFNet RCNN 69 .0 (07) \u2212\u2212 CVPR15 Use image classi\ufb01cation scores as globalcontextual informationto re\ufb01ne the detectionscores of each objectproposal ION (Bell et al. 2016) SS+EB VGG16 Fast RCNN 80 .17 7 .93 3 .1 CVPR16 The contextual information outsidethe region of interest isintegrated usingspatial recurrentneural networks CPF (Shrivastava and Gupta2016) RPN VGG16 Faster RCNN 76 .4",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S157",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(07+12) 72 .6 (07T+12) \u2212 ECCV16 Use semantic segmentation toprovide top-downfeedback Local context MRCNN (Gidaris andKomodakis2015) SS VGG16 SPPNet 78 .2 (07+12) 73 .9 (07+12) \u2212 ICCV15 Extract features from multiple regionssurrounding or insidethe object proposals.Integrate the semanticsegmentation-awarefeatures GBDNet (Zeng et al. 2016, 2017) CRAFT (Yang et al. 2016a) Inception v2 ResNet269PolyNet(Zhang et al.2017) Fast RCNN 77 .2 (07+12) \u2212 27.0 ECCV16 TPAMI18 A GBDNet module to learn the relations ofmultiscalecontextualized regionssurrounding an objectproposal; GBDNetpasses messagesamong features fromdifferent context regions through convolution betweenneighboring supportregions in twodirections 123 International Journal of Computer VisionTable 8 continued Group Detector name Region proposal Backbone DCNN Pipelined Used mAP@IoU =0.5 mAP Published in Highlights VOC07 VOC12 COCO ACCNN (Li et al. 2017b) SS VGG16 Fast",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S158",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "RCNN 72 .0 (07+12) 70 .6 (07T+12) \u2212 TMM17 Use LSTM to capture global context.Concatenate featuresfrom multi-scalecontextual regionssurrounding an objectproposal. The globaland local contextfeatures areconcatenated forrecognition CoupleNet (Zhu et al. 2017a) RPN ResNet101 RFCN 82.7 (07+12) 80.4 (07T+12) 34 .4 ICCV17 Concatenate features from multiscalecontextual regionssurrounding an objectproposal. Features ofdifferent contextualregions are thencombined byconvolution andelement-wise sum SMN (Chen and Gupta 2017) RPN VGG16 Faster RCNN 70 .0 (07) \u2212\u2212 ICCV17 Model object-object relationshipsef\ufb01ciently through aspatial memorynetwork. Learn thefunctionality of NMS automatically ORN (Hu et al. 2018a) RPN ResNet101 +DCN Faster RCNN \u2212\u2212 39.0 CVPR18 Model the relations of a set of object proposals through theinteractions betweentheir appearancefeatures and geometry.Learn the functionalityof NMS automatically SIN (Liu et al. 2018d) RPN VGG16",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S159",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Faster RCNN 76 .0 (07+12) 73 .1 (07T+12) 23 .2 CVPR18 Formulate object detection asgraph-structuredinference, whereobjects are graphnodes andrelationships the edges 123 International Journal of Computer Vision for each detection, and allows each candidate box to select a segment out of a large pool of object segmentation proposals and score the agreement between them. In Shrivastava and Gupta ( 2016), semantic segmentation was used as a form of contextual priming. 7.2 Local Context Local context (Zhang et al. 2013; Galleguillos and Belongie 2010; Rabinovich et al. 2007) considers the relationship among locally nearby objects, as well as the interactions between an object and its surrounding area. In general, mod- eling object relations is challenging, requiring reasoning about bounding boxes of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S160",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "different classes, locations, scales etc. Deep learning research that explicitly models object rela- tions is quite limited, with representative ones being Spatial Memory Network (SMN) (Chen and Gupta2017), Object Relation Network (Hu et al. 2018a), and Structure Inference Network (SIN) (Liu et al. 2018d). In SMN, spatial memory essentially assembles object instances back into a pseudo image representation that is easy to be fed into another CNN for object relations reasoning, leading to a new sequential reasoning architecture where image and memory are pro- cessed in parallel to obtain detections which further update memory. Inspired by the recent success of attention mod- ules in natural language processing (V aswani et al. 2017), ORN processes a set of objects simultaneously through",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S161",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "the interaction between their appearance feature and geometry. It does not require additional supervision, and it is easy to embed into existing networks, effective in improving object recognition and duplicate removal steps in modern object detection pipelines, giving rise to the \ufb01rst fully end-to-end object detector. SIN (Liu et al.2018d) considered two kinds of context: scene contextual information and object relation- ships within a single image. It formulates object detection as a problem of graph inference, where the objects are treated as nodes in a graph and relationships between objects are modeled as edges. A wider range of methods has approached the con- text challenge with a simpler idea: enlarging the detec- tion window size to extract some form of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S162",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "local context. Representative approaches include MRCNN (Gidaris and Komodakis2015), Gated BiDirectional CNN (GBDNet) Zeng et al. ( 2016), Zeng et al. ( 2017), Attention to Con- text CNN (ACCNN) (Li et al. 2017b), CoupleNet (Zhu et al. 2017a), and Sermanet et al. ( 2013). In MRCNN (Gidaris and Komodakis 2015) (Fig. 18a), in addition to the features extracted from the original object proposal at the last CONV layer of the backbone, Gidaris and Komodakis proposed to extract features from a number of different regions of an object proposal (half regions, border regions, central regions, contextual region and semantically segmented regions), in order to obtain a richer and more robust object representa- tion. All of these features are combined by concatenation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S163",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Quite a number of methods, all closely related to MRCNN, have been proposed since then. The method in Zagoruyko et al. (2016) used only four contextual regions, organized in a foveal structure, where the classi\ufb01ers along multiple paths are trained jointly end-to-end. Zeng et al. ( 2016), Zeng et al. (2017) proposed GBDNet (Fig. 18b) to extract features from multiscale contextualized regions surrounding an object pro- posal to improve detection performance. In contrast to the somewhat naive approach of learning CNN features for each region separately and then concatenating them, GBDNet passes messages among features from different contextual regions. Noting that message passing is not always helpful, but dependent on individual samples, Zeng et al. (2016)u s e d gated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S164",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "functions to control message transmission. Li et al. (2017b) presented ACCNN (Fig. 18c) to utilize both global and local contextual information: the global context was captured using a Multiscale Local Contextualized (MLC) subnetwork, which recurrently generates an attention map for an input image to highlight promising contextual locations; local context adopted a method similar to that of MRCNN (Gidaris and Komodakis 2015). As shown in Fig. 18d, Cou- pleNet (Zhu et al. 2017a) is conceptually similar to ACCNN ( L ie ta l . 2017b), but built upon RFCN (Dai et al. 2016c), which captures object information with position sensitive RoI pooling, CoupleNet added a branch to encode the global context with RoI pooling. 8 Detection Proposal Methods An object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S165",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "can be located at any position and scale in an image. During the heyday of handcrafted feature descrip- tors [SIFT (Lowe2004), HOG (Dalal and Triggs 2005) and LBP (Ojala et al. 2002)], the most successful methods for object detection [e.g. DPM (Felzenszwalb et al. 2008)] used sliding window techniques (Viola and Jones 2001; Dalal and Triggs 2005; Felzenszwalb et al. 2008; Harzallah et al. 2009; V edaldi et al. 2009). However, the number of windows is huge, growing with the number of pixels in an image, and the need to search at multiple scales and aspect ratios further increases the search space 12. Therefore, it is computationally too expensive to apply sophisticated classi\ufb01ers. Around 2011, researchers proposed to relieve the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S166",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "tension between computational tractability and high detection qual- 12 Sliding window based detection requires classifying around 10 4\u2013 105 windows per image. The number of windows grows signi\ufb01cantly to 10 6\u2013107 windows per image when considering multiple scales and aspect ratios. 123 International Journal of Computer Vision (a) (b) (c) (d) Fig. 18 Representative approaches that explore local surrounding contextual features: MRCNN (Gidaris and Komodakis 2015), GBDNet (Zeng et al. 2016, 2017), ACCNN (Li et al. 2017b) and CoupleNet (Zhu et al. 2017a); also see Table 8 ity by using detection proposals13 (V an de Sande et al. 2011; Uijlings et al. 2013). Originating in the idea of objectness proposed by Alexe et al. ( 2010), object proposals are a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S167",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "set of candidate regions in an image that are likely to contain objects, and if high object recall can be achieved with a mod- est number of object proposals (like one hundred), signi\ufb01cant speed-ups over the sliding window approach can be gained, allowing the use of more sophisticated classi\ufb01ers. Detection proposals are usually used as a pre-processing step, limit- ing the number of regions that need to be evaluated by the detector, and should have the following characteristics: 1. High recall, which can be achieved with only a few pro- posals; 2. Accurate localization, such that the proposals match the object bounding boxes as accurately as possible; and 3. Low computational cost. The success of object detection based on detection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S168",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "proposals (V an de Sande et al. 2011; Uijlings et al. 2013) has attracted broad interest (Carreira and Sminchisescu 2012; Arbel\u00e1ez et al. 2014; Alexe et al. 2012; Cheng et al. 2014; Zitnick and Doll\u00e1r 2014; Endres and Hoiem 2010; Kr\u00e4henb\u00fchl and Koltun 2014; Manen et al. 2013). A comprehensive review of object proposal algorithms is beyond the scope of this 13 We use the terminology detection proposals , object proposals and region proposals interchangeably. paper, because object proposals have applications beyond object detection (Arbel\u00e1ez et al. 2012; Guillaumin et al. 2014; Zhu et al. 2017b). We refer interested readers to the recent surveys (Hosang et al. 2016; Chavali et al. 2016) which provide in-depth analysis of many classical object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S169",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "proposal algorithms and their impact on detection performance. Our interest here is to review object proposal methods that are based on DCNNs, output class agnostic proposals, and are related to generic object detection. In 2014, the integration of object proposals (V an de Sande et al. 2011; Uijlings et al. 2013) and DCNN features (Krizhevsky et al. 2012a) led to the milestone RCNN (Gir- shick et al. 2014) in generic object detection. Since then, detection proposal has quickly become a standard prepro- cessing step, based on the fact that all winning entries in the PASCAL VOC (Everingham et al. 2010), ILSVRC (Rus- sakovsky et al. 2015) and MS COCO (Lin et al. 2014) object detection challenges since 2014 used detection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S170",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "proposals (Girshick et al. 2014; Ouyang et al. 2015; Girshick 2015; Ren et al. 2015; Zeng et al. 2017;H ee ta l . 2017). Among object proposal approaches based on traditional low-level cues (e.g., color, texture, edge and gradients), Selective Search (Uijlings et al.2013), MCG (Arbel\u00e1ez et al. 2014) and EdgeBoxes (Zitnick and Doll\u00e1r 2014) are among the more popular. As the domain rapidly progressed, tra- ditional object proposal approaches (Uijlings et al. 2013; Hosang et al. 2016; Zitnick and Doll\u00e1r 2014), which were adopted as external modules independent of the detectors, 123 International Journal of Computer Vision became the speed bottleneck of the detection pipeline (Ren et al. 2015). An emerging class of object proposal algorithms (Erhan et al.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S171",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "2014; Ren et al. 2015; Kuo et al. 2015; Ghodrati et al. 2015; Pinheiro et al. 2015; Yang et al. 2016a)u s i n g DCNNs has attracted broad attention. Recent DCNN based object proposal methods generally fall into two categories: bounding box based and object segment based, with representative methods summarized in Table 9. Bounding Box Proposal Methods are best exempli\ufb01ed by the RPC method of Ren et al. ( 2015), illustrated in Fig. 19. RPN predicts object proposals by sliding a small network over the feature map of the last shared CONV layer. At each sliding window location, k proposals are predicted by using k anchor boxes, where each anchor box 14 is centered at some location in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S172",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "the image, and is associated with a particular scale and aspect ratio. Ren et al. (2015) proposed integrating RPN and Fast RCNN into a single network by sharing their convo- lutional layers, leading to Faster RCNN, the \ufb01rst end-to-end detection pipeline. RPN has been broadly selected as the proposal method by many state-of-the-art object detectors, as can be observed from Tables 7 and 8. Instead of \ufb01xing ap r i o r i a set of anchors as MultiBox (Erhan et al. 2014; Szegedy et al. 2014) and RPN (Ren et al. 2015), Lu et al. ( 2016) proposed generating anchor locations by using a recursive search strategy which can adaptively guide computational resources to focus on sub-regions likely to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S173",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "contain objects. Starting with the whole image, all regions visited during the search process serve as anchors. For any anchor region encountered during the search procedure, a scalar zoom indicator is used to decide whether to further par- tition the region, and a set of bounding boxes with objectness scores are computed by an Adjacency and Zoom Network (AZNet), which extends RPN by adding a branch to com- pute the scalar zoom indicator in parallel with the existing branch. Further work attempts to generate object proposals by exploiting multilayer convolutional features. Concurrent with RPN (Ren et al. 2015), Ghodrati et al. ( 2015)p r o - posed DeepProposal, which generates object proposals by using a cascade of multiple convolutional features,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S174",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "building an inverse cascade to select the most promising object loca- tions and to re\ufb01ne their boxes in a coarse-to-\ufb01ne manner. An improved variant of RPN, HyperNet (Kong et al. 2016) designs Hyper Features which aggregate multilayer convolu- tional features and shares them both in generating proposals and detecting objects via an end-to-end joint training strat- egy. Yang et al. ( 2016a) proposed CRAFT which also used a cascade strategy, \ufb01rst training an RPN network to generate object proposals and then using them to train another binary Fast RCNN network to further distinguish objects from back- 14 The concept of \u201canchor\u201d \ufb01rst appeared in Ren et al. ( 2015). ground. Li et al. ( 2018a) proposed ZIP to improve RPN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S175",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "by predicting object proposals with multiple convolutional fea- ture maps at different network depths to integrate both low level details and high level semantics. The backbone used in ZIP is a \u201czoom out and in\u201d network inspired by the conv and deconv structure (Long et al.2015). Finally, recent work which deserves mention includes Deepbox (Kuo et al. 2015), which proposed a lightweight CNN to learn to rerank proposals generated by EdgeBox, and DeNet (TychsenSmith and Petersson2017) which introduces bounding box corner estimation to predict object proposals ef\ufb01ciently to replace RPN in a Faster RCNN style detector. Object Segment Proposal Methods Pinheiro et al. ( 2015), Pinheiro et al. ( 2016) aim to generate segment proposals that are likely to correspond",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S176",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "to objects. Segment proposals are more informative than bounding box proposals, and take a step further towards object instance segmentation (Hariha- ran et al. 2014; Dai et al. 2016b;L ie ta l . 2017e). In addition, using instance segmentation supervision can improve the per- formance of bounding box object detection. The pioneering work of DeepMask, proposed by Pinheiro et al. ( 2015), seg- ments proposals learnt directly from raw image data with a deep network. Similarly to RPN, after a number of shared convolutional layers DeepMask splits the network into two branches in order to predict a class agnostic mask and an associated objectness score. Also similar to the ef\ufb01cient slid- ing window strategy in OverFeat (Sermanet et al.2014), the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S177",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "trained DeepMask network is applied in a sliding win- dow manner to an image (and its rescaled versions) during inference. More recently, Pinheiro et al. (2016) proposed SharpMask by augmenting the DeepMask architecture with a re\ufb01nement module, similar to the architectures shown in Fig.17 (b1) and (b2), augmenting the feed-forward net- work with a top-down re\ufb01nement process. SharpMask can ef\ufb01ciently integrate spatially rich information from early fea- tures with strong semantic information encoded in later layers to generate high \ufb01delity object masks. Motivated by Fully Convolutional Networks (FCN) for semantic segmentation (Long et al. 2015) and DeepMask (Pinheiro et al. 2015; Dai et al. 2016a) proposed Instance- FCN to generate instance segment proposals. Similar to DeepMask, the InstanceFCN network is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S178",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "split into two fully convolutional branches, one to generate instance sensitive score maps, the other to predict the objectness score. Hu et al. (2017) proposed FastMask to ef\ufb01ciently generate instance segment proposals in a one-shot manner, similar to SSD (Liu et al.2016), in order to make use of multiscale convolutional features. Sliding windows extracted densely from multiscale convolutional feature maps were input to a scale-tolerant attentional head module in order to predict segmentation masks and objectness scores. FastMask is claimed to run at 13 FPS on 800 \u00d7 600 images. 123 International Journal of Computer VisionTable 9 Summary of object proposal methods using DCNN. Bold values indicates the number of object proposals Proposer name Backbone network Detector tested Recall@IoU (VOC07)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S179",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Detection results (mAP) Published in Highlights 0.50 .70 .9 VOC07 VOC12 COCO Bounding box object proposal methods MultiBox1 (Erhan et al. 2014) AlexNet RCNN \u2212\u2212\u2212 29.0( 10) (12) \u2212\u2212 CVPR14 Learns a class agnostic regressor on a small set of 800 prede\ufb01nedanchor boxes. Do notshare features fordetection DeepBox (Kuo et al. 2015) VGG16 Fast RCNN 0 .96 ( 1000)0 .84 ( 1000)0 .15 ( 1000) \u2212\u2212 37.8( 500) (IoU@0.5) ICCV15 Use a lightweight CNN to learn to rerankproposals generatedby EdgeBox. Can runat 0.26s per image. Donot share features fordetection RPN (Ren et al. 2015, 2017) VGG16 Faster RCNN 0 .97 ( 300) 0.98 ( 1000) 0.79 ( 300) 0.84 ( 1000) 0.04 ( 300) 0.04 ( 1000) 73.2( 300) (07+12)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S180",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "70.4( 300) (07++12) 21.9( 300) NIPS15 The \ufb01rst to generate object proposals bysharing full imageconvolutional featureswith detection. Mostwidely used objectproposal method.Signi\ufb01cantimprovements indetection speed DeepProposal (Ghodrati et al.2015) VGG16 Fast RCNN 0 .74 ( 100) 0.92 ( 1000) 0.58 ( 100) 0.80 ( 1000) 0.12 ( 100) 0.16 ( 1000) 53.2( 100) (07) \u2212\u2212 ICCV15 Generate proposals inside a DCNN in amultiscale manner.Share features with thedetection network CRAFT (Yang et al. 2016a) VGG16 Faster RCNN 0 .98 ( 300)0 .90 ( 300)0 .13 ( 300)7 5 .7 (07+12) 71.3 (12) \u2212 CVPR16 Introduced a classi\ufb01cation network(i.e. two class FastRCNN) cascade thatcomes after the RPN.Not sharing featuresextracted for detection 123 International Journal of Computer Vision Table 9 continued Proposer name Backbone network",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S181",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Detector tested Recall@IoU (VOC07) Detection results (mAP) Published in Highlights 0.50 .70 .9 VOC07 VOC12 COCO AZNet (Lu et al. 2016) VGG16 Fast RCNN 0 .91 ( 300)0 .71 ( 300)0 .11 ( 300)7 0 .4 (07) \u2212 22.3 CVPR16 Use coarse-to-\ufb01ne search: start from largeregions, thenrecursively search forsubregions that maycontain objects.Adaptively guidecomputationalresources to focus onlikely subregions ZIP (Li et al. 2018a) Inception v2 Faster RCNN 0 .85 ( 300) COCO 0.74 ( 300) COCO 0.35 ( 300) COCO 79.8 (07+12) \u2212\u2212 IJCV18 Generate proposals using conv-deconvnetwork withmultilayers; Proposeda map attentiondecision (MAD) unitto assign the weightsfor features fromdifferent layers DeNet (TychsenSmithand Petersson2017) ResNet101 Fast RCNN 0 .82 ( 300)0 .74 ( 300)0 .48 ( 300)7 7 .1 (07+12) 73.9 (07++12)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S182",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "33 .8 ICCV17 A lot faster than Faster RCNN; Introduces abounding box cornerestimation forpredicting objectproposals ef\ufb01ciently toreplace RPN; Does not require prede\ufb01ned anchors 123 International Journal of Computer Vision Table 9 continued Proposer name Backbone network Detector tested Box proposals (AR, COCO) Segment proposals (AR, COCO) Published in Highlights Segment proposal methods DeepMask (Pinheiro et al.2015) VGG16 Fast RCNN 0 .33 ( 100), 0 .48 ( 1000)0 .26 ( 100), 0 .37 ( 1000) NIPS15 First to generate object mask proposals withDCNN; Slowinference time; Needsegmentationannotations fortraining; Not sharingfeatures with detectionnetwork; AchievedmAP of 69.9% ( 500) with Fast RCNN InstanceFCN ( D a ie ta l .2016a) VGG16 \u2212\u2212 0.32 ( 100), 0 .39 ( 1000) ECCV16 Combines ideas of FCN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S183",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(Long et al. 2015)a n d DeepMask (Pinheiroet al.2015). Introduces instancesensitive score maps.Needs segmentationannotations to train thenetwork SharpMask (Pinheiro et al.2016) MPN (Zagoruykoet al.2016) Fast RCNN 0 .39 ( 100), 0 .53 ( 1000)0 .30 ( 100), 0 .39 ( 1000) ECCV16 Leverages features at multiple convolutionallayers by introducing atop-down re\ufb01nementmodule. Does notshare features withdetection network. Needs segmentation annotations fortraining FastMask (Hu et al. 2017) ResNet39 \u2212 0.43 ( 100), 0 .57 ( 1000)0 .32 ( 100), 0 .41 ( 1000) CVPR17 Generates instance segment proposalsef\ufb01ciently in one-shotmanner similar to SSD(Liu et al.2016). Uses multiscale convolutional features.Uses segmentationannotations fortraining The detection results on COCO are based on mAP@IoU[0.5, 0.95], unless stated otherwise 123 International Journal of Computer Vision Fig. 19",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S184",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Illustration of the region proposal network (RPN) introduced in Ren et al. ( 2015) 9 Other Issues Data Augmentation Performing data augmentation for learn- ing DCNNs (Chat\ufb01eld et al. 2014; Girshick 2015; Girshick et al. 2014) is generally recognized to be important for visual recognition. Trivial data augmentation refers to perturbing an image by transformations that leave the underlying cate- gory unchanged, such as cropping, \ufb02ipping, rotating, scaling, translating, color perturbations, and adding noise. By arti\ufb01- cially enlarging the number of samples, data augmentation helps in reducing over\ufb01tting and improving generalization. It can be used at training time, at test time, or both. Never- theless, it has the obvious limitation that the time required for training increases signi\ufb01cantly. Data augmentation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S185",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "may synthesize completely new training images (Peng et al. 2015; Wang et al. 2017), however it is hard to guarantee that the syn- thetic images generalize well to real ones. Some researchers (Dwibedi et al. 2017; Gupta et al. 2016) proposed augment- ing datasets by pasting real segmented objects into natural images; indeed, Dvornik et al. (2018) showed that appro- priately modeling the visual context surrounding objects is crucial to place them in the right environment, and proposed a context model to automatically \ufb01nd appropriate locations on images to place new objects for data augmentation. Novel Training Strategies Detecting objects under a wide range of scale variations, especially the detection of very small objects, stands out as a key challenge.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S186",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "It has been shown (Huang et al. 2017b; Liu et al. 2016) that image resolution has a considerable impact on detection accuracy, therefore scaling is particularly commonly used in data augmentation, since higher resolutions increase the possibility of detecting small objects (Huang et al. 2017b). Recently, Singh et al. proposed advanced and ef\ufb01cient data argumentation meth- ods SNIP (Singh and Davis 2018) and SNIPER (Singh et al. 2018b) to 1 illustrate the scale invariance problem, as sum- marized in Table 10. Motivated by the intuitive understanding that small and large objects are dif\ufb01cult to detect at smaller and larger scales, respectively, SNIP introduces a novel train- ing scheme that can reduce scale variations during training, but without reducing training samples;",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S187",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "SNIPER allows for ef\ufb01cient multiscale training, only processing context regions around ground truth objects at the appropriate scale, instead of processing a whole image pyramid. Peng et al. ( 2018) studied a key factor in training, the minibatch size, and proposed MegDet, a Large MiniBatch Object Detector, to enable the training with a much larger minibatch size than before (from 16 to 256). To avoid the failure of convergence and signi\ufb01cantly speed up the training process, Peng et al. (2018) proposed a learning rate policy and Cross GPU Batch Normalization, and effectively utilized 128 GPUs, allowing MegDet to \ufb01nish COCO training in 4 hours on 128 GPUs, and winning the COCO 2017 Detection Challenge. Reducing Localization Error In object detection,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S188",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "the Inter- section Over Union 15 (IOU) between a detected bounding box and its ground truth box is the most popular evalua- tion metric, and an IOU threshold (e.g. typical value of 0.5) is required to de\ufb01ne positives and negatives. From Fig. 13, in most state of the art detectors (Girshick 2015; Liu et al. 2016;H ee ta l . 2017; Ren et al. 2015; Redmon et al. 2016) object detection is formulated as a multitask learning prob- lem, i.e., jointly optimizing a softmax classi\ufb01er which assigns object proposals with class labels and bounding box regres- sors, localizing objects by maximizing IOU or other metrics between detection results and ground truth. Bounding boxes are only a crude approximation for articulated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S189",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "objects, con- sequently background pixels are almost invariably included in a bounding box, which affects the accuracy of classi\ufb01- cation and localization. The study in Hoiem et al. ( 2012) shows that object localization error is one of the most in\ufb02u- ential forms of error, in addition to confusion between similar objects. Localization error could stem from insuf\ufb01cient over- lap (smaller than the required IOU threshold, such as the green box in Fig. 20) or duplicate detections (i.e., multiple overlapping detections for an object instance). Usually, some post-processing step like NonMaximum Suppression (NMS) (Bodla et al.2017; Hosang et al. 2017) is used for eliminat- ing duplicate detections. However, due to misalignments the bounding box with better localization could be suppressed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S190",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "during NMS, leading to poorer localization quality (such as the purple box shown in Fig. 20). Therefore, there are quite a few methods aiming at improving detection performance by reducing localization error. MRCNN (Gidaris and Komodakis 2015) introduces iter- ative bounding box regression, where an RCNN is applied several times. CRAFT (Yang et al.2016a) and AttractioNet (Gidaris and Komodakis 2016) use a multi-stage detection sub-network to generate accurate proposals, to forward to Fast RCNN. Cai and V asconcelos ( 2018) proposed Cas- cade RCNN, a multistage extension of RCNN, in which a sequence of detectors is trained sequentially with increasing 15 Please refer to Sect. 4.2 for more details on the de\ufb01nition of IOU. 123 International Journal of Computer VisionTable",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S191",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "10 Representative methods for training strategies and class imbalance handling Detector name Region proposal Backbone DCNN Pipelined used VOC07 results VOC12 results COCO results Published in Highlights MegDet (Peng et al. 2018) RPN ResNet50+FPN FasterRCNN \u2212\u2212 52.5 CVPR18 Allow training with much larger minibatchsize than before byintroducing cross GPUbatch normalization;Can \ufb01nish the COCOtraining in 4 hours on128 GPUs andachieved improvedaccuracy; WonCOCO2017 detectionchallenge SNIP (Singh et al. 2018b) RPN DPN (Chen et al. 2017b)+ D C N (Dai et al.2017) RFCN \u2212\u2212 48.3 CVPR18 A new multiscale training scheme.Empirically examinedthe effect ofup-sampling for smallobject detection. During training, only select objects that \ufb01tthe scale of features aspositive samples SNIPER (Singh et al. 2018b) RPN ResNet101+DCN Faster RCNN \u2212\u2212 47.6 2018 An ef\ufb01cient",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S192",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "multiscale training strategy.Process contextregions aroundground-truth instancesat the appropriate scale OHEM (Shrivastavaet al.2016) SS VGG16 Fast RCNN 78 .9 (07+12) 76 .3 (07++12) 22 .4 CVPR16 A simple and effective Online Hard ExampleMining algorithm toimprove training ofregion based detectors 123 International Journal of Computer Vision Table 10 continued Detector name Region proposal Backbone DCNN Pipelined used VOC07 results VOC12 results COCO results Published in Highlights FactorNet (Ouyang et al.2016) SS GooglNet RCNN \u2212\u2212\u2212 CVPR16 Identify the imbalance in the number ofsamples for differentobject categories;propose adivide-and-conquerfeature learningscheme Chained Cascade (Cai andV asconcelos2018) SS CRAFT VGG Inceptionv2 Fast RCNN, Faster RCNN 80.4 (07+12) (SS+VGG) \u2212\u2212 ICCV17 Jointly learn DCNN and multiple stages ofcascaded classi\ufb01ers.Boost detectionaccuracy on PASCALVOC 2007 andImageNet for both fast RCNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S193",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "and Faster RCNN using differentregion proposalmethods Cascade RCNN (Cai andV asconcelos2018) RPN VGG ResNet101 +FPN Faster RCNN \u2212\u2212 42.8 CVPR18 Jointly learn DCNN and multiple stages ofcascaded classi\ufb01ers,which are learnedusing differentlocalization accuracyfor selecting positivesamples. Stackbounding boxregression at multiplestages RetinaNet (Lin et al. 2017b) \u2212 ResNet101 +FPN RetinaNet \u2212\u2212 39.1 ICCV17 Propose a novel Focal Loss which focusestraining on hardexamples. Handleswell the problem ofimbalance of positiveand negative sampleswhen training aone-stage detector Results on COCO are reported with Test Dev. The detection results on COCO are based on mAP@IoU[0.5, 0.95] 123 International Journal of Computer Vision Fig. 20 Localization error could stem from insuf\ufb01cient overlap or duplicate detections. Localization error is a frequent cause of false pos- itives (Color \ufb01gure online) IOU",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S194",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "thresholds, based on the observation that the output of a detector trained with a certain IOU is a good distribution to train the detector of the next higher IOU threshold, in order to be sequentially more selective against close false positives. This approach can be built with any RCNN-based detector, and is demonstrated to achieve consistent gains (about 2 to 4 points) independent of the baseline detector strength, at a marginal increase in computation. There is also recent work (Jiang et al. 2018; Rezato\ufb01ghi et al. 2019; Huang et al. 2019) formulating IOU directly as the optimization objective, and in proposing improved NMS results (Bodla et al.2017;H e et al. 2019; Hosang et al. 2017; TychsenSmith and Petersson 2018), such",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S195",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "as Soft NMS (Bodla et al. 2017) and learning NMS (Hosang et al. 2017). Class Imbalance Handling Unlike image classi\ufb01cation, object detection has another unique problem: the serious imbalance between the number of labeled object instances and the number of background examples (image regions not belonging to any object class of interest). Most back- ground examples are easy negatives, however this imbalance can make the training very inef\ufb01cient, and the large num- ber of easy negatives tends to overwhelm the training. In the past, this issue has typically been addressed via tech- niques such as bootstrapping (Sung and Poggio1994). More recently, this problem has also seen some attention (Li et al. 2019a; Lin et al. 2017b; Shrivastava et al. 2016).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S196",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Because the region proposal stage rapidly \ufb01lters out most background regions and proposes a small number of object candidates, this class imbalance issue is mitigated to some extent in two-stage detectors (Girshick et al. 2014; Girshick 2015; Ren et al. 2015;H ee ta l . 2017), although example mining",
      "page_hint": null,
      "token_count": 49,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S197",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(Shrivastava et al. 2016), may be used to maintain a rea- sonable balance between foreground and background. In the case of one-stage object detectors (Redmon et al. 2016;L i u et al. 2016), this imbalance is extremely serious (e.g. 100,000",
      "page_hint": null,
      "token_count": 40,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S198",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "background",
      "text": "proposed Focal Loss to address this by rectifying the Cross Entropy loss, such that it down-weights the loss assigned to correctly classi\ufb01ed examples. Li et al. (2019a) studied this issue from the perspective of gradient norm distribution, and proposed a Gradient Harmonizing Mechanism (GHM) to handle it.",
      "page_hint": null,
      "token_count": 47,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S199",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "discussion",
      "text": "Generic object detection is an important and challenging problem in computer vision and has received considerable attention. Thanks to remarkable developments in deep learn- ing techniques, the \ufb01eld of object detection has dramatically evolved. As a comprehensive survey on deep learning for generic object detection, this paper has highlighted the recent achievements, provided a structural taxonomy for methods according to their roles in detection, summarized existing popular datasets and evaluation criteria, and discussed perfor- mance for the most representative methods. We conclude this review with a discussion of the state of the art in Sect.10.1, an overall discussion of key issues in Sect. 10.2, and \ufb01nally suggested future research directions in Sect. 10.3. 10.1 State of the Art Performance A",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S200",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "discussion",
      "text": "large variety of detectors has appeared in the last few years, and the introduction of standard benchmarks, such as PASCAL VOC (Everingham et al.2010, 2015), ImageNet (Russakovsky et al. 2015) and COCO (Lin et al. 2014), has made it easier to compare detectors. As can be seen from our earlier discussion in Sects.5\u20139, it may be misleading to compare detectors in terms of their originally reported performance (e.g. accuracy, speed), as they can differ in fundamental / contextual respects, including the following choices: \u2022 Meta detection frameworks, such as RCNN (Girshick et al. 2014), Fast RCNN (Girshick 2015), Faster RCNN (Ren et al. 2015), RFCN (Dai et al. 2016c), Mask RCNN (He et al. 2017), YOLO (Redmon et al. 2016)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S201",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "discussion",
      "text": "and SSD (Liu et al. 2016); \u2022 Backbone networks such as VGG (Simonyan and Zis- serman 2015), Inception (Szegedy et al. 2015; Ioffe and Szegedy 2015; Szegedy et al. 2016), ResNet (He et al. 2016), ResNeXt (Xie et al. 2017), and Xception (Chollet 2017) etc. listed in Table 6; \u2022 Innovations such as multilayer feature combination (Lin et al. 2017a; Shrivastava et al. 2017;F ue ta l . 2017), deformable convolutional networks (Dai et al. 2017), deformable RoI pooling (Ouyang et al. 2015; Dai et al. 2017), heavier heads (Ren et al. 2016; Peng et al. 2018), and lighter heads (Li et al. 2018c); 123 International Journal of Computer Vision \u2022 Pretraining with datasets such as ImageNet (Russakovsky et al.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S202",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "discussion",
      "text": "2015), COCO (Lin et al. 2014), Places (Zhou et al. 2017a), JFT (Hinton et al. 2015) and Open Images (Krasin et al. 2017); \u2022 Different detection proposal methods and different num- bers of object proposals; \u2022 Train/test data augmentation, novel multiscale training strategies (Singh and Davis 2018; Singh et al. 2018b) etc, and model ensembling. Although it may be impractical to compare every recently proposed detector, it is nevertheless valuable to integrate representative and publicly available detectors into a com- mon platform and to compare them in a uni\ufb01ed manner. There has been very limited work in this regard, except for Huang\u2019s study (Huang et al. 2017b) of the three main fam- ilies of detectors [Faster RCNN (Ren et al.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S203",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "discussion",
      "text": "2015), RFCN (Dai et al. 2016c) and SSD (Liu et al. 2016)] by varying the backbone network, image resolution, and the number of box proposals. As can be seen from Tables 7, 8, 9, 10, 11,w eh a v es u m - marized the best reported performance of many methods on three widely used standard benchmarks. The results of these",
      "page_hint": null,
      "token_count": 61,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S204",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "their differing in one or more of the aspects listed above. Figures 3 and 21 present a very brief overview of the state of the art, summarizing the best detection results of the PAS- CAL VOC, ILSVRC and MSCOCO challenges; more results can be found at detection challenge websites (ILSVRC 2018; MS COCO 2018; PASCAL VOC 2018). The competition winner of the open image challenge object detection task achieved 61.71% mAP in the public leader board and 58 .66% mAP on the private leader board, obtained by combining the detection results of several two-stage detectors including Fast RCNN (Girshick 2015), Faster RCNN (Ren et al. 2015), FPN (Lin et al. 2017a), Deformable RCNN (Dai et al. 2017), and Cascade RCNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S205",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(Cai and V asconcelos2018). In summary, the backbone network, the detection framework, and the avail- ability of large scale datasets are the three most important factors in detection accuracy. Ensembles of multiple models, the incorporation of context features, and data augmentation all help to achieve better accuracy. In less than 5 years, since AlexNet (Krizhevsky et al. 2012a) was proposed, the Top5 error on ImageNet classi\ufb01ca- tion (Russakovsky et al. 2015) with 1000 classes has dropped from 16% to 2%, as shown in Fig. 15. However, the mAP of the best performing detector (Peng et al. 2018)o nC O C O (Lin et al. 2014), trained to detect only 80 classes, is only at 73%, even at 0.5 IoU, illustrating",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S206",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "how object detection is much harder than image classi\ufb01cation. The accuracy and robustness achieved by the state-of-the-art detectors far from satis\ufb01es the requirements of real world applications, so there remains signi\ufb01cant room for future improvement. 10.2 Summary and Discussion With hundreds of references and many dozens of methods discussed throughout this paper, we would now like to focus on the key factors which have emerged in generic object detection based on deep learning. (1) Detection frameworks: two stage versus one stage In Sect. 5 we identi\ufb01ed two major categories of detection frameworks: region based (two stage) and uni\ufb01ed (one stage): \u2022 When large computational cost is allowed, two-stage detectors generally produce higher detection accuracies than one-stage, evidenced by the fact",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S207",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "that most winning",
      "page_hint": null,
      "token_count": 3,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S208",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "predominantly based on two-stage frameworks, because their structure is more \ufb02exible and better suited for region based classi\ufb01cation. The most widely used frameworks are Faster RCNN (Ren et al. 2015), RFCN (Dai et al. 2016c) and Mask RCNN (He et al. 2017). \u2022 It has been shown in Huang et al. ( 2017b) that the detec- tion accuracy of one-stage SSD (Liu et al. 2016)i sl e s s sensitive to the quality of the backbone network than rep- resentative two-stage frameworks. \u2022 One-stage detectors like YOLO (Redmon et al. 2016) and SSD (Liu et al. 2016) are generally faster than two-stage ones, because of avoiding preprocessing algorithms, using lightweight backbone networks, performing pre- diction with fewer candidate regions, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S209",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "making the classi\ufb01cation subnetwork fully convolutional. However, two-stage detectors can run in real time with the intro- duction of similar techniques. In any event, whether one stage or two, the most time consuming step is the feature extractor (backbone network) (Law and Deng 2018;R e n et al. 2015). \u2022 It has been shown (Huang et al. 2017b; Redmon et al. 2016; Liu et al. 2016) that one-stage frameworks like YOLO and SSD typically have much poorer performance when detecting small objects than two-stage architec- tures like Faster RCNN and RFCN, but are competitive in detecting large objects. There have been many attempts to build better (faster, more accurate, or more robust) detectors by attacking each stage of the detection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S210",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "framework. No matter whether one, two or multiple stages, the design of the detection framework has converged towards a number of crucial design choices: \u2022 A fully convolutional pipeline \u2022 Exploring complementary information from other corre- lated tasks, e.g., Mask RCNN (He et al. 2017) \u2022 Sliding windows (Ren et al. 2015) 123 International Journal of Computer VisionTable 11 Summary of properties and performance of milestone detection frameworks for generic object detection Detector name RP Backbone DCNN Input ImgSize VOC07 results VOC12 results Speed (FPS) Published in Source code Highlights and Disadvantages Region based (Sect. 5.1) RCNN (Girshick et al. 2014) SS AlexNet Fixed 58 .5 (07) 53 .3 (12) < 0.1 CVPR14 Caffe Matlab Highlights: First to integrate CNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S211",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "with RP methods; Dramaticperformance improvement overprevious state of the artP Disadvantages: Multistage pipeline of sequentially-trained(External RP computation, CNN\ufb01netuning, each warped RPpassing through CNN, SVM andBBR training); Training isexpensive in space and time;Testing is slow SPPNet (He et al. 2014) SS ZFNet Arbitrary 60 .9 (07) \u2212 < 1 ECCV14 Caffe Matlab Highlights: First to introduce SPP into CNN architecture; Enableconvolutional feature sharing;Accelerate RCNN evaluation byorders of magnitude withoutsacri\ufb01cing performance; Fasterthan OverFeat Disadvantages: Inherit disadvantages of RCNN; Doesnot result in much trainingspeedup; Fine-tuning not able toupdate the CONV layers before SPP layer Fast RCNN (Girshick 2015) SS AlexNet VGGM VGG16 Arbitrary 70 .0 (VGG) (07+12) 68.4 (VGG) (07++12) < 1 ICCV15 Caffe Python Highlights: First to enable end-to-end detector training (ignoring",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S212",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "RP generation); Designa RoI pooling layer; Much fasterand more accurate than SPPNet;No disk storage required forfeature caching Disadvantages: External RP computation is exposed as thenew bottleneck; Still too slow forreal time applications 123 International Journal of Computer Vision Table 11 continued Detector name RP Backbone DCNN Input ImgSize VOC07 results VOC12 results Speed (FPS) Published in Source code Highlights and Disadvantages Faster RCNN (Ren et al.2015) RPN ZFnet VGG Arbitrary 73 .2 (VGG) (07+12) 70.4 (VGG) (07++12) < 5 NIPS15 Caffe Matlab Python Highlights: Propose RPN for generating nearly cost-free andhigh quality RPs instead ofselective search; Introducetranslation invariant andmultiscale anchor boxes asreferences in RPN; Unify RPNand Fast RCNN into a singlenetwork by sharing CONVlayers; An order of magnitudefaster than Fast",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S213",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "RCNN withoutperformance loss; Can run testing at 5 FPS with VGG16 Disadvantages: Training is complex, not a streamlined process; Still falls short of real time RCNN\u2296 R( L e n c and V edaldi2015) New ZFNet +SPP Arbitrary 59 .7 (07) \u2212 < 5B M V C 1 5 \u2212 Highlights: Replace selective search with static RPs; Prove thepossibility of building integrated,simpler and faster detectors thatrely exclusively on CNN Disadvantages: Falls short of real time; Decreased accuracy frompoor RPs RFCN (Dai et al. 2016c) RPN ResNet101 Arbitrary 80 .5 (07+12) 83.6 (07+12+CO) 77.6 (07++12) 82.0 (07++12+CO) < 10 NIPS16 Caffe Matlab Highlights: Fully convolutional detection network; Design a setof position sensitive score mapsusing a bank of specializedCONV layers; Faster than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S214",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "FasterRCNN without sacri\ufb01cing muchaccuracy Disadvantages: Training is not a streamlined process; Still fallsshort of real time 123 International Journal of Computer VisionTable 11 continued Detector name RP Backbone DCNN Input ImgSize VOC07 results VOC12 results Speed (FPS) Published in Source code Highlights and Disadvantages Mask RCNN (He et al. 2017) RPN ResNet101 ResNeXt101 Arbitrary 50 .3 (ResNeXt101) (COCO Result) < 5 ICCV17 Caffe Matlab Python Highlights: A simple, \ufb02exible, and effective framework for objectinstance segmentation; ExtendsFaster RCNN by adding anotherbranch for predicting an objectmask in parallel with the existingbranch for BB prediction;Feature Pyramid Network (FPN)is utilized; Outstandingperformance Disadvantages: Falls short of real time applications Uni\ufb01ed (Sect. 5.2) OverFeat ( S e r m a n e te ta l .2014)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S215",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "\u2212 AlexNet like Arbitrary \u2212\u2212 < 0.1 ICLR14 c++ Highlights: Convolutional feature sharing; Multiscale imagepyramid CNN feature extraction;Won the ISLVRC2013localization competition;Signi\ufb01cantly faster than RCNN Disadvantages: Multi-stage pipeline sequentially trained; Single bounding box regressor; Cannot handle multiple objectinstances of the same class; Tooslow for real time applications YOLO (Redmon et al. 2016) \u2212 GoogLeNet like Fixed 66 .4 (07+12) 57 .9 (07++12) < 25 (VGG) CVPR16 DarkNet Highlights: First ef\ufb01cient uni\ufb01ed detector; Drop RP processcompletely; Elegant and ef\ufb01cientdetection framework;Signi\ufb01cantly faster than previousdetectors; YOLO runs at 45 FPS,Fast YOLO at 155 FPS; Disadvantages: Accuracy falls far behind state of the art detectors;Struggle to localize small objects 123 International Journal of Computer Vision Table 11 continued Detector name RP Backbone DCNN Input ImgSize",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S216",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "VOC07 results VOC12 results Speed (FPS) Published in Source code Highlights and Disadvantages YOLOv2 (Redmon andFarhadi2017) \u2212 DarkNet Fixed 78 .6 (07+12) 73 .5 (07++12) < 50 CVPR17 DarkNet Highlights: Propose a faster DarkNet19; Use a number ofexisting strategies to improveboth speed and accuracy;Achieve high accuracy and highspeed; YOLO9000 can detectover 9000 object categories inreal time Disadvantages: Not good at detecting small objects SSD (Liu et al. 2016) \u2212 VGG16 Fixed 76 .8 (07+12) 81.5 (07+12+CO) 74.9 (07++12) 80.0 (07++12+CO) < 60 ECCV16 Caffe Python Highlights: First accurate and ef\ufb01cient uni\ufb01ed detector;Effectively combine ideas fromRPN and YOLO to performdetection at multi-scale CONVlayers; Faster and signi\ufb01cantlymore accurate than YOLO; Can run at 59 FPS; Disadvantages: Not good at detecting small objects",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S217",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "See Sect. 5 for a detailed discussion. Some architectures are illustrated in Fig. 13. The properties of the backbone DCNNs can be found in Table 6 Training data: \u201c07\u201d \u2190VOC2007 trainval; \u201c07T\u201d\u2190VOC2007 trainval and test; \u201c12\u201d \u2190VOC2012 trainval; \u201cCO\u201d \u2190COCO trainval. The \u201cSpeed\u201d column roughly estimates the detection speed with a single Nvidia Titan X GPURPregion proposal; SS selective search; RPN region proposal network; RCN N \u2296 R RCNN minus R and used a trivial RP method 123 International Journal of Computer Vision \u2022 Fusing information from different layers of the backbone. The evidence from recent success of cascade for object detec- tion (Cai and V asconcelos 2018; Cheng et al. 2018a, b) and instance segmentation on COCO (Chen et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S218",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "al. 2019a) and other challenges has shown that multistage object detection could be a future framework for a speed-accuracy trade-off. A teaser investigation is being done in the 2019 WIDER Challenge (Loy et al.2019). (2) Backbone networks As discussed in Sect. 6.1, backbone networks are one of the main driving forces behind the rapid improvement of detection performance, because of the key role played by dis- criminative object feature representation. Generally, deeper backbones such as ResNet (He et al.2016), ResNeXt (Xie et al. 2017), InceptionResNet (Szegedy et al. 2017) perform better; however, they are computationally more expensive and require much more data and massive computing for train- ing. Some backbones (Howard et al.2017; Iandola et al. 2016; Zhang et al.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S219",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "2018c) were proposed for focusing on speed instead, such as MobileNet (Howard et al. 2017) which has been shown to achieve VGGNet16 accuracy on ImageNet with only 1 30 the computational cost and model size. Back- bone training from scratch may become possible as more training data and better training strategies are available (Wu and He 2018; Luo et al. 2018, 2019). (3) Improving the robustness of object representation The variation of real world images is a key challenge in object recognition. The variations include lighting, pose, deforma- tions, background clutter, occlusions, blur, resolution, noise, and camera distortions. (3.1) Object scale and small object size Large variations of object scale, particularly those of small objects, pose a great challenge. Here",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S220",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "a summary and discus- sion on the main strategies identi\ufb01ed in Sect. 6.2: \u2022 Using image pyramids: They are simple and effective, helping to enlarge small objects and to shrink large ones. They are computationally expensive, but are nevertheless commonly used during inference for better accuracy. \u2022 Using features from convolutional layers of different resolutions: In early work like SSD (Liu et al. 2016), predictions are performed independently, and no infor- mation from other layers is combined or merged. Now it is quite standard to combine features from different layers, e.g. in FPN (Lin et al.2017a). Fig. 21 Evolution of object detection performance on COCO (Test-Dev results). Results are quoted from (Girshick 2015;H ee ta l . 2017;R e n",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S221",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "et al. 2017). The backbone network, the design of detection framework and the availability of good and large scale datasets are the three most important factors in detection accuracy \u2022 Using dilated convolutions (Li et al. 2018b, 2019b): A simple and effective method to incorporate broader con- text and maintain high resolution feature maps. \u2022 Using anchor boxes of different scales and aspect ratios: Drawbacks of having many parameters, and scales and aspect ratios of anchor boxes are usually heuristically determined. \u2022 Up-scaling: Particularly for the detection of small objects, high-resolution networks (Sun et al. 2019a, b) can be developed. It remains unclear whether super-resolution techniques improve detection accuracy or not. Despite recent advances, the detection accuracy for small objects",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S222",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "is still much lower than that of larger ones. There- fore, the detection of small objects remains one of the key challenges in object detection. Perhaps localization require- ments need to be generalized as a function of scale, since certain applications, e.g. autonomous driving, only require the identi\ufb01cation of the existence of small objects within a larger region, and exact localization is not necessary. (3.2) Deformation, occlusion, and other factors As discussed in Sect. 2.2, there are approaches to han- dling geometric transformation, occlusions, and deformation mainly based on two paradigms. The \ufb01rst is a spatial transformer network, which uses regression to obtain a deformation \ufb01eld and then warp features according to the deformation \ufb01eld (Dai et al. 2017). The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S223",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "second is based on a deformable part-based model (Felzenszwalb et al. 2010b), 123 International Journal of Computer Vision which \ufb01nds the maximum response to a part \ufb01lter with spa- tial constraints taken into consideration (Ouyang et al. 2015; Girshick et al. 2015; Wan et al. 2015). Rotation invariance may be attractive in certain applica- tions, but there are limited generic object detection work focusing on rotation invariance, because popular benchmark detection datasets (PASCAL VOC, ImageNet, COCO) do not have large variations in rotation. Occlusion handling is inten- sively studied in face detection and pedestrian detection, but very little work has been devoted to occlusion handling for generic object detection. In general, despite recent advances, deep networks are still limited by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S224",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "the lack of robustness to a number of variations, which signi\ufb01cantly constrains their real-world applications. (4) Context reasoning As introduced in Sect. 7, objects in the wild typically coexist with other objects and environments. It has been recog- nized that contextual information (object relations, global scene statistics) helps object detection and recognition (Oliva and Torralba 2007), especially for small objects, occluded objects, and with poor image quality. There was extensive work preceding deep learning (Malisiewicz and Efros2009; Murphy et al. 2003; Rabinovich et al. 2007; Divvala et al. 2009; Galleguillos and Belongie 2010), and also quite a few works in the era of deep learning (Gidaris and Komodakis 2015; Zeng et al. 2016, 2017; Chen and Gupta 2017;H ue ta",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S225",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "l . 2018a). How to ef\ufb01ciently and effectively incorporate con- textual information remains to be explored, possibly guided by how human vision uses context, based on scene graphs (Li et al. 2017d), or via the full segmentation of objects and scenes using panoptic segmentation (Kirillov et al. 2018). (5) Detection proposals Detection proposals signi\ufb01cantly reduce search spaces. As recommended in Hosang et al. ( 2016), future detection pro- posals will surely have to improve in repeatability, recall, localization accuracy, and speed. Since the success of RPN (Ren et al. 2015), which integrated proposal generation and detection into a common framework, CNN based detection proposal generation methods have dominated region pro- posal. It is recommended that new detection proposals should be",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S226",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "assessed for object detection, instead of evaluating detec- tion proposals alone. (6) Other factors As discussed in Sect. 9, there are many other factors affecting object detection quality: data augmentation, novel train- ing strategies, combinations of backbone models, multiple detection frameworks, incorporating information from other related tasks, methods for reducing localization error, han- dling the huge imbalance between positive and negative samples, mining of hard negative samples, and improving loss functions. 10.3 Research Directions Despite the recent tremendous progress in the \ufb01eld of object detection, the technology remains signi\ufb01cantly more primi- tive than human vision and cannot yet satisfactorily address real-world challenges like those of Sect. 2.2. We see a number of long-standing challenges: \u2022 Working in an open world:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S227",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "being robust to any number of environmental changes, being able to evolve or adapt. \u2022 Object detection under constrained conditions: learning from weakly labeled data or few bounding box annota- tions, wearable devices, unseen object categories etc. \u2022 Object detection in other modalities: video, RGBD images, 3D point clouds, lidar, remotely sensed imagery etc. Based on these challenges, we see the following directions of future research: (1) Open World Learning The ultimate goal is to develop object detection capable of accurately and ef\ufb01ciently recog- nizing and localizing instances in thousands or more object categories in open-world scenes, at a level competitive with the human visual system. Object detection algorithms are unable, in general, to recognize object categories outside of their",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S228",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "training dataset, although ideally there should be the ability to recognize novel object categories (Lake et al.2015; Hariharan and Girshick 2017). Current detection datasets (Everingham et al. 2010; Russakovsky et al. 2015; Lin et al. 2014) contain only a few dozen to hundreds of categories, signi\ufb01cantly fewer than those which can be recognized by humans. New larger-scale datasets (Hoffman et al. 2014; Singh et al. 2018a; Redmon and Farhadi 2017) with signi\ufb01- cantly more categories will need to be developed. (2) Better and More Ef\ufb01cient Detection Frameworks One of the reasons for the success in generic object detection has been the development of superior detection frameworks, both region-based [RCNN (Girshick et al.2014), Fast RCNN (Gir- shick 2015), Faster RCNN (Ren",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S229",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "et al. 2015), Mask RCNN (He et al. 2017)] and one-stage detectors [YOLO (Redmon et al. 2016), SSD (Liu et al. 2016)]. Region-based detectors have higher accuracy, one-stage detectors are generally faster and simpler. Object detectors depend heavily on the under- lying backbone networks, which have been optimized for image classi\ufb01cation, possibly causing a learning bias; learn- ing object detectors from scratch could be helpful for new detection frameworks. 123 International Journal of Computer Vision (3) Compact and Ef\ufb01cient CNN Features CNNs have increased remarkably in depth, from several layers [AlexNet (Krizhevsky et al.2012b)] to hundreds of layers [ResNet (He et al. 2016), DenseNet (Huang et al. 2017a)]. These networks have millions to hundreds of millions of param- eters, requiring",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S230",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "massive data and GPUs for training. In order reduce or remove network redundancy, there has been grow- ing research interest in designing compact and lightweight networks (Chen et al. 2017a; Alvarez and Salzmann 2016; Huang et al. 2018;H o w a r de ta l .2017; Lin et al. 2017c;Y u et al. 2018) and network acceleration (Cheng et al. 2018c; Hubara et al. 2016; Han et al. 2016;L ie ta l . 2017a, c;W e i et al. 2018). (4) Automatic Neural Architecture Search Deep learning bypasses manual feature engineering which requires human experts with strong domain knowledge, however DCNNs require similarly signi\ufb01cant expertise. It is natural to con- sider automated design of detection backbone architectures, such as the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S231",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "recent Automated Machine Learning (AutoML) (Quanming et al.2018), which has been applied to image classi\ufb01cation and object detection (Cai et al. 2018; Chen et al. 2019c;G h i a s ie ta l .2019; Liu et al. 2018a; Zoph and Le 2016; Zoph et al. 2018). (5) Object Instance Segmentation For a richer and more detailed understanding of image content, there is a need to tackle pixel-level object instance segmentation (Lin et al. 2014;H ee ta l . 2017;H ue ta l . 2018c), which can play an important role in potential applications that require the pre- cise boundaries of individual objects. (6) Weakly Supervised Detection Current state-of-the- art detectors employ fully supervised models learned from labeled data with object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S232",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "bounding boxes or segmentation masks (Everingham et al.2015; Lin et al. 2014; Russakovsky et al. 2015; Lin et al. 2014). However, fully supervised learn- ing has serious limitations, particularly where the collection of bounding box annotations is labor intensive and where the number of images is large. Fully supervised learning is not scalable in the absence of fully labeled training data, so it is essential to understand how the power of CNNs can be leveraged where only weakly / partially annotated data are provided (Bilen and V edaldi 2016; Diba et al. 2017; Shi et al. 2017). (7) Few / Zero Shot Object Detection The success of deep detectors relies heavily on gargantuan amounts of annotated training data. When the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S233",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "labeled data are scarce, the perfor- mance of deep detectors frequently deteriorates and fails to generalize well. In contrast, humans (even children) can learn a visual concept quickly from very few given exam- ples and can often generalize well (Biederman 1987b;L a k e et al. 2015; FeiFei et al. 2006). Therefore, the ability to learn from only few examples, few shot detection, is very appealing (Chen et al. 2018a; Dong et al. 2018; Finn et al. 2017; Kang et al. 2018;L a k ee ta l . 2015; Ren et al. 2018; Schwartz et al. 2019). Even more constrained, zero shot object detection localizes and recognizes object classes that have never been seen16 before (Bansal et al. 2018; Demirel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S234",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "et al. 2018; Rah- man et al. 2018b, a), essential for life-long learning machines that need to intelligently and incrementally discover new object categories. (8) Object Detection in Other Modalities Most detectors are based on still 2D images; object detection in other modal- ities can be highly relevant in domains such as autonomous vehicles, unmanned aerial vehicles, and robotics. These modalities raise new challenges in effectively using depth (Chen et al. 2015c; Pepik et al. 2015; Xiang et al. 2014;W u et al. 2015), video (Feichtenhofer et al. 2017; Kang et al. 2016), and point clouds (Qi et al. 2017, 2018). (9) Universal Object Detection: Recently, there has been increasing effort in learning universal representations, those which are effective in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S235",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "multiple image domains, such as nat- ural images, videos, aerial images, and medical CT images (Rebuf\ufb01 et al. 2017, 2018). Most such research focuses on image classi\ufb01cation, rarely targeting object detection (Wang et al. 2019), and developed detectors are usually domain spe- ci\ufb01c. Object detection independent of image domain and cross-domain object detection represent important future directions. The research \ufb01eld of generic object detection is still far from complete. However given the breakthroughs over the past 5 years we are optimistic of future developments and opportunities. Acknowledgements Open access funding provided by University of Oulu including Oulu University Hospital. The authors would like to thank the pioneering researchers in generic object detection and other related \ufb01elds. The authors would also",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S236",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "like to express their sincere appre- ciation to Professor Ji\u02c7r\u00ed Matas, the associate editor and the anonymous reviewers for their comments and suggestions. This work has been sup- ported by the Center for Machine Vision and Signal Analysis at the University of Oulu (Finland) and the National Natural Science Foun- dation of China under Grant 61872379. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adap- tation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indi- cate if changes were made. The images or other third party material",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S237",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copy- right holder. To view a copy of this licence, visithttp://creativecomm ons.org/licenses/by/4.0/. 16 Although side information may be provided, such as a wikipedia page or an attributes vector. 123 International Journal of Computer Vision References Agrawal, P ., Girshick, R., & Malik, J. (2014). Analyzing the perfor- mance of multilayer neural networks for object recognition. In ECCV(pp. 329\u2013344). Alexe, B., Deselaers, T., & Ferrari,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S238",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "V . (2010). What is an object? In CVPR (pp. 73\u201380). Alexe, B., Deselaers, T., & Ferrari, V . (2012). Measuring the objectness of image windows. IEEE TPAMI, 34(11), 2189\u20132202. Alvarez, J., & Salzmann, M. (2016). Learning the number of neurons in deep networks. In NIPS (pp. 2270\u20132278). Andreopoulos, A., & Tsotsos, J. (2013). 50 years of object recognition: Directions forward. Computer Vision and Image Understanding , 117(8), 827\u2013891. Arbel\u00e1ez, P ., Hariharan, B., Gu, C., Gupta, S., Bourdev, L., & Malik, J. (2012). Semantic segmentation using regions and parts. In CVPR (pp. 3378\u20133385). Arbel\u00e1ez, P ., Pont-Tuset, J., Barron, J., Marques, F., & Malik, J. (2014). Multiscale combinatorial grouping. In CVPR (pp. 328\u2013335). Azizpour, H., Razavian, A., Sullivan, J.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S239",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Maki, A., & Carlsson, S. (2016). Factors of transferability for a generic convnet represen- tation.IEEE TPAMI, 38(9), 1790\u20131802. Bansal, A., Sikka, K., Sharma, G., Chellappa, R., & Divakaran, A. (2018). Zero shot object detection. In ECCV. Bar, M. (2004). Visual objects in context. Nature Reviews Neuroscience, 5(8), 617\u2013629. Bell, S., Lawrence, Z., Bala, K., & Girshick, R. (2016). Inside outside net: Detecting objects in context with skip pooling and recurrent neural networks. InCVPR (pp. 2874\u20132883). Belongie, S., Malik, J., & Puzicha, J. (2002). Shape matching and object recognition using shape contexts. IEEE TPAMI, 24(4), 509\u2013522. Bengio, Y ., Courville, A., & Vincent, P . (2013). Representation learning: A review and new perspectives. IEEE TPAMI, 35(8), 1798\u20131828. Biederman, I. (1972). Perceiving",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S240",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "real world scenes. IJCV , 177(7), 77\u2013 80. Biederman, I. (1987a). Recognition by components: A theory of human image understanding. Psychological Review, 94(2), 115. Biederman, I. (1987b). Recognition by components: A theory of human image understanding. Psychological Review, 94(2), 115. Bilen, H., & V edaldi, A. (2016). Weakly supervised deep detection networks. In CVPR (pp. 2846\u20132854). Bodla, N., Singh, B., Chellappa, R., & Davis L. S. (2017). SoftNMS improving object detection with one line of code. In ICCV (pp. 5562\u20135570). Borji, A., Cheng, M., Jiang, H., & Li, J. (2014). Salient object detection: A survey, 1, 1\u201326. arXiv:1411.5878v1. Bourdev, L., & Brandt, J. (2005). Robust object detection via soft cas- cade. CVPR, 2, 236\u2013243. Bruna, J., & Mallat, S. (2013).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S241",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Invariant scattering convolution net- works. IEEE TPAMI , 35(8), 1872\u20131886. Cai, Z., & V asconcelos, N. (2018). Cascade RCNN: Delving into high quality object detection. In CVPR. Cai, Z., Fan, Q., Feris, R., & V asconcelos, N. (2016). A uni\ufb01ed multi- scale deep convolutional neural network for fast object detection. InECCV (pp. 354\u2013370). Cai, H., Yang, J., Zhang, W., Han, S., & Y u, Y . et al. (2018) Path-level network transformation for ef\ufb01cient architecture search. In ICML. Carreira, J., & Sminchisescu, C. (2012). CMPC: Automatic object seg- mentation using constrained parametric mincuts. IEEE TPAMI , 34(7), 1312\u20131328. Chat\ufb01eld, K., Simonyan, K., V edaldi, A., & Zisserman, A. (2014). Return of the devil in the details: Delving deep into convolutional",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S242",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "nets. InBMVC. Chavali, N., Agrawal, H., Mahendru, A., & Batra, D. (2016). Object proposal evaluation protocol is gameable. In CVPR (pp. 835\u2013844). Chellappa, R. (2016). The changing fortunes of pattern recognition and computer vision. Image and Vision Computing , 55, 3\u20135. Chen, G., Choi, W., Y u, X., Han, T., & Chandraker M. (2017a). Learning ef\ufb01cient object detection models with knowledge distillation. In NIPS. Chen, H., Wang, Y ., Wang, G., & Qiao, Y . (2018a). LSTD: A low shot transfer detector for object detection. In AAAI. Chen, K., Pang, J., Wang, J., Xiong, Y ., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., et al. (2019a). Hybrid task cascade for instance segmentation. InCVPR. Chen, L.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S243",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Papandreou, G., Kokkinos, I., Murphy, K., & Y uille, A. (2015a), Semantic image segmentation with deep convolutional nets and fully connected CRFs. InICLR. Chen, L., Papandreou, G., Kokkinos, I., Murphy, K., & Y uille, A. (2018b). DeepLab: Semantic image segmentation with deep con- volutional nets, atrous convolution, and fully connected CRFs. IEEE TPAMI, 40(4), 834\u2013848. Chen, Q., Song, Z., Dong, J., Huang, Z., Hua, Y ., & Yan, S. (2015b). Contextualizing object detection and classi\ufb01cation. IEEE TPAMI, 37(1), 13\u201327. Chen, X., & Gupta, A. (2017). Spatial memory for context reasoning in object detection. In ICCV. Chen, X., Kundu, K., Zhu, Y ., Berneshawi, A. G., Ma, H., Fidler, S., & Urtasun, R. (2015c) 3d object proposals for accurate object class",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S244",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "detection. InNIPS (pp. 424\u2013432). Chen, Y ., Li, J., Xiao, H., Jin, X., Yan, S., & Feng J. (2017b). Dual path networks. In NIPS (pp. 4467\u20134475). Chen, Y ., Rohrbach, M., Yan, Z., Yan, S., Feng, J., & Kalantidis, Y . (2019b), Graph based global reasoning networks. In CVPR. Chen, Y ., Yang, T., Zhang, X., Meng, G., Pan, C., & Sun, J. (2019c). DetNAS: Neural architecture search on object detection. arXiv:1903.10979. Cheng, B., Wei, Y ., Shi, H., Feris, R., Xiong, J., & Huang, T. (2018a). Decoupled classi\ufb01cation re\ufb01nement: Hard false positive suppres- sion for object detection.arXiv:1810.04002. Cheng, B., Wei, Y ., Shi, H., Feris, R., Xiong, J., & Huang, T. (2018b). Revisiting RCNN: On awakening the classi\ufb01cation power",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S245",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "of faster RCNN. InECCV. Cheng, G., Zhou, P ., & Han, J. (2016). RIFDCNN: Rotation invariant and \ufb01sher discriminative convolutional neural networks for object detection. InCVPR (pp. 2884\u20132893). Cheng, M., Zhang, Z., Lin, W., & Torr, P . (2014). BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR (pp. 3286\u20133293). Cheng, Y ., Wang, D., Zhou, P ., & Zhang, T. (2018c). Model compres- sion and acceleration for deep neural networks: The principles, progress, and challenges.IEEE Signal Processing Magazine , 35(1), 126\u2013136. Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. In CVPR (pp. 1800\u20131807). Cinbis, R., V erbeek, J., & Schmid, C. (2017). Weakly supervised object localization with multi-fold multiple instance learning. IEEE TPAMI, 39(1), 189\u2013203.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S246",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Csurka, G., Dance, C., Fan, L., Willamowski, J., & Bray, C. (2004). Visual categorization with bags of keypoints. In ECCV Workshop on statistical learning in computer vision . Dai, J., He, K., Li, Y ., Ren, S., & Sun, J. (2016a). Instance sensitive fully convolutional networks. In ECCV (pp. 534\u2013549). Dai, J., He, K., & Sun J. (2016b). Instance aware semantic segmentation via multitask network cascades. In CVPR (pp. 3150\u20133158). Dai, J., Li, Y ., He, K., & Sun, J. (2016c). RFCN: Object detection via region based fully convolutional networks. In NIPS (pp. 379\u2013387). Dai, J., Qi, H., Xiong, Y ., Li, Y ., Zhang, G., Hu, H., & Wei, Y . (2017). Deformable convolutional networks. In ICCV. Dalal, N.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S247",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "& Triggs, B. (2005). Histograms of oriented gradients for human detection. CVPR, 1, 886\u2013893. 123 International Journal of Computer Vision Demirel, B., Cinbis, R. G., & Ikizler-Cinbis, N. (2018). Zero shot object detection by hybrid region embedding. In BMVC. Deng, J., Dong, W., Socher, R., Li, L., Li, K., & Li, F. (2009). ImageNet: A large scale hierarchical image database. In CVPR (pp. 248\u2013255). Diba, A., Sharma, V ., Pazandeh, A. M., Pirsiavash, H., & V an Gool L. (2017). Weakly supervised cascaded convolutional networks. In CVPR( V o l .3 ,p .9 ) . Dickinson, S., Leonardis, A., Schiele, B., & Tarr, M. (2009). The evolu- tion of object categorization and the challenge of image abstraction in object categorization:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S248",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Computer and human vision perspectives. Cambridge: Cambridge University Press. Ding, J., Xue, N., Long, Y ., Xia, G., & Lu, Q. (2018). Learning RoI trans- former for detecting oriented objects in aerial images. In CVPR. Divvala, S., Hoiem, D., Hays, J., Efros, A., & Hebert, M. (2009). An empirical study of context in object detection. In CVPR (pp. 1271\u2013 1278). Dollar, P ., Wojek, C., Schiele, B., & Perona, P . (2012). Pedestrian detec- tion: An evaluation of the state of the art. IEEE TPAMI , 34(4), 743\u2013761. Donahue, J., Jia, Y ., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., et al. (2014). DeCAF: A deep convolutional activation feature for generic visual recognition.ICML, 32, 647\u2013655. Dong, X., Zheng, L.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S249",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Ma, F., Yang, Y ., & Meng, D. (2018). Few-example object detection with model communication. IEEE Transactions on Pattern Analysis and Machine Intelligence , 41(7), 1641\u20131654. Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., & Tian, Q. (2019). Cen- terNet: Keypoint triplets for object detection. arXiv:1904.08189. Dvornik, N., Mairal, J., & Schmid, C. (2018). Modeling visual context is key to augmenting object detection datasets. In ECCV (pp. 364\u2013 380). Dwibedi, D., Misra, I., & Hebert, M. (2017). Cut, paste and learn: Surprisingly easy synthesis for instance detection. In ICCV (pp. 1301\u20131310). Endres, I., & Hoiem, D. (2010). Category independent object propos- als. In K. Daniilidis, P . Maragos, & N. Paragios (Eds.), European Conference on Computer Vision (pp.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S250",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "575\u2013588). Berlin: Springer. Enzweiler, M., & Gavrila, D. M. (2009). Monocular pedestrian detec- tion: Survey and experiments. IEEE TPAMI, 31(12), 2179\u20132195. Erhan, D., Szegedy, C., Toshev, A., & Anguelov, D. (2014). Scalable object detection using deep neural networks. In CVPR (pp. 2147\u2013 2154). Everingham, M., Eslami, S., Gool, L. V ., Williams, C., Winn, J., & Zisserman, A. (2015). The pascal visual object classes challenge: A retrospective.IJCV, 111(1), 98\u2013136. Everingham, M., Gool, L. V ., Williams, C., Winn, J., & Zisserman, A. (2010). The pascal visual object classes (voc) challenge. IJCV, 88(2), 303\u2013338. Feichtenhofer, C., Pinz, A., & Zisserman, A. (2017). Detect to track and track to detect. In ICCV (pp. 918\u2013927). FeiFei, L., Fergus, R., & Perona, P .",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S251",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(2006). One shot learning of object categories. IEEE TPAMI, 28(4), 594\u2013611. Felzenszwalb, P ., Girshick, R., & McAllester, D. (2010a). Cascade object detection with deformable part models. In CVPR (pp. 2241\u2013 2248). Felzenszwalb, P ., Girshick, R., McAllester, D., & Ramanan, D. (2010b). Object detection with discriminatively trained part based models. IEEE TPAMI, 32(9), 1627\u20131645. Felzenszwalb, P ., McAllester, D., & Ramanan, D. (2008). A discrimi- natively trained, multiscale, deformable part model. In CVPR (pp. 1\u20138). Finn, C., Abbeel, P ., & Levine, S. (2017). Model agnostic meta learning for fast adaptation of deep networks. In ICML (pp. 1126\u20131135). Fischler, M., & Elschlager, R. (1973). The representation and matching of pictorial structures. IEEE Transactions on Computers , 100(1), 67\u201392. Fu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S252",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "C.-Y ., Liu, W., Ranga, A., Tyagi, A., & Berg, A. C. (2017). DSSD: Deconvolutional single shot detector. arXiv:1701.06659. Galleguillos, C., & Belongie, S. (2010). Context based object catego- rization: A critical survey. Computer Vision and Image Under- standing, 114, 712\u2013722. Geronimo, D., Lopez, A. M., Sappa, A. D., & Graf, T. (2010). Survey of pedestrian detection for advanced driver assistance systems. IEEE TPAMI, 32(7), 1239\u20131258. Ghiasi, G., Lin, T., Pang, R., & Le, Q. (2019). NASFPN: Learn- ing scalable feature pyramid architecture for object detection. arXiv:1904.07392. Ghodrati, A., Diba, A., Pedersoli, M., Tuytelaars, T., & V an Gool, L. (2015). DeepProposal: Hunting objects by cascading deep convo- lutional layers. InICCV (pp. 2578\u20132586). Gidaris, S., & Komodakis, N. (2015). Object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S253",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "detection via a multiregion and semantic segmentation aware CNN model. In ICCV (pp. 1134\u2013 1142). Gidaris, S., & Komodakis, N. (2016). Attend re\ufb01ne repeat: Active box proposal generation via in out localization. In BMVC. Girshick, R. (2015). Fast R-CNN. In ICCV (pp. 1440\u20131448). Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmenta- tion. In CVPR (pp. 580\u2013587). Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2016). Region-based convolutional networks for accurate object detection and segmen- tation.IEEE TPAMI, 38(1), 142\u2013158. Girshick, R., Iandola, F., Darrell, T., & Malik, J. (2015). Deformable part models are convolutional neural networks. In CVPR (pp. 437\u2013 446). Goodfellow, I., Bengio, Y ., & Courville,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S254",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "A. (2016). Deep learning . Cambridge: MIT press. Goodfellow, I., Shlens, J., & Szegedy, C. (2015). Explaining and har- nessing adversarial examples. In ICLR. Grauman, K., & Darrell, T. (2005). The pyramid match kernel: Dis- criminative classi\ufb01cation with sets of image features. ICCV, 2, 1458\u20131465. Grauman, K., & Leibe, B. (2011). Visual object recognition. Synthesis Lectures on Arti\ufb01cial Intelligence and Machine Learning , 5(2), 1\u2013181. Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., et al. (2018). Recent advances in convolutional neural networks. Pattern Recognition, 77, 354\u2013377. Guillaumin, M., K\u00fcttel, D., & Ferrari, V . (2014). Imagenet autoan- notation with segmentation propagation. International Journal of Computer Vision, 110(3), 328\u2013348. Gupta, A., V edaldi, A., & Zisserman, A.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S255",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(2016). Synthetic data for text localisation in natural images. In CVPR (pp. 2315\u20132324). Han, S., Dally, W. J., & Mao, H. (2016). Deep Compression: Compress- ing deep neural networks with pruning, trained quantization and huffman coding. InICLR. Hariharan, B., Arbel\u00e1ez, P ., Girshick, R., & Malik, J. (2014). Simulta- neous detection and segmentation. In ECCV (pp. 297\u2013312). Hariharan, B., Arbel\u00e1ez, P ., Girshick, R., & Malik, J. (2016). Object instance segmentation and \ufb01ne-grained localization using hyper- columns.IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4), 627\u2013639. Hariharan, B., & Girshick R. B. (2017). Low shot visual recognition by shrinking and hallucinating features. In ICCV (pp. 3037\u20133046). Harzallah, H., Jurie, F., & Schmid, C. (2009). Combining ef\ufb01cient object localization and image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S256",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "classi\ufb01cation. In ICCV (pp. 237\u2013244). He, K., Gkioxari, G., Doll\u00e1r, P ., & Girshick, R. (2017). Mask RCNN. In ICCV. He, K., Zhang, X., Ren, S., & Sun, J. (2014). Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV (pp. 346\u2013361). 123 International Journal of Computer Vision He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into recti\ufb01ers: Surpassing human-level performance on ImageNet classi\ufb01cation. InICCV (pp. 1026\u20131034). He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In CVPR (pp. 770\u2013778). He, T., Tian, Z., Huang, W., Shen, C., Qiao, Y ., & Sun, C. (2018). An end to end textspotter with explicit alignment and attention. In CVPR",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S257",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(pp. 5020\u20135029). He, Y ., Zhu, C., Wang, J., Savvides, M., & Zhang, X. (2019). Bounding box regression with uncertainty for accurate object detection. In CVPR. Hinton, G., & Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786), 504\u2013507. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv:1503.02531. Hoffman, J., Guadarrama, S., Tzeng, E. S., Hu, R., Donahue, J., Gir- shick, R., Darrell, T., & Saenko, K. (2014). LSDA: Large scale detection through adaptation. InNIPS (pp. 3536\u20133544). Hoiem, D., Chodpathumwan, Y ., & Dai, Q. (2012). Diagnosing error in object detectors. In ECCV (pp. 340\u2013353). Hosang, J., Benenson, R., Doll\u00e1r, P ., & Schiele, B. (2016). What makes for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S258",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "effective detection proposals? IEEE TPAMI, 38(4), 814\u2013829. Hosang, J., Benenson, R., & Schiele, B. (2017). Learning nonmaximum suppression. In ICCV. Hosang, J., Omran, M., Benenson, R., & Schiele, B. (2015). Taking a deeper look at pedestrians. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4073\u20134082). Howard, A., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., & Adam, H. (2017). Mobilenets: Ef\ufb01cient convolutional neural networks for mobile vision applications. In CVPR. Hu, H., Gu, J., Zhang, Z., Dai, J., & Wei, Y . (2018a). Relation networks for object detection. In CVPR. Hu, H., Lan, S., Jiang, Y ., Cao, Z., & Sha, F. (2017). FastMask: Segment multiscale object candidates in one shot.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S259",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "In CVPR (pp. 991\u2013999). Hu, J., Shen, L., & Sun, G. (2018b). Squeeze and excitation networks. In CVPR. Hu, P ., & Ramanan, D. (2017). Finding tiny faces. In CVPR (pp. 1522\u2013 1530). Hu, R., Doll\u00e1r, P ., He, K., Darrell, T., & Girshick, R. (2018c). Learning to segment every thing. In CVPR. Huang, G., Liu, S., van der Maaten, L., & Weinberger, K. (2018). Con- denseNet: An ef\ufb01cient densenet using learned group convolutions. InCVPR. Huang, G., Liu, Z., Weinberger, K. Q., & van der Maaten, L. (2017a). Densely connected convolutional networks. In CVPR. Huang, J., Rathod, V ., Sun, C., Zhu, M., Korattikara, A., Fathi, A., Fischer, I., Wojna, Z., Song, Y ., Guadarrama, S., & Murphy, K. (2017b).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S260",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Speed/accuracy trade offs for modern convolutional object detectors. InCVPR. Huang, Z., Huang, L., Gong, Y ., Huang, C., & Wang, X. (2019). Mask scoring rcnn. In CVPR. Hubara, I., Courbariaux, M., Soudry, D., ElYaniv, R., & Bengio, Y . (2016). Binarized neural networks. In NIPS (pp. 4107\u20134115). Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally, W., & Keutzer, K. (2016). SqueezeNet: Alexnet level accuracy with 50x fewer parameters and 0.5 mb model size.arXiv:1602.07360. ILSVRC detection challenge results. (2018). http://www.image-net. org/challenges/LSVRC/. Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Interna- tional conference on machine learning (pp. 448\u2013456). Jaderberg, M., Simonyan, K., Zisserman, A., et al. (2015). Spatial trans- former networks.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S261",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "In NIPS (pp. 2017\u20132025). Jia, Y ., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. In ACM MM (pp. 675\u2013 678). Jiang, B., Luo, R., Mao, J., Xiao, T., & Jiang, Y . (2018). Acquisition of localization con\ufb01dence for accurate object detection. In ECCV (pp. 784\u2013799). Kang, B., Liu, Z., Wang, X., Y u, F., Feng, J., & Darrell, T. (2018). Few shot object detection via feature reweighting. arXiv:1812.01866. Kang, K., Ouyang, W., Li, H., & Wang, X. (2016). Object detection from video tubelets with convolutional neural networks. In CVPR (pp. 817\u2013825). Kim, A., Sharma, A., & Jacobs, D. (2014). Locally scale invariant con- volutional",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S262",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "neural networks. In NIPS. Kim, K., Hong, S., Roh, B., Cheon, Y ., & Park, M. (2016). PV ANet: Deep but lightweight neural networks for real time object detec- tion. InNIPSW. Kim, Y , Kang, B.-N., & Kim, D. (2018). SAN: Learning relationship between convolutional features for multiscale object detection. In ECCV(pp. 316\u2013331). Kirillov, A., He, K., Girshick, R., Rother, C., & Doll\u00e1r, P . (2018). Panop- tic segmentation. arXiv:1801.00868. Kong, T., Sun, F., Tan, C., Liu, H., & Huang, W. (2018). Deep feature pyramid recon\ufb01guration for object detection. In ECCV (pp. 169\u2013 185). Kong, T., Sun, F., Yao, A., Liu, H., Lu, M., & Chen, Y . (2017). RON: Reverse connection with objectness prior networks for object detection. InCVPR.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S263",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Kong, T., Yao, A., Chen, Y ., & Sun, F. (2016). HyperNet: Towards accurate region proposal generation and joint object detection. In CVPR(pp. 845\u2013853). Kr\u00e4henb\u00fchl, P ., & Koltun, V . (2014), Geodesic object proposals. In ECCV. Krasin, I., Duerig, T., Alldrin, N., Ferrari, V ., AbuElHaija, S., Kuznetsova, A., et al. (2017). OpenImages: A public dataset for large scale multilabel and multiclass image classi\ufb01cation. Dataset available fromhttps://storage.googleapis.com/openimages/web/ index.html. Krizhevsky, A., Sutskever, I., & Hinton, G. (2012a). ImageNet clas- si\ufb01cation with deep convolutional neural networks. In NIPS (pp. 1097\u20131105). Krizhevsky, A., Sutskever, I., & Hinton, G. (2012b). ImageNet clas- si\ufb01cation with deep convolutional neural networks. In NIPS (pp. 1097\u20131105). Kuo, W., Hariharan, B., & Malik, J. (2015). DeepBox: Learning object-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S264",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "ness with convolutional networks. In ICCV (pp. 2479\u20132487). Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., PontTuset, J., et al. (2018). The open images dataset v4: Uni\ufb01ed image classi- \ufb01cation, object detection, and visual relationship detection at scale. arXiv:1811.00982. Lake, B., Salakhutdinov, R., & Tenenbaum, J. (2015). Human level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338. Lampert, C. H., Blaschko, M. B., & Hofmann, T. (2008). Beyond sliding windows: Object localization by ef\ufb01cient subwindow search. In CVPR(pp. 1\u20138). Law, H., & Deng, J. (2018). CornerNet: Detecting objects as paired keypoints. In ECCV. Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. CVPR, 2, 2169\u20132178. LeCun,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S265",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Y ., Bengio, Y ., & Hinton, G. (2015). Deep learning. Nature, 521, 436\u2013444. LeCun, Y ., Bottou, L., Bengio, Y ., & Haffner, P . (1998). Gradient based learning applied to document recognition. Proceedings of the IEEE , 86(11), 2278\u20132324. 123 International Journal of Computer Vision Lee, C., Xie, S., Gallagher, P ., Zhang, Z., & Tu, Z. (2015). Deeply supervised nets. In Arti\ufb01cial intelligence and statistics (pp. 562\u2013 570). Lenc, K., & V edaldi, A. (2015). R-CNN minus R. In BMVC15. Lenc, K., & V edaldi, A. (2018). Understanding image representations by measuring their equivariance and equivalence. In IJCV. Li, B., Liu, Y ., & Wang, X. (2019a). Gradient harmonized single stage detector. In AAAI. Li, H., Kadav,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S266",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "A., Durdanovic, I., Samet, H., & Graf, H. P . (2017a). Pruning \ufb01lters for ef\ufb01cient convnets. In ICLR. Li, H., Lin, Z., Shen, X., Brandt, J., & Hua, G. (2015a). A convolutional neural network cascade for face detection. In CVPR (pp. 5325\u2013 5334). Li, H., Liu, Y ., Ouyang, W., & Wang, X. (2018a). Zoom out and in network with map attention decision for region proposal and object detection. InIJCV. Li, J., Wei, Y ., Liang, X., Dong, J., Xu, T., Feng, J., et al. (2017b). Attentive contexts for object detection. IEEE Transactions on Mul- timedia, 19(5), 944\u2013954. Li, Q., Jin, S., & Yan, J. (2017c). Mimicking very ef\ufb01cient network for object detection. In CVPR (pp. 7341\u20137349). Li, S. Z.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S267",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "& Zhang, Z. (2004). Floatboost learning and statistical face detection. IEEE TPAMI, 26(9), 1112\u20131123. Li, Y ., Chen, Y ., Wang, N., & Zhang, Z. (2019b). Scale aware trident networks for object detection. arXiv:1901.01892. Li, Y ., Ouyang, W., Zhou, B., Wang, K., & Wang, X. (2017d). Scene graph generation from objects, phrases and region captions. In ICCV(pp. 1261\u20131270). Li, Y ., Qi, H., Dai, J., Ji, X., & Wei, Y . (2017e). Fully convolutional instance aware semantic segmentation. In CVPR (pp. 4438\u20134446). Li, Y ., Wang, S., Tian, Q., & Ding, X. (2015b). Feature representation for statistical learning based object detection: A review. Pattern Recognition, 48(11), 3542\u20133559. Li, Z., Peng, C., Y u, G., Zhang, X., Deng, Y .,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S268",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "& Sun, J. (2018b). DetNet: A backbone network for object detection. In ECCV. Li, Z., Peng, C., Y u, G., Zhang, X., Deng, Y ., & Sun, J. (2018c). Light head RCNN: In defense of two stage object detector. In CVPR. Lin, T., Doll\u00e1r, P ., Girshick, R., He, K., Hariharan, B., & Belongie, S. (2017a). Feature pyramid networks for object detection. In CVPR. Lin, T., Goyal, P ., Girshick, R., He, K., & Doll\u00e1r, P . (2017b). Focal loss for dense object detection. In ICCV. Lin, T., Maire, M., Belongie, S., Hays, J., Perona, P ., Ramanan, D., Dol- l\u00e1r, P ., & Zitnick, L. (2014). Microsoft COCO: Common objects in context. InECCV (pp. 740\u2013755). Lin, X., Zhao, C.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S269",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "& Pan, W. (2017c). Towards accurate binary convo- lutional neural network. In NIPS (pp. 344\u2013352). Litjens, G., Kooi, T., Bejnordi, B., Setio, A., Ciompi, F., Ghafoorian, M., et al. (2017). A survey on deep learning in medical image analysis. Medical Image Analysis , 42, 60\u201388. Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L., FeiFei, L., Y uille, A., Huang, J., & Murphy, K. (2018a). Progressive neural architecture search. InECCV (pp. 19\u201334). Liu, L., Fieguth, P ., Guo, Y ., Wang, X., & Pietik\u00e4inen, M. (2017). Local binary features for texture classi\ufb01cation: Taxonomy and experi- mental study.Pattern Recognition, 62, 135\u2013160. Liu, S., Huang, D., & Wang, Y . (2018b). Receptive \ufb01eld block net for accurate and fast",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S270",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "object detection. In ECCV. Liu, S., Qi, L., Qin, H., Shi, J., & Jia, J. (2018c). Path aggregation network for instance segmentation. In CVPR (pp. 8759\u20138768). Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C., & Berg, A. (2016). SSD: Single shot multibox detector. In ECCV (pp. 21\u201337). Liu, Y ., Wang, R., Shan, S., & Chen, X. (2018d). Structure inference net: Object detection using scene level context and instance level relationships. InCVPR (pp. 6985\u20136994). Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional net- works for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3431\u20133440). Lowe, D. (1999). Object recognition from local scale invariant features. ICCV, 2, 1150\u20131157.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S271",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Lowe, D. (2004). Distinctive image features from scale-invariant key- points. IJCV, 60(2), 91\u2013110. Loy, C., Lin, D., Ouyang, W., Xiong, Y ., Yang, S., Huang, Q., et al. (2019). WIDER face and pedestrian challenge 2018: Methods and results.arXiv:1902.06854. Lu, Y ., Javidi, T., & Lazebnik, S. (2016). Adaptive object detection using adjacency and zoom prediction. In CVPR (pp. 2351\u20132359). Luo, P ., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in batch normalization. In ICLR. Luo, P ., Zhang, R., Ren, J., Peng, Z., & Li, J. (2019). Switch- able normalization for learning-to-normalize deep representation. IEEE Transactions on Pattern Analysis and Machine Intelligence. https://doi.org/10.1109/TPAMI.2019.2932062. Malisiewicz, T., & Efros, A. (2009). Beyond categories: The visual memex model for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S272",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "reasoning about object relationships. In NIPS. Ma, J., Shao, W., Ye, H., Wang, L., Wang, H., Zheng, Y ., et al. (2018). Arbitrary oriented scene text detection via rotation proposals. IEEE TMM, 20(11), 3111\u20133122. Manen, S., Guillaumin, M., & V an Gool, L. (2013). Prime object propos- als with randomized prim\u2019s algorithm. In CVPR (pp. 2536\u20132543). Mikolajczyk, K., & Schmid, C. (2005). A performance evaluation of local descriptors. IEEE TPAMI, 27(10), 1615\u20131630. Mordan, T., Thome, N., Henaff, G., & Cord, M. (2018). End to end learning of latent deformable part based representations for object detection. InIJCV (pp. 1\u201321). MS COCO detection leaderboard. (2018). http://cocodataset.org/# detection-leaderboard. Mundy, J. (2006). Object recognition in the geometric era: A retrospec- tive. In J. Ponce,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S273",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "M. Hebert, C. Schmid, & A. Zisserman (Eds.), Book toward category level object recognition(pp. 3\u201328). Berlin: Springer. Murase, H., & Nayar, S. (1995a). Visual learning and recognition of 3D objects from appearance. IJCV, 14(1), 5\u201324. Murase, H., & Nayar, S. (1995b). Visual learning and recognition of 3d objects from appearance. IJCV, 14(1), 5\u201324. Murphy, K., Torralba, A., & Freeman, W. (2003). Using the forest to see the trees: A graphical model relating features, objects and scenes. InNIPS. Newell, A., Huang, Z., & Deng, J. (2017). Associative embedding: End to end learning for joint detection and grouping. In NIPS (pp. 2277\u20132287). Newell, A., Yang, K., & Deng, J. (2016). Stacked hourglass networks for human pose estimation. In ECCV (pp. 483\u2013499). Ojala,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S274",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "T., Pietik\u00e4inen, M., & Maenp\u00e4\u00e4, T. (2002). Multiresolution gray- scale and rotation invariant texture classi\ufb01cation with local binary patterns.IEEE TPAMI, 24(7), 971\u2013987. Oliva, A., & Torralba, A. (2007). The role of context in object recogni- tion. Trends in cognitive sciences , 11(12), 520\u2013527. Opelt, A., Pinz, A., Fussenegger, M., & Auer, P . (2006). Generic object recognition with boosting. IEEE TPAMI, 28(3), 416\u2013431. Oquab, M., Bottou, L., Laptev, I., & Sivic, J. (2014). Learning and transferring midlevel image representations using convolutional neural networks. InCVPR (pp. 1717\u20131724). Oquab, M., Bottou, L., Laptev, I., & Sivic, J. (2015). Is object local- ization for free? weakly supervised learning with convolutional neural networks. InCVPR (pp. 685\u2013694). Osuna, E., Freund, R., & Girosit, F. (1997).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S275",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Training support vector machines: An application to face detection. In CVPR (pp. 130\u2013 136). 123 International Journal of Computer Vision Ouyang, W., & Wang, X. (2013). Joint deep learning for pedestrian detection. In ICCV (pp. 2056\u20132063). Ouyang, W., Wang, X., Zeng, X., Qiu, S., Luo, P ., Tian, Y ., Li, H., Yang, S., Wang, Z., Loy, C.-C., et al. (2015). DeepIDNet: Deformable deep convolutional neural networks for object detection. InCVPR (pp. 2403\u20132412). Ouyang, W., Wang, X., Zhang, C., & Yang, X. (2016). Factors in \ufb01ne- tuning deep model for object detection with long tail distribution. InCVPR (pp. 864\u2013873). Ouyang, W., Wang, K., Zhu, X., & Wang, X. (2017a). Chained cascade network for object detection. In ICCV. Ouyang, W., Zeng,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S276",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "X., Wang, X., Qiu, S., Luo, P ., Tian, Y ., et al. (2017b). DeepIDNet: Object detection with deformable part based convo- lutional neural networks.IEEE TPAMI, 39(7), 1320\u20131334. Parikh, D., Zitnick, C., & Chen, T. (2012). Exploring tiny images: The roles of appearance and contextual information for machine and human object recognition.IEEE TPAMI, 34(10), 1978\u20131991. PASCAL VOC detection leaderboard. (2018). http://host.robots.ox.ac. uk:8080/leaderboard/main_bootstrap.php Peng, C., Xiao, T., Li, Z., Jiang, Y ., Zhang, X., Jia, K., Y u, G., & Sun, J. (2018). MegDet: A large minibatch object detector. In CVPR. Peng, X., Sun, B., Ali, K., & Saenko, K. (2015). Learning deep object detectors from 3d models. In ICCV (pp. 1278\u20131286). Pepik, B., Benenson, R., Ritschel, T., & Schiele, B.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S277",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "(2015). What is holding back convnets for detection? In German conference on pattern recognition (pp. 517\u2013528). Perronnin, F., S\u00e1nchez, J., & Mensink, T. (2010). Improving the \ufb01sher kernel for large scale image classi\ufb01cation. In ECCV (pp. 143\u2013156). Pinheiro, P ., Collobert, R., & Dollar, P . (2015). Learning to segment object candidates. In NIPS (pp. 1990\u20131998). Pinheiro, P ., Lin, T., Collobert, R., & Doll\u00e1r, P . (2016). Learning to re\ufb01ne object segments. In ECCV (pp. 75\u201391). Ponce, J., Hebert, M., Schmid, C., & Zisserman, A. (2007). Toward category level object recognition . Berlin: Springer. Pouyanfar, S., Sadiq, S., Yan, Y ., Tian, H., Tao, Y ., Reyes, M. P ., et al. (2018). A survey on deep learning: Algorithms,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S278",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "techniques, and applications.ACM Computing Surveys , 51(5), 92:1\u201392:36. Qi, C. R., Liu, W., Wu, C., Su, H., & Guibas, L. J. (2018). Frustum pointnets for 3D object detection from RGBD data. In CVPR (pp. 918\u2013927). Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). PointNet: Deep learning on point sets for 3D classi\ufb01cation and segmentation. In CVPR(pp. 652\u2013660). Quanming, Y ., Mengshuo, W., Hugo, J. E., Isabelle, G., Yiqi, H., Y ufeng, L., et al. (2018). Taking human out of learning applications: A survey on automated machine learning.arXiv:1810.13306. Rabinovich, A., V edaldi, A., Galleguillos, C., Wiewiora, E., & Belongie, S. (2007). Objects in context. In ICCV. Rahman, S., Khan, S., & Barnes, N. (2018a). Polarity loss for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S279",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "zero shot object detection. arXiv:1811.08982. Rahman, S., Khan, S., & Porikli, F. (2018b). Zero shot object detection: Learning to simultaneously recognize and localize novel concepts. InACCV. Razavian, R., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). CNN features off the shelf: An astounding baseline for recognition. In CVPR workshops(pp. 806\u2013813). Rebuf\ufb01, S., Bilen, H., & V edaldi, A. (2017). Learning multiple visual domains with residual adapters. In Advances in neural information processing systems (pp. 506\u2013516). Rebuf\ufb01, S., Bilen, H., & V edaldi A. (2018). Ef\ufb01cient parametrization of multidomain deep neural networks. In CVPR (pp. 8119\u20138127). Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). Y ou only look once: Uni\ufb01ed, real time object detection. In CVPR (pp. 779\u2013 788).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S280",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Redmon, J., & Farhadi, A. (2017). YOLO9000: Better, faster, stronger. In CVPR. Ren, M., Trianta\ufb01llou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J. B., Larochelle, H., & Zemel R. S. (2018). Meta learning for semisupervised few shot classi\ufb01cation. InICLR. Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real time object detection with region proposal networks. In NIPS (pp. 91\u201399). Ren, S., He, K., Girshick, R., & Sun, J. (2017). Faster RCNN: Towards real time object detection with region proposal networks. IEEE TPAMI, 39(6), 1137\u20131149. Ren, S., He, K., Girshick, R., Zhang, X., & Sun, J. (2016). Object detec- tion networks on convolutional feature maps. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(7),",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S281",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "1476\u20131481. Rezato\ufb01ghi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., & Savarese, S. (2019). Generalized intersection over union: A metric and a loss for bounding box regression. InCVPR. Rowley, H., Baluja, S., & Kanade, T. (1998). Neural network based face detection. IEEE TPAMI, 20(1), 23\u201338. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015). ImageNet large scale visual recognition challenge. IJCV, 115(3), 211\u2013252. Russell, B., Torralba, A., Murphy, K., & Freeman, W. (2008). LabelMe: A database and web based tool for image annotation. IJCV, 77(1\u2013 3), 157\u2013173. Schmid, C., & Mohr, R. (1997). Local grayvalue invariants for image retrieval. IEEE TPAMI, 19(5), 530\u2013535. Schwartz, E., Karlinsky, L., Shtok, J., Harary, S., Marder, M.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S282",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Pankanti, S., Feris, R., Kumar, A., Giries, R., & Bronstein, A. (2019). Rep- Met: Representative based metric learning for classi\ufb01cation and one shot object detection. InCVPR. Sermanet, P ., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., & LeCun, Y . (2014). OverFeat: Integrated recognition, localization and detection using convolutional networks. InICLR. Sermanet, P ., Kavukcuoglu, K., Chintala, S., & LeCun, Y . (2013). Pedes- trian detection with unsupervised multistage feature learning. In CVPR(pp. 3626\u20133633). Shang, W., Sohn, K., Almeida, D., & Lee, H. (2016). Understanding and improving convolutional neural networks via concatenated recti- \ufb01ed linear units. InICML (pp. 2217\u20132225). Shelhamer, E., Long, J., & Darrell, T. (2017). Fully convolutional net- works for semantic segmentation. IEEE TPAMI. Shen, Z., Liu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S283",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Z., Li, J., Jiang, Y ., Chen, Y ., & Xue, X. (2017). DSOD: Learning deeply supervised object detectors from scratch. InICCV. Shi, X., Shan, S., Kan, M., Wu, S., & Chen, X. (2018). Real time rotation invariant face detection with progressive calibration networks. In CVPR. Shi, Z., Yang, Y ., Hospedales, T., & Xiang, T. (2017). Weakly supervised image annotation and segmentation with objects and attributes. IEEE TPAMI, 39(12), 2525\u20132538. Shrivastava, A., & Gupta A. (2016), Contextual priming and feedback for Faster RCNN. In ECCV (pp. 330\u2013348). Shrivastava, A., Gupta, A., & Girshick, R. (2016). Training region based object detectors with online hard example mining. In CVPR (pp. 761\u2013769). Shrivastava, A., Sukthankar, R., Malik, J., & Gupta, A. (2017).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S284",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Beyond skip connections: Top down modulation for object detection. In CVPR. Simonyan, K., & Zisserman, A. (2015). V ery deep convolutional net- works for large scale image recognition. In ICLR. Singh, B., & Davis, L. (2018). An analysis of scale invariance in object detection-SNIP . In CVPR. Singh, B., Li, H., Sharma, A., & Davis, L. S. (2018a). RFCN 3000 at 30fps: Decoupling detection and classi\ufb01cation. In CVPR. Singh, B., Najibi, M., & Davis, L. S. (2018b). SNIPER: Ef\ufb01cient mul- tiscale training. arXiv:1805.09300. 123 International Journal of Computer Vision Sivic, J., & Zisserman, A. (2003). Video google: A text retrieval",
      "page_hint": null,
      "token_count": 100,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S285",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "on Computer Vision (ICCV) , 2, 1470\u20131477. Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. In ICCV (pp. 843\u2013852). Sun, K., Xiao, B., Liu, D., & Wang, J. (2019a). Deep high resolution representation learning for human pose estimation. In CVPR. Sun, K., Zhao, Y ., Jiang, B., Cheng, T., Xiao, B., Liu, D., et al. (2019b). High resolution representations for labeling pixels and regions. CoRR.,. arXiv:1904.04514. Sun, S., Pang, J., Shi, J., Yi, S., & Ouyang, W. (2018). FishNet: A versatile backbone for image, region, and pixel level prediction. In NIPS(pp. 754\u2013764). Sun, Z., Bebis, G., & Miller, R. (2006). On road vehicle detection: A review. IEEE TPAMI, 28(5),",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S286",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "694\u2013711. Sung, K., & Poggio, T. (1994). Learning and example selection for object and pattern detection. MIT AI Memo (1521). Swain, M., & Ballard, D. (1991). Color indexing. IJCV , 7(1), 11\u201332. Szegedy, C., Liu, W., Jia, Y ., Sermanet, P ., Reed, S., Anguelov, D., Erhan, D., V anhoucke, V ., & Rabinovich, A. (2015). Going deeper with convolutions. InCVPR (pp. 1\u20139). Szegedy, C., Ioffe, S., V anhoucke, V ., & Alemi, A. (2017). Inception v4, inception resnet and the impact of residual connections on learning. InAAAI (pp. 4278\u20134284). Szegedy, C., Reed, S., Erhan, D., Anguelov, D., & Ioffe, S. (2014). Scalable, high quality object detection. arXiv:1412.1441. Szegedy, C., Toshev, A., & Erhan, D. (2013). Deep neural networks for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S287",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "object detection. In NIPS (pp. 2553\u20132561). Szegedy, C., V anhoucke, V ., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In CVPR(pp. 2818\u20132826). Torralba, A. (2003). Contextual priming for object detection. IJCV , 53(2), 169\u2013191. Turk, M. A., & Pentland, A. (1991). Face recognition using eigenfaces. In CVPR (pp. 586\u2013591). Tuzel, O., Porikli, F., & Meer P . (2006). Region covariance: A fast descriptor for detection and classi\ufb01cation. In ECCV (pp. 589\u2013600). TychsenSmith, L., & Petersson, L. (2017). DeNet: Scalable real time object detection with directed sparse sampling. In ICCV. TychsenSmith, L., & Petersson, L. (2018). Improving object localiza- tion with \ufb01tness nms and bounded iou loss. In CVPR. Uijlings, J., van de",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S288",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Sande, K., Gevers, T., & Smeulders, A. (2013). Selective search for object recognition. IJCV , 104(2), 154\u2013171. V aillant, R., Monrocq, C., & LeCun, Y . (1994). Original approach for the localisation of objects in images. IEE Proceedings Vision, Image and Signal Processing , 141(4), 245\u2013250. V an de Sande, K., Uijlings, J., Gevers, T., & Smeulders, A. (2011). Segmentation as selective search for object recognition. In ICCV (pp. 1879\u20131886). V aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., & Polosukhin, I. (2017). Attention is all you need. InNIPS (pp. 6000\u20136010). V edaldi, A., Gulshan, V ., V arma, M., & Zisserman, A. (2009). Multiple kernels for object detection. In ICCV (pp. 606\u2013613).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S289",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Viola, P ., & Jones, M. (2001). Rapid object detection using a boosted cascade of simple features. CVPR, 1, 1\u20138. Wan, L., Eigen, D., & Fergus, R. (2015). End to end integration of a convolution network, deformable parts model and nonmaximum suppression. InCVPR (pp. 851\u2013859). Wang, H., Wang, Q., Gao, M., Li, P ., & Zuo, W. (2018). Multiscale location aware kernel representation for object detection. In CVPR. Wang, X., Cai, Z., Gao, D., & V asconcelos, N. (2019). Towards universal object detection by domain attention. arXiv:1904.04402. Wang, X., Han, T., & Yan, S. (2009). An HOG-LBP human detector with partial occlusion handling. In International conference on computer vision (pp. 32\u201339). Wang, X., Shrivastava, A., & Gupta, A. (2017). A",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S290",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Fast RCNN: Hard positive generation via adversary for object detection. In CVPR. Wei, Y ., Pan, X., Qin, H., Ouyang, W., & Yan, J. (2018). Quantization mimic: Towards very tiny CNN for object detection. In ECCV (pp. 267\u2013283). Woo, S., Hwang, S., & Kweon, I. (2018). StairNet: Top down semantic aggregation for accurate one shot detection. In WACV (pp. 1093\u2013 1102). Worrall, D. E., Garbin, S. J., Turmukhambetov, D., & Brostow, G. J. (2017). Harmonic networks: Deep translation and rotation equiv- ariance. InCVPR (V ol. 2). Wu, Y ., & He, K. (2018). Group normalization. In ECCV (pp. 3\u201319). Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., & Y u, P . S. (2019). A com- prehensive survey",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S291",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "on graph neural networks. arXiv:1901.00596. Wu, Z., Song, S., Khosla, A., Y u, F., Zhang, L., Tang, X., & Xiao, J. (2015). 3D ShapeNets: A deep representation for volumetric shapes. InCVPR (pp. 1912\u20131920). Xia, G., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., Datcu, M., Pelillo, M., & Zhang, L. (2018). DOTA: A large-scale dataset for object detection in aerial images. In CVPR (pp. 3974\u20133983). Xiang, Y ., Mottaghi, R., & Savarese, S. (2014). Beyond PASCAL: A benchmark for 3D object detection in the wild. In WACV (pp. 75\u2013 82). Xiao, R., Zhu, L., & Zhang, H. (2003). Boosting chain learning for object detection. In ICCV (pp. 709\u2013715). Xie, S., Girshick, R., Doll\u00e1r, P ., Tu, Z., &",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S292",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "He, K. (2017). Aggregated residual transformations for deep neural networks. In CVPR. Yang, B., Yan, J., Lei, Z., & Li, S. (2016a). CRAFT objects from images. In CVPR (pp. 6043\u20136051). Yang, F., Choi, W., & Lin, Y . (2016b). Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classi\ufb01ers. InCVPR (pp. 2129\u20132137). Yang, M., Kriegman, D., & Ahuja, N. (2002). Detecting faces in images: As u r v e y .IEEE TPAMI, 24(1), 34\u201358. Ye, Q., & Doermann, D. (2015). Text detection and recognition in imagery: A survey. IEEE TPAMI, 37(7), 1480\u20131500. Y osinski, J., Clune, J., Bengio, Y ., & Lipson, H. (2014). How trans- ferable are features in deep neural",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S293",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "networks? In NIPS (pp. 3320\u20133328). Y oung, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends in deep learning based natural language processing. IEEE Com- putational Intelligence Magazine , 13(3), 55\u201375. Y u, F., & Koltun, V . (2015). Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122. Y u, F., Koltun, V ., & Funkhouser, T. (2017). Dilated residual networks. In CVPR ( V o l .2 ,p .3 ) . Y u, R., Li, A., Chen, C., Lai, J., et al. (2018). NISP: Pruning networks using neuron importance score propagation. In CVPR. Zafeiriou, S., Zhang, C., & Zhang, Z. (2015). A survey on face detection in the wild: Past, present and future. Computer Vision and Image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S294",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Understanding, 138, 1\u201324. Zagoruyko, S., Lerer, A., Lin, T., Pinheiro, P ., Gross, S., Chintala, S., & Doll\u00e1r, P . (2016). A multipath network for object detection. In BMVC. Zeiler, M., & Fergus, R. (2014). Visualizing and understanding convo- lutional networks. In ECCV (pp. 818\u2013833). Zeng, X., Ouyang, W., Yan, J., Li, H., Xiao, T., Wang, K., et al. (2017). Crafting gbd-net for object detection. IEEE Transactions on Pat- tern Analysis and Machine Intelligence , 40(9), 2109\u20132123. Zeng, X., Ouyang, W., Yang, B., Yan, J., & Wang, X. (2016). Gated bidirectional cnn for object detection. In ECCV (pp. 354\u2013369). 123 International Journal of Computer Vision Zhang, K., Zhang, Z., Li, Z., & Qiao, Y . (2016a). Joint face detection and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S295",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "alignment using multitask cascaded convolutional networks. IEEE SPL, 23(10), 1499\u20131503. Zhang, L., Lin, L., Liang, X., & He, K. (2016b). Is faster RCNN doing well for pedestrian detection? In ECCV (pp. 443\u2013457). Zhang, S., Wen, L., Bian, X., Lei, Z., & Li, S. (2018a). Single shot re\ufb01nement neural network for object detection. In CVPR. Zhang, S., Yang, J., & Schiele, B. (2018b). Occluded pedestrian detec- tion through guided attention in CNNs. In CVPR (pp. 2056\u20132063). Zhang, X., Li, Z., Change Loy, C., & Lin, D. (2017). PolyNet: A pursuit of structural diversity in very deep networks. In CVPR (pp. 718\u2013 726). Zhang, X., Yang, Y ., Han, Z., Wang, H., & Gao, C. (2013). Object class detection: A survey. ACM",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S296",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Computing Surveys , 46(1), 10:1\u201310:53. Zhang, X., Zhou, X., Lin, M., & Sun, J. (2018c). Shuf\ufb02eNet: An extremely ef\ufb01cient convolutional neural network for mobile devices. InCVPR. Zhang, Z., Geiger, J., Pohjalainen, J., Mousa, A. E., Jin, W., & Schuller, B. (2018d). Deep learning for environmentally robust speech recognition: An overview of recent developments.ACM Trans- actions on Intelligent Systems and Technology , 9(5), 49:1\u201349:28. Zhang, Z., Qiao, S., Xie, C., Shen, W., Wang, B., & Y uille, A. (2018e). Single shot object detection with enriched semantics. In CVPR. Zhao, Q., Sheng, T., Wang, Y ., Tang, Z., Chen, Y ., Cai, L., & Ling, H. (2019). M2Det: A single shot object detector based on multilevel feature pyramid network. InAAAI. Zheng, S.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S297",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "Jayasumana, S., Romera-Paredes, B., Vineet, V ., Su, Z., Du, D., Huang, C., & Torr, P . (2015). Conditional random \ufb01elds as recurrent neural networks. InICCV (pp. 1529\u20131537). Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2015). Object detectors emerge in deep scene CNNs. In ICLR. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., & Torralba, A. (2016a). Learning deep features for discriminative localization. In CVPR (pp. 2921\u20132929). Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., & Torralba, A. (2017a). Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 40(6), 1452\u20131464. Zhou, J., Cui, G., Zhang, Z., Yang, C., Liu, Z., & Sun, M. (2018a). Graph neural networks:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S298",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "A review of methods and applications. arXiv:1812.08434. Zhou, P ., Ni, B., Geng, C., Hu, J., & Xu, Y . (2018b). Scale transferrable object detection. In CVPR. Zhou, Y ., Liu, L., Shao, L., & Mellor, M. (2016b). DA VE: A uni\ufb01ed framework for fast vehicle detection and annotation. In ECCV (pp. 278\u2013293). Zhou, Y ., Ye, Q., Qiu, Q., & Jiao, J. (2017b). Oriented response net- works. In CVPR (pp. 4961\u20134970). Zhu, X., Tuia, D., Mou, L., Xia, G., Zhang, L., Xu, F., et al. (2017). Deep learning in remote sensing: A comprehensive review and list of resources.IEEE Geoscience and Remote Sensing Magazine , 5(4), 8\u201336. Zhu, X., V ondrick, C., Fowlkes, C., & Ramanan, D. (2016a). Do we",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S299",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "need more training data? IJCV, 119(1), 76\u201392. Zhu, Y ., Urtasun, R., Salakhutdinov, R., & Fidler, S. (2015). SegDeepM: Exploiting segmentation and context in deep neural networks for object detection. InCVPR (pp. 4703\u20134711). Zhu, Y ., Zhao, C., Wang, J., Zhao, X., Wu, Y ., & Lu, H. (2017a). CoupleNet: Coupling global structure with local parts for object detection. InICCV. Zhu, Y ., Zhou, Y ., Ye, Q., Qiu, Q., & Jiao, J. (2017b). Soft proposal networks for weakly supervised object localization. In ICCV (pp. 1841\u20131850). Zhu, Z., Liang, D., Zhang, S., Huang, X., Li, B., & Hu, S. (2016b). Traf\ufb01c sign detection and classi\ufb01cation in the wild. In CVPR (pp. 2110\u20132118). Zitnick, C., & Doll\u00e1r, P . (2014). Edge",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    },
    {
      "snippet_id": "Pdoi_https_doi_org_10_1007_s11263_019_01247_4:S300",
      "paper_id": "doi:https://doi.org/10.1007/s11263-019-01247-4",
      "section": "method",
      "text": "boxes: Locating object proposals from edges. In ECCV (pp. 391\u2013405). Zoph, B., & Le, Q. (2016). Neural architecture search with reinforce- ment learning. arXiv preprint arXiv:1611.01578. Zoph, B., V asudevan, V ., Shlens, J., & Le, Q. (2018). Learning trans- ferable architectures for scalable image recognition. In CVPR (pp. 8697\u20138710). Publisher\u2019s Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional af\ufb01liations. 123",
      "page_hint": null,
      "token_count": 69,
      "paper_year": 2019,
      "paper_venue": "International Journal of Computer Vision",
      "citation_count": 2673
    }
  ]
}