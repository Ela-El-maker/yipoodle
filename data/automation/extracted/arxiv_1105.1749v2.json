{
  "paper": {
    "paper_id": "arxiv:1105.1749v2",
    "title": "A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control",
    "authors": [
      "Todd Hester",
      "Michael Quinlan",
      "Peter Stone"
    ],
    "year": 2011,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Reinforcement Learning (RL) is a method for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few actions, while continually taking those actions in real-time. Existing model-based RL methods learn in relatively few actions, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.",
    "pdf_path": "data/automation/papers/arxiv_1105.1749v2.pdf",
    "url": "https://arxiv.org/pdf/1105.1749v2",
    "doi": null,
    "arxiv_id": "1105.1749v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.077208+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_1105_1749v2:S1",
      "paper_id": "arxiv:1105.1749v2",
      "section": "body",
      "text": "arXiv:1105.1749v2  [cs.AI]  21 May 2011 A Real-Time Model-Based Reinforcement Learning Architecture for Robot Control Todd Hester, Michael Quinlan, Peter Stone The University of Texas at Austin 1616 Guadalupe, Suite 2.408 Austin, TX 78701 {todd,mquinlan,pstone}@cs.utexas.edu",
      "page_hint": null,
      "token_count": 34,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S2",
      "paper_id": "arxiv:1105.1749v2",
      "section": "abstract",
      "text": "learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few actions, while continually taking those actions in real- time. Existing model-based RL methods learn in relatively f ew actions, but typically take too much time between each actio n for practical on-line learning. In this paper , we present a n ovel parallel architecture for model-based RL that runs in real- time by 1) taking advantage of sample-based approximate plannin g",
      "page_hint": null,
      "token_count": 93,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S3",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "planning processes such that the acting process is suf\ufb01cien tly fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well a s",
      "page_hint": null,
      "token_count": 29,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S4",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "given unlimited time, and greatly out-perform these method s on tasks that require real-time actions such as controlling an autonomous vehicle. I. I NTRODUCTION Robots have the potential to solve many problems in soci- ety by working in dangerous places or performing jobs that no one wants. One barrier to their widespread deployment is the need to hand-program behaviors for every situation they may encounter. For robots to meet their potential, we need methods for them to learn and adapt to novel situations. Reinforcement learning (RL) [1] is a method for learning sequential decision making processes that could solve the problems of learning and adaptation on robots. An RL agent seeks to maximize long-term rewards through experience in its environment.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S5",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "The decision making tasks in these environments are usually formulated as Markov Decision Processes (MDPs). RL has been applied to a few carefully chosen robotic tasks that are achievable with limited training and infrequ ent action selections (e.g. [2]), or allow for an off-line learn ing phase (e.g. [3]). However, none of these methods allow for continual learning on the robot running in its environment. For RL to be practical on tasks requiring lifelong continual control of a robot, such as low-level control tasks, it must meet at least the following two requirements: 1) It must learn in very few actions (be sample ef\ufb01cient ). 2) it must take actions continually in real-time (even while learning). Model-based methods such as",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S6",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "R - MAX [4] are a class of RL algorithms that meet the \ufb01rst requirement by learning a model of the domain from their experiences, and then planning a policy on that model. By updating their policy using their model rather than by taking actions in the real world, they limit the number of real world actions needed to learn. However, most existing model-based methods fail to meet the second requirement because they take signi\ufb01cant period s of (wall-clock) time to update their model and plan between each action. These action times are acceptable when learnin g in simulation or planning off-line, but for on-line robot co n- trol learning, actions must be given at a \ufb01xed, fast frequenc y.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S7",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "Some model-based methods that do take actions at this fast frequency have been applied to robots in the past (e.g. [3], [5]), but they perform learning off-line during pauses where they stop controlling the robot entirely. DYNA [6], which does run in real-time, uses a simplistic model and is not very sample ef\ufb01cient. Model-free methods also learn in real - time, but often take thousands of potentially expensive or dangerous real-world actions to learn: they meet our second requirement, but not the \ufb01rst. The main contribution of this paper is a novel RL architec- ture, called Real-Time Model Based Architecture ( RTMBA ), that is the \ufb01rst to exhibit both sample ef\ufb01cient and real- time learning, meeting both of our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S8",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "requirements. It does so by leveraging recent sample-based approximate plan- ning methods, and most uniquely, by parallelizing model- based methods to run in real-time. WithRTMBA , the crucial computations needed to make model-based methods sample ef\ufb01cient are still performed, but threaded such that action s are not delayed. We compare RTMBA with other methods in simulation when they are all given unlimited time for computation between actions. We then demonstrate that it is the only algorithm among them that successfully learns to control an autonomous vehicle, both in simulation and on the robot. II. B ACKGROUND We adopt the standard Markov Decision Process (MDP) formalism of RL [1]. An MDP consists of a set of states S, a set of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S9",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "actions A, a reward function R(s, a ), and a transition function P (s\u2032|s, a ). In each state s \u2208 S, the agent takes an action a \u2208 A. Upon taking this action, the agent receives a Fig. 1: A diagram of how model learning and planning are typic ally interleaved in a model-based agent. rewardR(s, a ) and reaches a new state s\u2032, determined from the probability distribution P (s\u2032|s, a ). The value Q\u2217(s, a ) of a state-action (s, a ) is an estimate of the expected long-term rewards that can be obtained from (s, a ) and is determined by solving the Bellman equation: Q\u2217(s, a ) =R(s, a ) +\u03b3 \u2211 s\u2032 P (s\u2032|s,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S10",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "a ) max a\u2032 Q\u2217(s\u2032, a \u2032) (1) where 0 < \u03b3 < 1 is the discount factor. The agent\u2019s goal is to \ufb01nd the policy \u03c0 mapping states to actions that maximizes the expected discounted total reward over the agent\u2019s lifetime . The optimal policy \u03c0 is then: \u03c0(s) =argmax aQ\u2217(s, a ) (2) Model-based RL methods learn a model of the domain by approximating R(s, a ) and P (s\u2032|s, a ). The agent then computes a policy by planning on this model with a method such as value iteration [1]. RL algorithms can also work without a model, updating action-values only when taking them in the real task. Generally model-based methods are more sample ef\ufb01cient than model-free",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S11",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "methods; their sample ef\ufb01ciency is only constrained by how many actions it takes to learn a good model. Figure 1 shows the typical architecture for a model-based algorithm. When the agent gets its new state, s\u2032, and reward, r, it updates its model with the new transition \u27e8s, a, s \u2032, r \u27e9. Once the model has been updated, it computes a new policy by re-planning on its model. The agent then returns the actio n for its current state determined by its policy. Each of these computations is performed sequentially and both the model learning and planning can take signi\ufb01cant time. The DYNA framework [6] presents an alternative to this",
      "page_hint": null,
      "token_count": 111,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S12",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "based methods while still running in real-time.DYNA saves its experiences, and then performs k Bellman updates on randomly selected experiences between each action. Instea d of performing full value iteration between each action as above, its planning is broken up into a few updates between each action. However, it uses a very simplistic model (saved experiences) and thus does not have very good sample ef\ufb01ciency. In the next section, we introduce a novel parallel architecture to allow more sophisticated model-based algo - rithms to run in real-time regardless of how long the model learning or planning may take. III. T HE A RCHITECTURE We make two main modi\ufb01cations to the standard model- based paradigm that, together, allow it to run",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S13",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "in real-time. Fig. 2: A diagram of the proposed parallel architecture for r eal-time model-based RL. First, we limit planning time by using approximate instead of exact planning. Second, we parallelize the model learning, planning, and acting such that the computation-intensive processes (model learning and planning) are spread out over time. Actions are produced as quickly as dictated by the robot control loop, while still being based on the most recent models and plans available. First, instead of planning exactly with value iteration (li ke methods such as R - MAX ), our method follows the approach of Silver et al. [7] (among others) in using a sample- based planning algorithm from the Monte Carlo Tree Search (MCTS) family (such",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S14",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "as Sparse Sampling [8] orUCT [9]) to plan approximately . These sample-based planners perform rollouts from the agent\u2019s current state, sampling ahead to update the values of the sampled actions. The agent performs as many rollouts as it can in the given time, with its value estimate improving with more rollouts. These methods can be more ef\ufb01cient than dynamic programming approaches in large domains because they focus their updates on states the agent is likely to visit soon rather than iterating over the entire statespace. Second, since both the model learning and planning can take signi\ufb01cant computation (and thus also wall-clock time ), we place both of those processes in their own parallel thread s in the background, shown in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S15",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "Figure 2. A third thread interact s with the environment, receiving the agent\u2019s new state and reward and returning the action given by the agent\u2019s current policy. By de-coupling this action thread from the time- consuming model-learning and planning processes,RTMBA releases the algorithm from the need to complete the model update and planning between actions. Now, it can return an action immediately whenever one is requested by the environment. For the three threads to operate properly, they must share information while avoiding race conditions and data inconsistencies. The model learning thread must know which new transitions to add to its model, the planning thread must access the model being learned, the planner must know what state the agent is currently",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S16",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "at, and the action thread must access the policy being planned.RTMBA uses mutex locks to control access to these variables, as summarized in Table I. The action thread receives the agent\u2019s new state and reward, and adds the new transition experience, \u27e8s, a, s \u2032, r \u27e9, to a list to be updated into the model. It then sets the agent\u2019s current state for use by the planner and returns the action determined by the agent\u2019s policy. The update list and current state are both protected by mutex locks, and the agent\u2019s policy is protected by individual mutex locks for each state . The model learning thread checks if there are any expe- riences in the update list to be",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S17",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "added to its model. If there are, it makes a copy of its model, updates it with the new experiences, and replaces the original model with the copy. The other threads can continue accessing the original model while the copy is being updated. Only the swapping of the models requires locking the model mutex. After updating the model, the model learning thread repeats, checking for new experiences to add to the model. The model learning thread can incorporate any type of model learning algorithm, such as a tabular model [4], random forests [10] (as used in this paper), or Gaussian Process regression [5]. Depending on how long the model update takes and how fast the agent is acting, the agent",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S18",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "can add tens or hundreds of new experiences to its model at a time, or it can wait for long periods for a new experience. When adding many experiences at a time, full model updates are not performed between each individual action. In this case, the algorithm\u2019s sample ef\ufb01ciency is likely to suffer compared to that of sequential methods, but in exchange, it continues to act in real time. The planning thread uses any MCTS planning algorithm to plan approximately (we use a variant of UCT ). It retrieves the agent\u2019s current state and its sample-based planner perform s a rollout from that state. The thread repeats, continually performing rollouts from the agent\u2019s current state. With mo re rollouts, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S19",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "algorithm\u2019s estimates of action values impro ve, resulting in more accurate policies. Even if very few rollou ts are performed from the current state before the algorithm returns an action, many of the rollouts performed from the previous state should have gone through the current state (if the model is accurate), giving the algorithm a good estimate of its true value. The action thread returns actions in real-time. When an V ariable Threads Use Update List Action Store experiences to Model Learning be updated into model Current State Action Set current state Planning to plan from Policy (by state) Action Update policy used (V alue Function) Planning to select actions Model Model Learning Latest model Planning to plan on TABLE",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S20",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "I: This table shows all the variables that are protecte d under mutex locks in the proposed architecture, along with their p urpose and which threads use them. action is requested, the action thread only has to add an experience to the update list, set the agent\u2019s current state , and access the agent\u2019s policy to return an action. All of thes e items are under mutex locks, but the update list is only used by the model learning thread between model updates, the agent\u2019s current state is only accessed by the planning thread between each rollout, and the policy is under individual loc ks for each state. Thus, any given state is freely accessible mo st of the time.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S21",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "When the planner does happen to be using the same state the action thread wants, it releases it immediate ly after updating the values for that state. In addition to enab ling real-time action, this architecture enables the agent to ta ke full advantage of multi-core processors by running each thread on a separate core.1 IV. E XPERIMENTS To demonstrate the effectiveness of this architecture, we performed experiments on two problems. Our \ufb01rst experi- ments measure the cost of parallelization in terms of en- vironmental reward compared to a traditional sequential architecture. We use the standard toy domain of mountain car, in which the simulated environment can wait as long as necessary for the agent to return an action (or",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S22",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "it can execute actions as fast as the algorithm returns them). Our second set of experiments measure the performance gains due to parallelization on an autonomous vehicle, where real-time actions are absolutely necessary. We perform experiments, both in simulation and on the robot, that show that existing sequential approaches are not a viable option on this type of problem. A. Mountain Car Fig. 3: Mountain Car Our \ufb01rst experiments were performed in the Mountain Car domain [1], shown in Figure 3. Mountain Car is a continuous task, where the agent controls an under-powered car that does not have enough power to drive directly up the hill to the goal. Instead, it must go up the left- ward slope to gain",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S23",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "momentum \ufb01rst. The agent has three actions, accelerating it leftward, rightward, or not at all. The agent\u2019s state is made up of two features: its POSITION and its VELOCITY . The agent receives a reward of \u22121 each time step until it reaches the goal, when the episode terminates with a reward of0. We discretized both state features into 100 values each, and ran the algorithms on the discretized version of the domain. Following the evaluation methodolog y of Hester and Stone [10], each algorithm was initialized wit h one experience ( \u27e8s, a, s \u2032, r \u27e9 tuple) of the car reaching the goal to jump start learning. We ran experiments with a typical model-free RL method ( Q",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S24",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "- LEARNING [11]), DYNA , two sequential model-based",
      "page_hint": null,
      "token_count": 8,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S25",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "1 Source code for the architecture is available at: http://www.ros.org/wiki/rl-texplore-ros-pkg . -600 -500 -400 -300 -200 -100 0 0 10 20 30 40 50 Average Reward Episode Number Mountain Car Fig. 4: Average reward per episode on Mountain Car, averaged over 30 trials. Results are averaged over a 4 episode sliding wind ow. experiences between each action. The sequential methods varied in their planning; one used value iteration for exact planning and one usedMCTS for approximate planning. We modi\ufb01ed MCTS to use UCT action selection [9], eligibility traces, and to generalize values across depths in the search tree. Between each action, the two sequential methods per- formed a full model update, then planned on their model by running value iteration",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S26",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "to convergence or runningMCTS for 0.1 seconds. We compared these algorithms with RTMBA using the same version of MCTS , running at three different action rates: 10 Hz, 25 Hz, and 100 Hz. All of the algorithms used random forests to model the domain, similar to the approach taken by Hester and Stone [10]. We ran 30 trials of each algorithm learning for 1,000 episodes in the domain. Each trial was run on a single core of a machine with 2.4 - 2.66 GHz Intel Xeon processors and 4 GB of memory. Our aim was to compare the real-time algorithms with the sequential methods when they were given the time needed to fully complete their computation between each step. Thus",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S27",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "we can examine the performance lost by the real-time algorithms due to acting quickly. In contrast, the model-free methods could act as fast as they wanted, resulting in learning that took little wall clock time but many more actions. To perform these experiments, the environment waited for each algorithm to return its action. This is only possible in simulation, whereas on a real robot, the action rate is de\ufb01ned by the robot rather than the algorithm. Figure 4 shows the average reward per episode for each al- gorithm over the \ufb01rst 50 episodes in the domain and Figure 5 shows the reward plotted against clock time in seconds (note the log scale on thex axis). The \ufb01rst plot shows that",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S28",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "the two sequential methods perform better than RTMBA in sample ef\ufb01ciency, in particular, receiving signi\ufb01cantly more rew ard per episode than RTMBA running at 25 and 100 Hz over the \ufb01rst 5 episodes ( p < 0. 05). RTMBA running at 10 Hz did not perform signi\ufb01cantly worse than the sequential method usin g MCTS . However, Figure 5 shows that better performance of the sequential methods came at the cost of more computation time. For the sequential methods, switching from exact to approximate planning reduces the time to complete the \ufb01rst -600 -500 -400 -300 -200 -100 0 0.1 1 10 100 1000 10000 100000 Average Reward Time (s) Mountain Car Q-Learning Dyna Sequential VI Sequential MCTS RTMBA 10",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S29",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "Hz RTMBA 25 Hz RTMBA 100 Hz Fig. 5: Average reward versus clock time on Mountain Car, ave raged over 30 trials. Results are averaged over a 4 episode sliding window. Note that the x-axis is in log scale. -300 -250 -200 -150 -100 -50 0 2 4 6 8 10 12 14 16 18 Reward Episode Number Mountain Car: Multiple vs. Single Core RTMBA - Multiple Cores RTMBA - Single Core Sequential MCTS - Multiple Cores Fig. 6: Comparisons of the methods using a multiple core mach ine. Each method is averaged over 30 trials on Mountain Car. episode from 1541 to 142 seconds, but the MCTS method is still restricted by the need to perform complete model updates",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S30",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "between actions. This restriction is removed with RTMBA , and all three versions using it complete the \ufb01rst episode within 20 seconds. In fact, all three RTMBA methods start performing well after 120 seconds, likely because the y all took this much time to learn an accurate model of the domain. Compared with the sequential methods, RTMBA is only slightly worse in sample ef\ufb01ciency, and is able to act much faster, meeting our second requirement of continual real-time action selection. The two model-free approaches, Q - LEARNING and DYNA , select actions extremely quickly and converge to the optima l policy in less wall clock time than any version of RTMBA . However, Figure 4 shows that they are not",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S31",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "as sample ef\ufb01cient. WhileRTMBA converges to the optimal policy within tens of episodes, DYNA takes approximately 650 episodes to converge, and Q - LEARNING takes approximately 22,000. These methods learn in less wall clock time simply because they are able to take many more actions thanRTMBA in a given amount of time. On an actual robot, it will not be possible to take actions faster than the robot\u2019s control frequency, and the poor sample ef\ufb01ciency of these methods will result in longer wall clock learning times as well. In comparison,RTMBA learns in fewer actions, meeting our \ufb01rst requirement of sample ef\ufb01ciency even while running at reasonable robot control rates between 10 and 100 Hz. In addition to enabling real-time learning,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S32",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "another bene\ufb01t of RTMBA is its ability to take advantage of multi-core processors. We ran experiments comparing the performance ofRTMBA when running on one versus multiple cores. These",
      "page_hint": null,
      "token_count": 28,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S33",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "AMD Opteron processors. Figure 6 shows the average reward per episode for these experiments, running at 25 Hz. For comparison, we ran the sequential method usingMCTS as a planner on the multi-core machine. It had unlimited time for model updates and then planned for 0.04 seconds (the same time given toRTMBA for both computations). Since the sequential architecture only has a single thread, it only us ed a single core even on the multi-core machine. Meanwhile, RTMBA utilized three processors with each thread running on its own core. Using the extra processors allowed the parallel version to perform more model updates and planning rollouts between actions than the single core version. Due to these advantages, the multi-core version performs better",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S34",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "than the single core version, receiving signi\ufb01cantly more rewards on every episode (p < 0. 005). In addition, it even performs better than the sequential method on episodes 3 to 14 (p < 0. 01), even though the sequential method is given unlimited time for model updates. These results demonstrate that the algorithms using our real-time architecture are able to accomplish both require - ments set forth in the introduction (sample ef\ufb01ciency and real-time action selection), while existing model-free an d model-based methods are each only able to accomplish one of the two requirements. We have demonstrated that while using approximate planning reduces the time required by model-based methods, they do not reach real-time perfor- mance without our parallel architecture.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S35",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "While agents using RTMBA do not learn as much as the sequential methods per action due to the limited time between actions, they still to ok no more than 5 extra episodes to learn the task. In addition, they were able to learn the task much faster in wall clock time than the sequential algorithms and perform better when run on multiple cores. Next, we look at how the algorithms compare on a task thatrequires real-time actions, where the world will not wait while the agent decides what to do. B. Autonomous V ehicle Our next task was to control an autonomous vehicle. Here, actions must be taken in real-time, as the car cannot wait for an action while a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S36",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "car is stopping in front of it or it approaches a turn in the road. This task was the main motivator for the creation ofRTMBA . To the best of our knowledge, no prior RL algorithm is able to learn in this domain in real time : with no prior data-gathering phase for training a model. These",
      "page_hint": null,
      "token_count": 57,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S37",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "autonomous vehicle [12], and on its simulation in ROS stage [13]. The vehicle is an Isuzu V ehiCross (Figure 7) that Fig. 7: The autonomous vehicle operated by Austin Robot Tech - nology and The University of Texas at Austin. has been upgraded to run autonomously by adding shift-by- wire, steering, and braking actuators to the vehicle. Our experiments were to learn to drive the vehicle at a desired velocity by controlling the pedals. For learning this task, the RL agent\u2019s state was the desired velocity of the vehicle, the current velocity, and the current position of the brake and accelerator pedals. Desired velocity was discretized into 0.5 m/s increments, current velocity into 0.1 m/s increments, and the pedal positions",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S38",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "into tenths of maximum position. The agent\u2019s reward at each step was\u221210 times the error in velocity in m/s. Each episode was run at 20 Hz (the frequency that the vehicle receives new sensations) for 10 seconds. The agent had 5 actions: one did nothing (no-op), two increased or decreased the brake position by 0.1 while setting the accelerator to 0, and two increased or decreased the accelerator position by 0.1 while setting the brake position to 0. The autonomous vehicle software uses ROS [13] as the underlying middleware. We created an RL Interface node that wraps sensor values intostates , translates actions into actuator commands, and generates reward . This node uses a standard set of messages to communicate",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S39",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "with the learning algorithm2 , similar to the messages used by RL - GLUE [14]. At each time step, it computes the current state and reward and publishes them as a message to the RL agent. The RL agent can then process this information and publish an action message, which the interface will convert into actuator commands. Whereas the RL agents using RTMBA respond with an action message immediately after receiving the stat e and reward message, the sequential methods may have a long delay to complete model updates and planning before sending back an action message. In this case, the vehicle continues with all the actuators in their current positionsuntil it receives a new action message. We ran the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S40",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "\ufb01rst experiment in the ROS stage simulation with the vehicle starting at 2 m/s with a target velocity of 7 m/s. Figure 8 shows the average rewards per episode for this task. Again the model-free methods are not able to learn the task within the given number of episodes. As before, 2 These messages are de\ufb01ned at: http://www.ros.org/wiki/rl_msgs -20000 -15000 -10000 -5000 0 0 10 20 30 40 50 Average Reward Episode Number Simulated Vehicle Velocity Control from 2 to 7 m/s Q-Learning Dyna RTMBA Sequential VI Sequential MCTS Fig. 8: Average rewards of the algorithms controlling the au - tonomous vehicle in simulation from 2 to 7 m/s. Results are averaged over a 4 episode sliding window. -7000 -6000",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S41",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "-5000 -4000 -3000 -2000 -1000 0 0 200 400 600 800 1000 1200 1400 Average Reward Episode Number Simulated Vehicle Control between Random Velocities Learned Policy PID Controller Fig. 9: Average rewards of the algorithms controlling the au - tonomous vehicle in simulation from between random velocit ies. Results are averaged over a 50 episode sliding window. 1.5 2 2.5 3 3.5 4 4.5 5 5.5 6 0 2 4 6 8 10 Velocity (m/s) Time (s) Autonomous Vehicle Learning Trials Episode 17 Episode 10 Episode 1 Episode 0 Target Velocity Fig. 10: Control pro\ufb01les of learning trials performed on the physical vehicle. planning approximately with MCTS is better than perform- ing exact planning, but using RTMBA is better than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S42",
      "paper_id": "arxiv:1105.1749v2",
      "section": "experiments",
      "text": "either. The varying time taken between actions by the sequential",
      "page_hint": null,
      "token_count": 10,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S43",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "vehicle will have accelerated/decelerated by varying amounts between each action. In only a few minutes, RTMBA learns to quickly accelerate to and maintain a velocity of 7 m/s. Next, we evaluated RTMBA on the full velocity control problem, with starting and target velocities selected rand omly from between 0 and 11 m/s. Figure 9 shows the reward accrued by the RL agent on each episode in the simulator while learning this task. For comparison, we show the reward that would be received by the PID controller that was previously used for controlling the car\u2019s velocity. The previous controller was hand-tuned for performance on the actual car. The learned controller received more reward than the PID controller after episode 350, which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S44",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "equates to about 1 hour of driving. It was signi\ufb01cantly better than the PID controller (p < 0. 005) after episode 750. After testing in simulation, we ran learning experiments in real-time on the physical vehicle, learning to drive at 5 m/s from a start of 2 m/s. The velocity curves for a few of the 20 episodes are shown in Figure 10. Similar to the simulation results for 2 to 7 m/s, the algorithm learned quickly and was able to accurately track the velocity after 18 episodes (3 minutes of driving). V. R ELATED W ORK Batch methods such as experience replay [15], \ufb01tted Q- iteration [16], and LSPI [17] improve the sample ef\ufb01ciency of model-free methods by saving",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S45",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "experiences and re-using them in periodic batch updates. However, these methods typicall y run one policy for a number of episodes, stop to perform their batch update, and then repeat. Our architecture will also update the model with batches of experience at a time when the agent is acting faster than it can update the model. However,RTMBA continues taking actions in real-time even while these updates are occurring. DYNA [6] takes a similar approach to these methods, performing small batch updates between each action. The DYNA -2 framework [7] extends DYNA to use UCT as its planning algorithm, combined with permanent and transient memories using linear function approximation. This im- proves the planning performance of the algorithm, but the sample",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S46",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "ef\ufb01ciency of these methods still does not meet the requirements for on-line learning laid out in the introduction. Deisenroth and Rasmussen [5] develop a sample ef\ufb01cient model-based algorithm that uses Gaussian Process regressi on to compute the model and the policy. It runs in batches, col- lecting experiences with the current policy before stoppin g to update its model and plan. The algorithm learns to control a physical cart-pole device with few samples, but pauses for 10 minutes of computation after every 2.5 seconds of action. RTMBA similarly does batch-type updates to its model, but its parallel architecture allows it to act continually in th e domain while performing these updates. In summary, while there is related work on making",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S47",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "model- free methods more sample-ef\ufb01cient and making model-based",
      "page_hint": null,
      "token_count": 8,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S48",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "have long pauses in learning to perform batch updates, or require complete model update or planning steps between actions. None of these methods accomplish both goals of being sample ef\ufb01cient and acting continually in real-time, whileRTMBA accomplishes both. VI. C ONCLUSION For RL to be practical for continual, on-line learning on a broad range of robotic tasks, it must both (1) be sample- ef\ufb01cient and (2) learn while taking actions continually in real-time. This paper introduces a novel parallel architecture for model-based RL that is the \ufb01rst to enable an agent to act in real-time while maintaining the sample ef\ufb01ciency of model-based RL. It uses sample-based approximate planning and performs model learning and planning in parallel threads, while a third",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S49",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "thread returns actions at a rate dictated by the task. In addition,RTMBA enables RL algorithms to take advantage of the multi-core processors available on many robotic platforms. Our experiments, in simulation and on a real robot, demonstrate that this architecture is necessary for learning on robots that require fast real-time actions. Our ongoing research agenda includes testingRTMBA on other robotic platforms, as well as testing other model learning andMCTS planning algorithms within the framework. C ODE Source code for the real-time architecture, algorithms, and experiments described in this paper are available as part of a ROS repository available at: http://www.ros.org/wiki/rl-texplore-ros-pkg The architecture and theTEXPLORE algorithm used for model learning are available in that repository in theRL AGENT package available at:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S50",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "http://www.ros.org/wiki/rl_agent In addition, the mountain car task and a simpli\ufb01ed version of the autonomous vehicle velocity control task are also available in the repository in theRL ENV package available at: http://www.ros.org/wiki/rl_env Finally, the de\ufb01nitions of the ROS messages used to communicate between agent and environment are available in the repository in theRL MSGS package available at: http://www.ros.org/wiki/rl_msgs The experimental results presented in this paper can be reproduced easily. For example, to run TEXPLORE controlling the simulated vehicle from 2 to 7 m/s, with theRL EXPERIMENT package installed, type: rosrun rl experiment experiment --agent texplore --env car2to7 --actrate 20 A CKNOWLEDGMENTS This work has taken place in the Learning Agents Re- search Group (LARG) at the Arti\ufb01cial Intelligence Labora- tory, The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S51",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "University of Texas at Austin. LARG research is supported in part by grants from the National Science Foun- dation (IIS-0917122), ONR (N00014-09-1-0658), and the Federal Highway Administration (DTFH61-07-H-00030). R EFERENCES [1] R. Sutton and A. Barto, Reinforcement Learning: An Introduction . Cambridge, MA: MIT Press, 1998. [2] N. Kohl and P . Stone, \u201cMachine learning for fast quadrupe dal locomo- tion,\u201d in Proceedings of the Nineteenth AAAI Conference on Arti\ufb01cial Intelligence , 2004. [3] A. Ng, H. J. Kim, M. Jordan, and S. Sastry, \u201cAutonomous hel icopter \ufb02ight via reinforcement learning,\u201d in Advances in Neural Information Processing Systems (NIPS) 16 , 2003. [4] R. Brafman and M. Tennenholtz, \u201cR-Max - a general polynom ial time algorithm for near-optimal reinforcement learning,\u201d",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S52",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "in Proceedings of the Seventeenth International Joint Conference on Arti\ufb01ci al Intelli- gence (IJCAI) , 2001, pp. 953\u2013958. [5] M. Deisenroth and C. Rasmussen, \u201cEf\ufb01cient reinforcemen t learning for motor control,\u201d in 10th International PhD W orkshop on Systems and Control , Hluboka nad Vltavou, Czech Republic, Sept. 2009. [6] R. Sutton, \u201cIntegrated architectures for learning, pla nning, and reacting based on approximating dynamic programming,\u201d in Proceedings of the Seventh International Conference on Machine Learning (ICM L) , 1990, pp. 216\u2013224. [7] D. Silver, R. Sutton, and M. M\u00a8 uller, \u201cSample-based lear ning and search with permanent and transient memories,\u201d in Proceedings of the Twenty-Fifth International Conference on Machine Learnin g (ICML) , 2008, pp. 968\u2013975. [8] M. Kearns, Y",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S53",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": ". Mansour, and A. Ng, \u201cA sparse sampling algor ithm for near-optimal planning in large Markov Decision Processes, \u201d in Pro- ceedings of the Sixteenth International Joint Conference o n Arti\ufb01cial Intelligence (IJCAI) , 1999, pp. 1324\u20131331. [9] L. Kocsis and C. Szepesv\u00b4 ari, \u201cBandit based Monte-Carlo planning,\u201d in Proceedings of the Seventeenth European Conference on Mach ine Learning (ECML) , 2006. [10] T. Hester and P . Stone, \u201cReal time targeted exploration in large domains,\u201d in Proceedings of the Ninth International Conference on Development and Learning (ICDL) , August 2010. [11] C. Watkins, \u201cLearning from delayed rewards,\u201d Ph.D. dis sertation, University of Cambridge, 1989. [12] P . Beeson, J. O\u2019Quin, B. Gillan, T. Nimmagadda, M. Ristr oph, D. Li,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S54",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "and P . Stone, \u201cMultiagent interactions in urban driving,\u201d Journal of Physical Agents , vol. 2, no. 1, pp. 15\u201330, March 2008. [13] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. L eibs, R. Wheeler, and A. Ng, \u201cROS: an open-source robot operating system,\u201d inICRA W orkshop on Open Source Software , 2009. [14] B. Tanner and A. White, \u201cRL-Glue : Language-independen t software for reinforcement-learning experiments,\u201d Journal of Machine Learning Research , vol. 10, pp. 2133\u20132136, September 2009. [15] L.-J. Lin, \u201cReinforcement learning for robots using ne ural networks,\u201d Ph.D. dissertation, Pittsburgh, PA, USA, 1992. [16] D. Ernst, P . Geurts, and L. Wehenkel, \u201cIteratively exte nding time horizon reinforcement learning,\u201d in Proceedings of the F",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1105_1749v2:S55",
      "paper_id": "arxiv:1105.1749v2",
      "section": "method",
      "text": "ourteenth European Conference on Machine Learning (ECML) , 2003, pp. 96\u2013 107. [17] M. Lagoudakis and R. Parr, \u201cLeast-squares policy itera tion,\u201d Journal of Machine Learning Research , vol. 4, pp. 1107\u20131149, 2003.",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2011,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9523038418967826,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 7,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5046,
        "empty": false
      },
      {
        "page": 2,
        "chars": 4663,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5877,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4524,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5551,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4051,
        "empty": false
      },
      {
        "page": 7,
        "chars": 5960,
        "empty": false
      }
    ],
    "quality_score": 0.9523,
    "quality_band": "good"
  }
}