{
  "paper": {
    "paper_id": "arxiv:2407.21017v1",
    "title": "Matting by Generation",
    "authors": [
      "Zhixiang Wang",
      "Baiang Li",
      "Jian Wang",
      "Yu-Lun Liu",
      "Jinwei Gu",
      "Yung-Yu Chuang",
      "Shin'ichi Satoh"
    ],
    "year": 2024,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues. Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality. The project page for this paper is available at https://lightchaserx.github.io/matting-by-generation/",
    "pdf_path": "data/automation/papers/arxiv_2407.21017v1.pdf",
    "url": "https://arxiv.org/pdf/2407.21017v1",
    "doi": null,
    "arxiv_id": "2407.21017v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.074466+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2407_21017v1:S1",
      "paper_id": "arxiv:2407.21017v1",
      "section": "body",
      "text": "Matting by Generation Zhixiang Wang The University of Tokyo Tokyo, Japan wangzx@nii.ac.jp Baiang Li Hefei University of Technology Hefei, China ztmotalee@gmail.com Jian Wang\u2020 Snap Research New York, USA jwang4@snap.com Yu-Lun Liu National Yang Ming Chiao Tung University Hsinchu, Taiwan yulunliu@cs.nycu.edu.tw Jinwei Gu The Chinese University of Hong Kong Hong Kong SAR jwgu@cuhk.edu.hk Yung-Yu Chuang National Taiwan University Taipei, Taiwan cyy@csie.ntu.edu.tw Shin\u2019ichi Satoh National Institute of Informatics Tokyo, Japan satoh@nii.ac.jp Input ViTAE-S [Ma et al. 2023] Ours Human annotation Figure 1: Matting by Generation. We crack the trimap-free matting problem in a conditional generative way as opposed to the previous regression-based fashion. With only an image as input, our method automatically extracts the foreground ( e.g., person) and generates high-quality boundary",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S2",
      "paper_id": "arxiv:2407.21017v1",
      "section": "body",
      "text": "details benefiting from the rich generative prior, leading to photorealistic compositions. Compared with the human annotation, our results provide crisper details and greater fidelity to the input image in this example.",
      "page_hint": null,
      "token_count": 31,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S3",
      "paper_id": "arxiv:2407.21017v1",
      "section": "abstract",
      "text": "This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent \u2020Corresponding author Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA \u00a9 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0525-0/24/07. https://doi.org/10.1145/3641519.3657519 diffusion models,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S4",
      "paper_id": "arxiv:2407.21017v1",
      "section": "abstract",
      "text": "enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with supe- rior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, ac- commodating a variety of additional cues. Our comprehensive eval- uation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method\u2019s robust effectiveness but also highlight its ability to generate visually compelling mattes that",
      "page_hint": null,
      "token_count": 88,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S5",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "at https://lightchaserx.github.io/matting-by-generation/. arXiv:2407.21017v1  [cs.CV]  30 Jul 2024 SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Wang, Li, Wang, Liu, Gu, Chuang, and Satoh CCS CONCEPTS \u2022 Computing methodologies \u2192Image manipulation ; Ma- chine learning approaches ; KEYWORDS Diffusion models, image matting ACM Reference Format: Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, and Shin\u2019ichi Satoh. 2024. Matting by Generation . In Special Interest Group on Computer Graphics and Interactive Techniques Confer- ence Conference Papers \u201924 (SIGGRAPH Conference Papers \u201924), July 27- August 1, 2024, Denver, CO, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3641519.3657519",
      "page_hint": null,
      "token_count": 102,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S6",
      "paper_id": "arxiv:2407.21017v1",
      "section": "introduction",
      "text": "Image matting as a fundamental problem in computer vision has been investigated for decades [Li et al. 2023d]. It enables many real applications, such as visual effects synthesis [Li et al. 2022a], image editing [Kawar et al. 2023], etc. Its goal is to predict the foreground and the alpha matte from an input image. This is a highly ill-posed inverse problem with only the input being known. The forward model is the composition equation [Porter and Duff 1984] given by: \ud835\udc36 = \ud835\udefc\ud835\udc39 +(1 \u2212\ud835\udefc)\ud835\udc35, (1) where \ud835\udc36 is the input, \ud835\udc39 is the foreground, \ud835\udc35is the background, and \ud835\udefc \u2208[0,1]is the linear combination coefficient. The main challenge lies in the ill-posedness, which is a mixed difficulty \u2014 to find",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S7",
      "paper_id": "arxiv:2407.21017v1",
      "section": "introduction",
      "text": "where the foreground is and what the opacity value is in the boundary. Existing methods, regardless of traditional or learning-based, leverage additional inputs to reduce the ill-posedness. For example, one could mitigate unknown parameter \ud835\udc35 by capturing another background image [Lin et al. 2021; Sengupta et al. 2020], or could add priors for \ud835\udefc through user annotated trimaps.1 Besides using additional input provided by humans, methods [Li et al. 2023a; Yu et al. 2021] employing rough masks from other algorithms, such as Segment Anything (SAM) [Kirillov et al. 2023a], aim to alleviate the training burden in segmentation and enhance focus on boundary matte quality. However, these approaches are not entirely satisfac- tory, primarily due to their reliance on the segmentation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S8",
      "paper_id": "arxiv:2407.21017v1",
      "section": "introduction",
      "text": "network\u2019s accuracy. Imprecise initial segmentation can significantly compro- mise the quality of matting results, particularly at the boundaries. This dependency raises concerns about the efficacy of solely relying on rough segmentation masks for achieving high-quality matting. Recent advancements in end-to-end matting methods [Ke et al. 2022; Li et al . 2021] have sought to address these limitations by eliminating the need for these additional inputs, thereby reducing the reliance on human-generated data. Nevertheless, developing an effective end-to-end matting algorithm from scratch poses sig- nificant challenges due to the task\u2019s inherent complexity. These",
      "page_hint": null,
      "token_count": 92,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S9",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "cation domain to portrait images [Li et al. 2021; Ma et al. 2023] and imposing implicit segmentation prior [Ke et al. 2022]. While these",
      "page_hint": null,
      "token_count": 24,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S10",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "1Even with a known background image \ud835\udc35, the problem is still ill-posed (3 unknowns for foreground color, plus unknown alpha, for 3 equations across R, G, B channels.) Training sample Annotation ViTAE-S Ours Figure 2: Imperfect human annotation. The training data are usually either blurry or lacking in some details. There- fore, the regression-based model would overfit the imperfect ground truth. and encourage the model to capture boundary details more effec- tively, achieving high-quality boundary mattes remains challenging, as shown in Figure 1. The prevailing issue with existing matting approaches lies in their handling of boundary regions, which are of- ten challenging due to factors such aslow visibility (contrast, image quality) and imperfect human annotations2. These limitations can result in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S11",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "unnatural compositions, highlighting the need for more sophisticated solutions. In this paper, we propose a simple yet effective technique of mat- ting by generation. We transform the traditionalregression problem into a conditional generative modeling problem, leveraging a dif- fusion model enriched with pre-trained knowledge about image semantics and matte details. There are several key advantages to this approach. Firstly, the generative model is adept at handling the uncertainties inherent in data, enabling it to learn the matte distri- bution more effectively than regression models. It also allows us to mitigate the negative impact of imperfect labels, such as ground- truth (GT) mattes generated by either humans or machines. These GT mattes, often derived from low-level image features, tend to con-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S12",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "tain imperfections, as exemplified in Figure 1. Utilizing such flawed mattes to train a regression-based model can lead to overfitting and suboptimal outcomes, as demonstrated in Figure 2. In contrast, our generative prior empowers our method to identify semantically correct boundaries and even generate results surpassing the GT mattes\u2019 quality. Secondly, our pre-trained diffusion model, with its vast database of billions of images, captures a more comprehensive image distribution. This broader understanding aids in regularizing the training process, offering more detailed and low-level property insights. Thus, the generative capabilities of our model shine in sce- narios where image visibility is limited. Finally, our method offers versatility, accommodating both guidance-free and guidance-based matting. In most instances, it can perform accurate matting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S13",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "without additional hints. Nevertheless, in cases where the foreground is ambiguous, users can provide supplementary guidance to extract the desired matte. List of Contributions. Our research makes the following three significant contributions: \u2022We convert the regression problem into a generative model- ing problem, utilizing generative diffusion prior to regularize training effectively. 2Although there are solutions for capturing ground-truth alpha matte [Smirnov et al. 2023], they often involve hardware requirements and are hard to scale up. Matting by Generation SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA \u2022We develop a model capable of processing high-resolution inputs efficiently and effectively. \u2022Our model is versatile and capable of handling scenarios with a variety of hints, including trimaps, masks, texts, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S14",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "no hints at all.",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S15",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "Guidance-based Matting. Given only a single image, matting is an ill-posed inverse problem. Therefore, some matting meth- ods require additional guidance, such as trimaps [Chuang et al . 2001], scribles [Levin et al. 2008], and clicks [Wei et al. 2021]. Meth- ods of this category are typically referred to as guidance-based or trimap-based methods. Conventional trimap-based matting meth- ods can be roughly divided into two categories: sampling-based methods [Chuang et al. 2001; Feng et al. 2016; Gastal and Oliveira 2010; He et al. 2011; Shahrian et al. 2013; Wang and Cohen 2007; Yang et al. 2018] and propagation-based methods [Aksoy et al. 2017; Chen et al. 2013; Grady et al. 2005; Levin et al. 2008; Sun et al. 2004; Wang",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S16",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "and Cohen 2007]. Sampling-based methods usually resolve matting on a pixel-by-pixel basis by collecting color samples and forming a probabilistic distribution for each pixel\u2019s neighborhood. In contrast, propagation-based methods aim to obtain the matte for the entire image at once by establishing pixel affinities and solving an equation. Complex scenes often pose a challenge to these meth- ods. In recent years, deep learning has been introduced to solve the matting problem and gained success [Cho et al. 2019; Liu et al. 2021; Lu et al. 2019a,b; Sun et al. 2021; Xu et al. 2017]. For example, Mask-guided matting [Yu et al. 2021] takes a general coarse mask as guidance, and proposes a Progressive Refinement Network mod- ule to achieve robust",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S17",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "guidance. Matting Anything [Li et al. 2023a] leverages the recent Segment Anything Model (SAM), and further proposes a model that can estimate the alpha matte of any target instance with prompt-based user guidance in an image. Guidance-free Matting. Given the considerable expense associ- ated with acquiring additional guidance, efforts have been made to conduct matting without them, especially for specific foreground scenarios like portraits. These approaches are commonly referred to as guidance-free or trimap-free methods. Example methods of this type include SHM [Chen et al. 2018], SHMC [Liu et al. 2020], HATT [Qiao et al. 2020] and GFM [Li et al. 2022b]. MODNet [Ke et al. 2022] performs portrait matting by optimizing a series of sub- objectives simultaneously via explicit",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S18",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "constraints. DugMatting [Wu et al. 2023] explores the explicitly decomposed uncertainties to effi- ciently and effectively improve matting. P3M-Net [Ma et al. 2023] specifically models the interactions between encoders and decoders to perform privacy-preserving portrait matting better. Diffusion Models. Our approach builds upon the diffusion model [Ho et al. 2020; Song et al. 2021], a generative model that has gar- nered significant attention owing to its exceptional generative ca- pabilities [Rombach et al. 2022]. Diffusion models have also demon- strated remarkable results in text-based image editing tasks, in- cluding InstructPix2Pix [Brooks et al. 2023], Imagic [Kawar et al. 2023], and SINE [Zhang et al. 2023]. In addition, it has been success- fully used for various tasks [Fei et al. 2023;",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S19",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "Xia et al. 2023] such as super-resolution [Yue et al. 2023], inpainting [Tang et al. 2023], seg- mentation [Burgert et al. 2023; Xu et al. 2023b]. Our work represents a pioneering effort in applying diffusion models to image matting. Compared with the recent concurrent unpublished diffusion-based methods for image matting [Hu et al . 2023; Xu et al. 2023a], the main difference with our work is the setting. Those methods are trimap-based, while our method facilitates both trimap-free and guidance-based matting. In addition, they are based on the pixel diffusion model, whereas we employ the latent diffusion model (LDM) [Rombach et al. 2022]. The LDM pre-trained on billions of images offers powerful prior. Furthermore, the latent mechanism helps mitigate the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S20",
      "paper_id": "arxiv:2407.21017v1",
      "section": "related_work",
      "text": "impact of potentially imperfect training data, as shown in Figure 1 and Figure 2.",
      "page_hint": null,
      "token_count": 14,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S21",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "We solve the matting problem in a conditional generation manner by training a diffusion model to jointly model the distribution of alpha matte \ud835\udc5d(\ud835\udf36 )and draw an alpha matte \ud835\udf36 from the distribution conditioned on the input image x. Thanks to its generative abil- ity and pre-trained rich image knowledge, our model can find the foreground and generate alpha matte with fine boundary details without guidance (Section 3.2). Our tailored high-resolution infer- ence enables the process of arbitrary-resolution images (Section 3.3). Besides guidance-free matting, we can seamlessly integrate addi- tional guidance to our trained model, such as a trimap, coarse mask, scribbles, and texts, to alleviate ambiguity in matting (Section 3.4). 3.1 Generative Formulation We model the distribution of alpha",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S22",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "matte \ud835\udc5d(\ud835\udf36 )with a pre-trained latent diffusion model [Rombach et al. 2022]. Given an alpha matte \ud835\udf36 \u223c\ud835\udc5d(\ud835\udf36 ), we encode it with the pre-trained encoder Eto get its latent representation z(\ud835\udf36 ) = E(\ud835\udf36 ). We then apply the diffusion process to the latent representation. Let z0 := z(\ud835\udf36 ), the forward process gradually adds a small amount of Gaussian noises to the latent of alpha matte z0 in \ud835\udc47 steps. Therefore, a discrete Markov chain {z0,z1,..., z\ud835\udc47}is constructed such that z\ud835\udc61 = \u221a\ufe01 1 \u2212\ud835\udefd\ud835\udc61z(\ud835\udf36 ) \ud835\udc61\u22121 + \u221a\ufe01 \ud835\udefd\ud835\udc61\ud835\udf50\ud835\udc61\u22121 = \u221a\ud835\udf0e\ud835\udc61z0 +\u221a1 \u2212\ud835\udf0e\ud835\udc61\ud835\udf50 , (2) where the step \ud835\udc61 \u2208{1,...\ud835\udc47 }, \ud835\udf50\ud835\udc61,\ud835\udf50 \u223cN( 0,I)are Gaussian noises and \ud835\udf0e\ud835\udc61 := \u00ce\ud835\udc61 \ud835\udc60=1 \ud835\udefd\ud835\udc60. The variance schedule {\ud835\udefd1,...,\ud835\udefd \ud835\udc47}enables",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S23",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "multiple scales of Gaussian noises added to z0. To model the distribution of z(\ud835\udf36 ), the backward process trains a score-based model \ud835\udf16\ud835\udf03 to predict the noise introduced to the noisy sample z\ud835\udc61 at step \ud835\udc61. The objective of training is to minimize E\ud835\udf16\u223cN(0,1),\ud835\udc61,z0 \u0002 \u2225\ud835\udf16\ud835\udc61 \u2212\ud835\udf16\ud835\udf03(z\ud835\udc61,\ud835\udc61)\u22252 2 \u0003 . (3) Training the model on a set of alpha mattes{\ud835\udf36\ud835\udc56}\ud835\udc41 \ud835\udc56=1 \u223c\ud835\udc5d(\ud835\udf36 )enables modeling their distribution \ud835\udc5d(\ud835\udf36 ). After training, we can perform ancestral sampling [Song et al. 2020] to generate a sample z0 from a normally distributed variable z\ud835\udc47 \u2208 N(0,1). Subsequently, by passing z0 through the decoder D, we obtain a matte \u02c6\ud835\udf36 \u223c\ud835\udc5d(\ud835\udf36 ). 3.2 Conditional Generation with a Single Input Image The matting task aims to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S24",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "produce the alpha matte corresponding to a given input imagex, rather than generating a random alpha matte. SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Wang, Li, Wang, Liu, Gu, Chuang, and Satoh Patch 1 Patch K Collage \u2026 \u2026 Patch 1 Patch K Latent Diffusion U-Net concat Text prompt add Condition E Latent Diffusion U-Net Split blend Guidance a b Uncertainty Candidate c up add Latent Output E downInput E pixel spaceHR Inference (latent space)LR Interference (latent space) \ud835\udc47 \ud835\udc47\u2032 \ud835\udc3f Frozen Pre-trained LR guidance Noise Figure 3: Method. (a) The low-resolution inference path can be used alone if we do not need very high-quality mattes or have a limited computational budget. The input is the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S25",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "low-resolution latent feature z(x\u2193) of the down-sampled image x \u2193and the sampled noise \ud835\udf50\ud835\udc61. If there is spatial guidance \ud835\udc50Spresent, we will combine it with the sampled noise as the noisy sample. If a text prompt \ud835\udc50Tis provided, we will deliver it to the U-Net. The output of this path is the denoised latent feature \u02c6z0. This path requires a few steps \ud835\udc47\u2032\u223c10. (c) We run this step multiple times with different random seeds to get \ud835\udc3fpredictions in the pixel space. With them, we estimate the uncertainty map U, and the set of candidate regions B= {\ud835\udc4f\ud835\udc56}\ud835\udc35 1 . (b) The high-resolution path. We first add the up-sampled latent feature to the sampled noise. Then, we split the high-resolution latent",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S26",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "input and noise into overlapped patches according to B. These patches are respectively fed into the diffusion denoising network. Finally, we merge all denoised patches to get a collage. We perform \u201csplit\u201d and \u201ccollage\u201d during every denoising step \ud835\udc61 \u2208{1,...,\ud835\udc47 }. We will use a specific text prompt: \u201cenhance details\u201d if there is a text prompt used in the LR path. As a result, we condition the generation process on the input image x. Specifically, we concatenate the latent z(x):= E(x)of the input image x with the noisy sample z\ud835\udc61 and then feed the concatenated tensor to the model. To teach the model to generate alpha matte \ud835\udf36 conditioned on the input image x, we train it with paired data",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S27",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "{(x\ud835\udc56,\ud835\udf36\ud835\udc56)}\ud835\udc41 \ud835\udc56=1 \u2208\ud835\udc5ddata by minimizing: L= E\ud835\udf16\u223cN(0,1),\ud835\udc61,z0,z(x) \u0002 \u2225\ud835\udf16\ud835\udc61 \u2212\ud835\udf16\ud835\udf03(z\ud835\udc61,z(x),\ud835\udc61)\u22252 2 \u0003 . (4) We initialize model \ud835\udf16\ud835\udf03 with pre-trained weights from Stable Diffusion (SD) [Rombach et al. 2022]. The weights learned on billion- level natural images [Schuhmann et al . 2022] possess extensive knowledge of image semantics and details. To adapt the denoising score-based model \ud835\udf16\ud835\udf03 for alpha matte generation, we extend its architecture by duplicating its input layers. The weights of these newly added layers are initialized to 0. Following this modification, we proceed to fine-tune the denoising score-based model. Upon completing the training process, we can draw a sample \u02c6\ud835\udf36 from \ud835\udc5d(\ud835\udf36 )conditioned on the input imagex with the ancestral sampling and decoding. 3.3 HR Inference with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S28",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "LR Guidance The current image resolution is typically high, often exceeding 2K. Applying the diffusion model to such high-resolution (HR) inputs requires computational resources that are not readily avail- able. Inference with low-resolution (LR) images is sub-optimal for generating mattes with detailed boundaries. To address this is- sue, we propose an HR inference method leveraging patch-based inference. However, applying patch-based inference, like MultiDif- fusion [Bar-Tal et al. 2023], to high-resolution matting presents two major challenges: the lack of context and redundant computations. To overcome the issue of limited context, we use a predicted low- resolution matte to guide the process. For reducing computational load, we take advantage of the sparsity inherent in alpha mattes. Patch Sampling. The diffusion model produces",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S29",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "stochastic alpha mattes under different random seeds. However, stochastic results generally occur at the boundary regions, where the matte quality is inadequate, while other regions are deterministic and their matte quality is good enough. We pay attention to these boundary re- gions, which represent small portions of the input. Other regions can be directly determined through up-sampling the matte from LR inference. Taking advantage of the sparsity of fractional alpha values in the matte, we can reduce computations while maintain- ing good quality. First, we downsample the HR image and pass it through the diffusion model, yielding a low-resolution matte pre- diction \u02c6\ud835\udf36\ud835\udc56. We perform the low-resolution inference \ud835\udc3ftimes using varying random seeds, resulting in \ud835\udc3f low-resolution predictions Matting by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S30",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "Generation SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA A= {\u02c6\ud835\udf36 1, \u02c6\ud835\udf36 2,..., \u02c6\ud835\udf36\ud835\udc3f}. Their standard deviation is calculated to ap- proximate the uncertainty map U= \u221a\ufe01 E(A\u2212 E(A)). We identify regions on the uncertainty mapUwith high information entropy as candidate patches B= {\ud835\udc4f\ud835\udc56}\ud835\udc35 \ud835\udc56=1 that require refinement. As depicted in Figure 3, high uncertainty is often observed around complex regions, such as hair boundaries. Thus, based on this uncertainty map, we select candidate regions for further processing. Patch-Based Inference. We perform inference on the selected patches B. The noise latent for each patch is not independently sampled. This strategy ensures consistent prediction for differ- ent patches, especially for overlapped areas. We sample a noise z\ud835\udc47",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S31",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "\u2208 N(0,1)with the same size as the input image. Then, we crop patches {z1 \ud835\udc47,z2 \ud835\udc47,..., z\ud835\udc3e \ud835\udc47 }from the sampled noise z\ud835\udc47, where z\ud835\udc58 \ud835\udc47 = \ud835\udc39(z\ud835\udc47|\ud835\udc4f\ud835\udc56)and \ud835\udc39 denotes the cropping operator. The image con- dition used to condition the model is cropped from the input image\u2019s latent similarly. We feed the noise and image latent of the patch to the diffusion model. During the ancestral sampling, each step \ud835\udc61 will produce latent samples {z1 \ud835\udc61,z2 \ud835\udc61,..., z\ud835\udc3e \ud835\udc61 }for patches {\ud835\udc4f1,\ud835\udc4f2,...,\ud835\udc4f \ud835\udc3e}. Before passing them to the next step \ud835\udc61\u22121, we merge them by \u00afz\ud835\udc61 = \ud835\udc3e\u2211\ufe01 \ud835\udc58=1 \ud835\udc39\u22121 (z\ud835\udc58 \ud835\udc61 |\ud835\udc4f\ud835\udc58), (5) where \ud835\udc39\u22121 is the uncropping operator which puts the latent patch z\ud835\udc58 \ud835\udc61 back to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S32",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "the patch location \ud835\udc4f\ud835\udc58 where it was cropped from the input image. Then, we get the latent patches for the next step from \u00afz\ud835\udc61. After the ancestral sampling, we will obtain the denoised latent \u00afz0. It is finally merged with the up-sampled LR matte on latent space to get the final alpha matte. The coarse-to-fine strategy is similar to previous high-resolution matting methods [Lin et al. 2021]; however, the main difference is the proposed guidance mechanism for the diffusion model. Guidance Mechanism. Performing the model on cropped patches often produces flawed results because the model cannot perceive the context information of the whole image and could be misled by local patches. To address this issue, we propose to use the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S33",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "predicted LR matte as guidance. Although the matte predicted from LR input has imperfect boundary details, it has sufficiently accurate predictions for other regions. Thus, instead of starting from pure noise \ud835\udf50 \u2208 N(0,1), we start the backward process from z\ud835\udc47 = \u221a\ufe01 (1 \u2212\ud835\udf0e\ud835\udc47)/\ud835\udf0e\ud835\udc47 \ud835\udf50 +\u02c6z\u2191 0, (6) where \u02c6z\u2191 0 denotes the upsampled latent corresponding to one of the predicted LR mattes {\u02c6\ud835\udf36\ud835\udc59}\ud835\udc3f 1 This strategy is simple but effec- tive. During training, z\ud835\udc47 is a summation of the ground truth alpha matte latent z0 and Gaussian noise \ud835\udf50. z0 contains low-frequency (foreground and background) and high-frequency (boundary) infor- mation while \ud835\udf16\ud835\udc47 is high-frequency. The noise will flood the high- frequency information inz0. In other words,z\ud835\udc47 is approximately",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S34",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "the combination of the low frequency ofz0 and the high frequency of\ud835\udf50. The model learns to extract low-frequency data from noisy samples. During inference, the given \u02c6z\u2191 0 also contains both low-frequency (foreground and background) and high-frequency (boundary) infor- mation. The high frequency, which could be inaccurate, is flooded, and the model can extract the correct low frequency. This strategy can also facilitate the incorporation of users\u2019 guidance in the next section. 3.4 Additional Guidance Matting without any guidance could lead to ambiguity. For example, when there are multiple people in an image, it could be difficult to determine which one to extract. Additional guidance, such as a human-annotated trimap, a coarse mask derived from semantic segmentation, scribbles, clicks, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S35",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "a text prompt, would be helpful in this case. Our method can incorporate additional guidance if present. Text Guidance. Adding text guidance is relatively easy since we use the text-to-image generative diffusion model. We annotate the text description of training images with BLIP2 [Li et al . 2023b]. Each annotation describes the target foreground in the training image. Given the CLIP feature \ud835\udc50Tof the text prompt T, we use the cross-attention mechanism to inject the control into the denoising model. We train the denoising model with the annotated prompt by minimizing L= E\ud835\udf16\u223cN(0,1),\ud835\udc61,z0,z(x),\ud835\udc50T \u0002 \u2225\ud835\udf16\ud835\udc61 \u2212\ud835\udf16\ud835\udf03(z\ud835\udc61,z(x),\ud835\udc50T,\ud835\udc61)\u22252 2 \u0003 . (7) During training, for small patches, we use a specific prompt \u201cen- hance details\u201d instead of avoiding confusing the model. Spatial",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S36",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "Guidance. Spatial guidance like a trimap, coarse mask [Yu et al. 2021], and scribbles are more popular than text prompts for image matting. Inspired by the guidance mechanism described in Section 3.3, we use a similar method for injecting spatial guidanceS. We extract this guidance\u2019s latent representation\ud835\udc50Sand a mask indi- cating unknown regions \ud835\udc5aunknown. For coarse mask, \ud835\udc5aunknown = I and for scribble \ud835\udc5aunknown represents regions without scribbles. At inference, we perform ancestral sampling from z\ud835\udc47 = \u221a\ufe01 (1 \u2212\ud835\udf0e\ud835\udc47)/\ud835\udf0e\ud835\udc47 \ud835\udf50 +(1 \u2212\ud835\udc5aunknown)\ud835\udc50S. (8) We can apply various kinds of guidance3 directly at the inference time without training with them.",
      "page_hint": null,
      "token_count": 100,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S37",
      "paper_id": "arxiv:2407.21017v1",
      "section": "experiments",
      "text": "4.1 Protocol Dataset. We conduct experiments on real-world datasets rather than synthetic ones. We train our model on the training set of P3M-10K [Li et al. 2021], a dataset containing 9,421 high-resolution real-world face-blurred portrait images and human-annotated alpha mattes that are not perfectly accurate. We evaluate the performance on three benchmarks: P3M-P dataset containing 500 face blurred images. Each image has a corresponding trimap and alpha matte. They are validation sets from P3M-10K that share a similar distribu- tion of alpha matte with the training set. PPM-100 [Ke et al. 2022] is a dataset with 100 high-resolution images with corresponding fine annotations. RVP [Yu et al. 2021] consists of 636 portraits with alpha mattes and coarse segmentation masks. 3The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S38",
      "paper_id": "arxiv:2407.21017v1",
      "section": "experiments",
      "text": "domain of the guide image should match that of the alpha matte. SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Wang, Li, Wang, Liu, Gu, Chuang, and Satoh Table 1: Quantitative results of trimap-free portrait matting. We compare our method with trimap-free portrait matting",
      "page_hint": null,
      "token_count": 47,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S39",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "a mask with all pixels labeled unknown. \u2217We removed am- biguous samples from the dataset, 5 out of 100 from PPM and 20 out of 500 from P3M-P, which will be elaborated on in the supplementary. PPM\u2217 P3M-P\u2217 MSE\u2193MAD\u2193SAD\u2193Conn\u2193MSE\u2193MAD\u2193SAD\u2193Conn\u2193 DiffMat\u2020[Xu et al. 2023a]522.1 594.9 5681.3 5623.9 510.2 582.7 999.1 989.0 MODNet [Ke et al. 2022] 4.5 10.1 96.0 81.1 11.3 17.4 29.9 26.6 P3M [Li et al. 2021] 5.8 9.6 93.3 96.1 2.7 5.1 8.8 8.3 ViTAE-S [Ma et al. 2023]3.4 6.5 62.6 59.3 1.8 4.3 7.4 7.2 Ours 2.5 6.3 56.9 54.0 1.6 4.1 7.1 6.8 Table 2: Quantitative results of guidance-based portrait mat- ting. We compare our method with guidance-based portrait matting methods. The guidance is a mask",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S40",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "in the top por- tion of the table, while in the bottom portion, it is a trimap. \u2020P3M-P does not provide the segmentation mask; therefore, we use coarse masks extracted from trimaps \ud835\udc5aas guidance (\ud835\udc5a[\ud835\udc5a >= 0.5]= 1). Although our scores are worse than some",
      "page_hint": null,
      "token_count": 45,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S41",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "better. Note that P3M has the same distribution as the train- ing set; therefore, it could not reflect overfitting the imperfect labels. RVP P3M-P \u2020 MSE\u2193MAD\u2193SAD\u2193Conn\u2193MSE\u2193MAD\u2193SAD\u2193Conn\u2193 MAM [Li et al. 2023a] 20.7 36.3 48.5 44.6 7.9 13.4 23.1 20.5 MG-Mat [Yu et al. 2021] 9.4 20.7 29.2 25.5 5.7 12.8 22.0 18.4 DiffMat [Xu et al. 2023a] 16.6 32.5 44.4 41.2 45.0 49.5 84.4 84.5 Ours - mask 11.6 19.6 26.7 26.1 1.6 4.2 7.2 6.2 IndexNet [Lu et al. 2019b] \u2013 \u2013 \u2013 \u2013 1.2 4.2 7.0 6.0 MatteFormer [Park et al. 2022]\u2013 \u2013 \u2013 \u2013 1.4 4.1 7.1 6.5 DiffMat [Xu et al. 2023a] \u2013 \u2013 \u2013 \u2013 1.0 3.6 6.2 5.2 Ours - trimap \u2013 \u2013 \u2013",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S42",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "\u2013 1.6 4.0 6.9 6.0 Compared methods. We compare our approach against several state-of-the-art matting methods. Guidance-free: MODNet [Ke et al. 2022], P3M [Li et al. 2021], ViTAE-S [Ma et al. 2023]. These",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S43",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "based: IndexNet [Lu et al. 2019b], MatteFormer [Park et al. 2022], and DiffMat [Xu et al. 2023a], which is a concurrent method using diffusion models for trimap-based matting.Mask Guided Matting: MAM [Li et al . 2023a], which incorporates SAM [Kirillov et al . 2023b] as its backbone, and MG-Mat [Yu et al. 2021]. Metrics. Evaluation metrics include the Sum of Absolute Differ- ences (SAD), Mean Squared Error (MSE), Mean Absolute Difference (MAD), and Connectivity (Conn.) [Rhemann et al. 2009]. We apply all metrics on whole images. We scale the MAD and MSE values by a factor of 103. 4We utilize the publicly available checkpoints released by MODNet. It was trained on proprietary datasets. We attempted to align MODNet\u2019s training data",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S44",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "with ours. However, this reproduction resulted in unsatisfactory outcomes. 4.2 Trimap-free Matting Table 1 presents the quantitative results for trimap-free setting. Our method achieves the best scores in all metrics. It consistently deliv- ers good results on three benchmarks, showcasing its robustness and versatility across diverse scenarios. Our approach outperforms established methods like MODNet, P3M, ViTAE-S, and MAM, par- ticularly regarding accuracy and boundary detail handling. This is evident in the lower SAD, MSE, MAD, and improved Connectivity scores compared to the competing methods. These results high- light the effectiveness of our generative modeling approach and the use of pre-trained diffusion models in addressing the complexities of trimap-free matting, especially in challenging cases involving intricate details and varying image qualities.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S45",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "Figure 4 presents the qualitative comparisons. The input shows a complex scene with a person that includes intricate details like hair and shoelaces, which are challenging for matting algorithms. As highlighted by the insets, MODNet and P3M outputs lack fine detail, particularly in the hair and feet regions. In contrast, the results from ViTAE-S, while quantitatively close to our method, visually lack the nuanced details that our approach captures. Our result is more similar to the ground truth, including a clear and precise boundary matte, which faithfully reproduces the fine details of the subject, such as individual hair strands and the shoe\u2019s silhouette. This is evident even in cases where quantitative scores are similar, showcasing the added benefit our approach",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S46",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "brings in creating high- fidelity human boundary mattes. Figure 9 showcases the visual results on the RVP dataset, focusing on a challenging scenario involving complex hair details against a sunset backdrop. The input image presents significant matting difficulty due to the intricate hair strands silhouetted against the varying tones of the sky. MODNet and P3M results display notable artifacts and fail to capture the finer hair details, as evidenced in the zoomed insets. ViTAE-S, although quantitatively competitive, visually lacks fidelity in reproducing the hair\u2019s fine structure, as the comparison with the ground truth reveals a less accurate matte. Our method, on the other hand, shows a remarkable capture of detail, closely mirroring the ground truth. The insets highlight our",
      "page_hint": null,
      "token_count": 119,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S47",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "the subtle nuances in the silhouette, which are critical for a realistic matting outcome. Despite their close quantitative scores, this visual comparison underscores the qualitative edge of our method over ViTAE-S, illustrating our approach\u2019s advanced ability to generate detailed human boundary mattes that are distinct and more aligned with the actual scene. 4.3 Guidance-based Matting When the foreground is ambiguous, it is inherently challenging for trimap-free matting. In contrast, our method can also use guidance such as text prompt and coarse mask to reduce ambiguity (Figure 5). Table 2 shows the quantitative comparisons for the guidance- based setting. Although the trimap-based method\u2014DiffMat [Xu et al. 2023a]\u2014performs better than our method in the trimap-based setting, it relies on the high quality",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S48",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "of the trimaps. They would fail when using coarse guidance, such as masks from semantic seg- mentation. The mask-based method MAM [Li et al. 2023a] refines the mask generated from SAM [Kirillov et al . 2023a]. Benefiting Matting by Generation SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Input MODNet [Ke et al. 2022] P3M [Li et al. 2021] ViTAE-S [Ma et al. 2023] Ours Human annotation Figure 4: Visual results of trimap-free matting on PPM-100 [Ke et al . 2022]. Our method achieves more accurate matting results, especially around thin and detailed structures, compared to prior work. We extracted the foreground using the technique proposed by Germer et al. [Germer et al . 2021] and composited",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S49",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "it onto a new background sampled from a public background database [Lin et al. 2021]. \u201cforeground person. . . \u201d Input w/o guidance Guidance w/ guidance Figure 5: Use of guidance. With various guidance, we can reduce ambiguity. from SAM, it can predict alpha mattes for \u201cany\u201d foreground objects. However, its matting performance is sub-optimal. Figure 11 shows the qualitative comparison under the trimap- based setting. The scores of DiffMat [Xu et al. 2023a] are better, but we notice their visual results are worse than ours. At the same time, the imperfection of the ground-truth mattes is also observed. We suppose the worse results of DiffMat compared to us are because Table 3: Ablation study. We implement four variants of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S50",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "our",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S51",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "model without using the pre-trained SD weights; (2) Training with the same prompt for all cropped patches from an image; (3) Our model trained with resized full image rather than patches with different scales; (4) Adding pixel losses to our training phase. MSE MAD Ours 2.5 6.3 - (1) w/o denoising prior 63.6 71.1 - (2) w/o specific patch prompt 38.6 46.8 - (3) w/o multi-scale data 8.6 15.1 + (4) pixel loss 58.0 65.1 they are based on the pixel diffusion model, which overfits the training labels. 5 ANALYSES AND DISCUSSIONS Ablation studies. Table 3 demonstrates the importance of the pre- trained generative prior, the prompting and the multi-scale training strategy (will be elaborated on in the supplementary), and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S52",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "operating SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Wang, Li, Wang, Liu, Gu, Chuang, and Satoh 0 20 40 60 80 100steps 200 400 600SAD Figure 6: Randomness test. We use 5 different random seeds to test the model on selected images. With the increase of diffusion steps, the mean and std. of SAD error decrease. Input LR w/o guidance w/ guidance Figure 7: HR inference with LR guidance. in latent space. Stable Diffusion (SD) with vast pre-trained knowl- edge significantly induces semantic information. Table 3 indicates, without this, training the diffusion model is prone to lose semantic information, e.g. resulting in an incomplete person. Besides se- mantic information, this generative prior enables us to hallucinate",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S53",
      "paper_id": "arxiv:2407.21017v1",
      "section": "method",
      "text": "details, e.g., hair boundary. Without it, we could converge to the existing methods. Besides, operating within the latent space adeptly preserves essential details, crucial for producing high-quality mat- tes with emphasis on boundary regions. This is an advantage of our method over concurrent works built on pixel-based diffusion models. Figure 7 shows the effectiveness of HR inference with LR guidance. Effect of randomness. Figure 6 depicts that with the increases of steps, the randomness decreases. Besides, we notice that infer with larger patches will also reduce the randomness. Soft matte. Our model can produce soft mattes for out-of-focus blur, as shown in Figure 10, even though the training dataset does not contain annotations for such blur.",
      "page_hint": null,
      "token_count": 116,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S54",
      "paper_id": "arxiv:2407.21017v1",
      "section": "limitations",
      "text": "HR images compared to naive approaches, it is important to clarify that the inherent limitations of the diffusion model make it less ef- ficient than prior regression-based methods. Processing a 512\u00d7512 images with 50 steps requires about 5s on a NVIDIA V100 GPU card. However, it is worth noting that some ongoing research efforts are focused on enhancing sampling efficiency, which could help miti- gate this limitation [Li et al. 2023c; Song et al. 2023]. Secondly, our model trained on portrait datasets shows potential for adaptation to other domains, such as animal matting (see Figure 12), but is unsuitable for matting scenarios with markedly different charac- teristics, such as subjects like fire. Thirdly, our method is designed for image matting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S55",
      "paper_id": "arxiv:2407.21017v1",
      "section": "limitations",
      "text": "and cannot guarantee temporal consistency for videos (see Figure 8). Enhancing temporal coherence remains a subject for future research. Figure 8: Video inference. By individually processing down- sampled frames, our method produces temporal inconsis- tency in videos. While employing high-resolution frames mitigates this issue, it still suffers from problems similar to regression-based methods.",
      "page_hint": null,
      "token_count": 53,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S56",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Our approach presents a straightforward yet highly efficient tech- nique for matting. It can perform both trimap-free and guidance- based image matting tasks. By reframing the problem as a gen- erative task and leveraging diffusion models enriched with pre- trained knowledge for regularization, we have devised innovative designs that empower our model to produce high-resolution and high-quality results. Our experimental results on three benchmark datasets not only demonstrate the efficacy of our method in quanti- tative terms but also showcase its exceptional visual performance, making it a promising solution for the field of matting. ACKNOWLEDGMENTS We thank anonymous reviewers for their constructive feedback. We also thank Yutong Dai and Zhanghan Ke for their help. This research was supported by NTU112L9009,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S57",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "NTU113L894003, NSTC112-2634- F-002-006, MOST110-2221-E-002-124-MY3, JSPS KAKENHI Grant Number JP23K24876, JST ASPIRE Program Grant Number JPM- JAP2303, and the Value Exchange Engineering, a joint research project between Mercari, Inc. and RIISE. REFERENCES Yagiz Aksoy, Tun\u00e7 Ozan Aydin, and Marc Pollefeys. 2017. Designing effective inter- pixel information flow for natural image matting. In CVPR. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. In ICML. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. InstructPix2Pix: Learning to Follow Image Editing Instructions. In CVPR. Ryan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. 2023. Peekaboo: Text to image diffusion models are zero-shot segmentors. In CVPRW. Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S58",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Xinxin Yang, and Kun Gai. 2018. Semantic human matting. In ACM MM. Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. 2013. KNN matting. IEEE TPAMI 35, 9 (2013), 2175\u20132188. Donghyeon Cho, Yu-Wing Tai, and In So Kweon. 2019. Deep Convolutional Neural Network for Natural Image Matting Using Initial Alpha Mattes. IEEE TIP 28, 3 (2019), 1054\u20131067. Yung-Yu Chuang, Brian Curless, David H. Salesin, and Richard Szeliski. 2001. A Bayesian Approach to Digital Matting. In CVPR. Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. 2023. Generative Diffusion Prior for Unified Image Restoration and Enhancement. In CVPR. Xiaoxue Feng, Xiaohui Liang, and Zili Zhang. 2016. A Cluster Sampling Method for Image Matting via Sparse",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S59",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Coding. In ECCV. Eduardo S. L. Gastal and Manuel M. Oliveira. 2010. Shared Sampling for Real-Time Alpha Matting. In Eurographics. Matting by Generation SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Thomas Germer, Tobias Uelwer, Stefan Conrad, and Stefan Harmeling. 2021. Fast multi-level foreground estimation. In ICPR. Leo Grady, Thomas Schiwietz, Shmuel Aharon, and R\u00fcdiger Westermann. 2005. Ran- dom walks for interactive alpha-matting. In Proceedings of the IASTED International Conference on Visualization, Imaging and Image Processing . Kaiming He, Christoph Rhemann, Carsten Rother, Xiaoou Tang, and Jian Sun. 2011. A global sampling method for alpha matting. In CVPR. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In NeurIPS. Yihan Hu, Yiheng Lin, Wei",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S60",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Wang, Yao Zhao, Yunchao Wei, and Humphrey Shi. 2023. Diffusion for Natural Image Matting. arXiv preprint arXiv:2312.05915 (2023). Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-Based Real Image Editing with Diffusion Models. In CVPR. Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Rynson WH Lau. 2022. MODNet: Real-time trimap-free portrait matting via objective decomposition. In AAAI. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Pi- otr Dollar, and Ross Girshick. 2023a. Segment Anything. In ICCV. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S61",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Wan-Yen Lo, Pi- otr Doll\u00e1r, and Ross Girshick. 2023b. Segment Anything. In ICCV. Anat Levin, Dani Lischinski, and Yair Weiss. 2008. A Closed-Form Solution to Natural Image Matting. IEEE TPAMI 30, 2 (2008), 228\u2013242. Jiachen Li, Jitesh Jain, and Humphrey Shi. 2023a. Matting Anything. arXiv: 2306.05399 (2023). Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML. Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. 2021. Privacy-Preserving Portrait Matting. In ACM MM. Jizhizi Li, Jing Zhang, Stephen J. Maybank, and Dacheng Tao. 2022b. Bridging composite and real: towards end-to-end deep image matting. IJCV 130, 2 (2022), 246\u2013266. Jizhizi Li, Jing Zhang, and Dacheng Tao. 2023d.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S62",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Deep Image Matting: A Comprehensive Survey. arXiv preprint arXiv:2304.04672 (2023). Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. 2022a. GANimator: Neural Motion Synthesis from a Single Sequence. ACM TOG 41, 4 (2022), 138. Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. 2023c. SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds. In NeurIPS. Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian L Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2021. Real-time high-resolution background matting. In CVPR. Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, and Xian sheng Hua. 2020. Boosting semantic human matting with coarse annotations. In CVPR. Yuhao Liu, Jiake Xie,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S63",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. 2021. Tripartite information mining and integration for image matting. In ICCV. Hao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu. 2019a. Context-Aware Image Matting for Simultaneous Foreground and Alpha Estimation. In ICCV. Hao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu. 2019b. Indices matter: Learning to index for deep image matting. In ICCV. Sihan Ma, Jizhizi Li, Jing Zhang, He Zhang, and Dacheng Tao. 2023. Rethinking Portrait Matting with Pirvacy Preserving. IJCV 131, 8 (2023), 2172\u20132197. GyuTae Park, SungJoon Son, JaeYoung Yoo, SeHo Kim, and Nojun Kwak. 2022. Matte- former: Transformer-based image matting via prior-tokens. In CVPR. Thomas Porter and Tom Duff. 1984. Compositing Digital Images. In SIGGRAPH. Yu",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S64",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. 2020. Attention-guided hierarchical structure aggregation for image matting.. In CVPR. Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. 2009. A perceptually motivated online benchmark for image matting. In CVPR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In CVPR. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight- man, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS (2022). Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2020.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S65",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Background matting: The world is your green screen. In CVPR. Ehsan Shahrian, Deepu Rajan, Brian Price, and Scott Cohen. 2013. Improving image matting using comprehensive sampling sets. In CVPR. Dmitriy Smirnov, Chloe LeGendre, Xueming Yu, and Paul Debevec. 2023. Magenta Green Screen: Spectrally Multiplexed Alpha Matting with Deep Colorization. In Proceedings of the Digital Production Symposium . Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In ICML. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. 2023. Consistency models. In ICML. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. Score-Based Generative Modeling through Stochastic Differential Equations. In ICML. Jian Sun, Jiaya Jia, Chi-Keung Tang, and Heung-Yeung Shum. 2004. Poisson",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S66",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "matting. ACM TOG 23, 3 (2004), 315\u2013321. Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. 2021. Semantic image matting. In CVPR. Luming Tang, Nataniel Ruiz, Chu Qinghao, Yuanzhen Li, Aleksander Holynski, David E Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, and Michael Rubinstein. 2023. RealFill: Reference-Driven Generation for Authentic Image Com- pletion. arXiv preprint arXiv:2309.16668 (2023). Jue Wang and Michael F. Cohen. 2007. Optimized Color Sampling for Robust Matting. In CVPR. Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao, Weiming Zhang, and Nenghai Yu. 2021. Improved Image Matting via Real-time User Clicks and Uncertainty Estimation. In CVPR. Jiawei Wu, Changqing Zhang, Zuoyong Li, Huazhu Fu, Xi Peng, and Joey Tianyi Zhou. 2023. dugMatting: decomposed-uncertainty-guided matting. In ICML.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S67",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wen- ming Yang, and Luc Van Gool. 2023. DiffIR: Efficient Diffusion Model for Image Restoration. In ICCV. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. 2023b. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In CVPR. Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. 2017. Designing effective inter-pixel information flow for natural image matting. In CVPR. Yangyang Xu, Shengfeng He, Wenqi Shao, Kwan-Yee K Wong, Yu Qiao, and Ping Luo. 2023a. DiffusionMat: Alpha Matting as Sequential Refinement Learning. arXiv preprint arXiv:2311.13535 (2023). Xin Yang, Ke Xu, Shaozhe Chen, Shengfeng He, Baocai Yin Yin, and Rynson Lau. 2018. Active Matting. In NeurIPS. Qihang",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S68",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. 2021. Mask guided matting via progressive refinement network. In CVPR. Zongsheng Yue, Jianyi Wang, and Chen Change Loy. 2023. ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting. In NeurIPS. Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. 2023. SINE: SINgle Image Editing with Text-to-Image Diffusion Models. In CVPR. SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA Wang, Li, Wang, Liu, Gu, Chuang, and Satoh Input MODNet [Ke et al. 2022] P3M [Li et al. 2021] ViTAE-S [Ma et al. 2023] Ours Human annotation Input MODNet [Ke et al. 2022] P3M [Li et al. 2021] ViTAE-S [Ma",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S69",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "et al. 2023] Ours Human annotation Figure 9: Visual results of guidance-free matting on RVP [Yu et al. 2021] dataset. Input ViTAE-S [Ma et al. 2023] Ours Human annotation Figure 10: Matting with out-of-focus blur. Compared to the hard label in the out-of-focus regions of the human annotations, we generate soft mattes. Matting by Generation SIGGRAPH Conference Papers \u201924, July 27-August 1, 2024, Denver, CO, USA DiffMat [Xu et al. 2023a] Ours Human annotation MSE: 1.0 / SAD: 5.2 MSE: 1.5 / SAD: 5.4 \u2013 / \u2013 Figure 11: Results of trimap-based matting. Our visual results look better, but our evaluation score is worse than DiffMat, mainly because of the imperfect human annotation. Input MAM [Li et al. 2023a] ViTAE-S",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2407_21017v1:S70",
      "paper_id": "arxiv:2407.21017v1",
      "section": "conclusion",
      "text": "[Ma et al. 2023] Ours Human annotation Figure 12: Matting beyond portraits. Based on SAM, MAM can generate a semantically correct alpha matte for the giraffe image but sacrifice some detail. ViTAE-S, on the other hand, fails to produce a semantically correct result and loses details. Our result closely matches the human annotation.",
      "page_hint": null,
      "token_count": 53,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9564807973631867,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 11,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 2757,
        "empty": false
      },
      {
        "page": 2,
        "chars": 6120,
        "empty": false
      },
      {
        "page": 3,
        "chars": 6511,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4411,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5996,
        "empty": false
      },
      {
        "page": 6,
        "chars": 6435,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2026,
        "empty": false
      },
      {
        "page": 8,
        "chars": 5319,
        "empty": false
      },
      {
        "page": 9,
        "chars": 7797,
        "empty": false
      },
      {
        "page": 10,
        "chars": 595,
        "empty": false
      },
      {
        "page": 11,
        "chars": 731,
        "empty": false
      }
    ],
    "quality_score": 0.9565,
    "quality_band": "good"
  }
}