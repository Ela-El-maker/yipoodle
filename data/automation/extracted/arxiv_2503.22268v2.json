{
  "paper": {
    "paper_id": "arxiv:2503.22268v2",
    "title": "Segment Any Motion in Videos",
    "authors": [
      "Nan Huang",
      "Wenzhao Zheng",
      "Chenfeng Xu",
      "Kurt Keutzer",
      "Shanghang Zhang",
      "Angjoo Kanazawa",
      "Qianqian Wang"
    ],
    "year": 2025,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numerous downstream applications. Humans can effortlessly segment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that combines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize motion while integrating semantic support. Extensive testing on diverse datasets demonstrates state-of-the-art performance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/.",
    "pdf_path": "data/automation/papers/arxiv_2503.22268v2.pdf",
    "url": "https://arxiv.org/pdf/2503.22268v2",
    "doi": null,
    "arxiv_id": "2503.22268v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.788182+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2503_22268v2:S1",
      "paper_id": "arxiv:2503.22268v2",
      "section": "body",
      "text": "Segment Any Motion in Videos Nan Huang1,2 Wenzhao Zheng1 Chenfeng Xu1 Kurt Keutzer1 Shanghang Zhang2 Angjoo Kanazawa1 Qianqian Wang1 1UC Berkeley 2Peking University",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S2",
      "paper_id": "arxiv:2503.22268v2",
      "section": "abstract",
      "text": "Moving object segmentation is a crucial task for achieving a high-level understanding of visual scenes and has numer- ous downstream applications. Humans can effortlessly seg- ment moving objects in videos. Previous work has largely relied on optical flow to provide motion cues; however, this approach often results in imperfect predictions due to challenges such as partial motion, complex deformations, motion blur and background distractions. We propose a novel approach for moving object segmentation that com- bines long-range trajectory motion cues with DINO-based semantic features and leverages SAM2 for pixel-level mask densification through an iterative prompting strategy. Our model employs Spatio-Temporal Trajectory Attention and Motion-Semantic Decoupled Embedding to prioritize mo- tion while integrating semantic support. Extensive testing on diverse datasets demonstrates",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S3",
      "paper_id": "arxiv:2503.22268v2",
      "section": "abstract",
      "text": "state-of-the-art perfor- mance, excelling in challenging scenarios and fine-grained segmentation of multiple objects. Our code is available at https://motion-seg.github.io/. 1. Introduction Segmenting moving objects in videos is crucial for a range of applications, including action recognition, autonomous driving [10, 22, 66], and 4D reconstruction [58]. Many prior works address this problem under terms such as Video Object Segmentation (VOS) or motion segmentation. In this paper, we define our task as moving object segmen- tation (MOS) \u2013 segmenting objects that exhibit observable motion within the video. This definition differs from Video Object Segmentation, which includes objects that have the potential to move even if they remain static in the video, and from motion segmentation, which may also capture back- ground motion,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S4",
      "paper_id": "arxiv:2503.22268v2",
      "section": "abstract",
      "text": "such as flowing water. This task is challeng- ing as it implicitly requires distinguishing between camera motion and object motion, robustly tracking objects despite deformations, occlusions, rapid or transient movement, and segmenting them out with precise, clean masks. Recently, promptable visual segmentation has made sig- Images Inputs Filtered Dynamic Tracks Dynamic Masks Long-range Tracks Inputs Figure 1. Our method is capable of handling challenging scenar- ios, including articulated structures, shadow reflections, dynamic",
      "page_hint": null,
      "token_count": 72,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S5",
      "paper_id": "arxiv:2503.22268v2",
      "section": "background",
      "text": "ing per object level fine-grained moving object masks. nificant progress. Taking points, masks, or bounding boxes as prompts, SAM2 [51] segments and tracks the associated objects in videos effectively. However, SAM2 cannot na- tively handle MOS, as it has no mechanism to detect which objects are moving. We propose an innovative combination of long-range tracks with SAM2 for moving object segmentation to ex- ploit the capabilities of SAM2. First, point tracking cap- tures valuable long-range pixel motion information which is robust to deformation and occlusion, as shown in Fig. 2. At the same time, we incorporate DINO feature [12, 45], to add semantic context as a complementary source of information to support motion-based segmentation. We depart from tra- ditional MOS",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S6",
      "paper_id": "arxiv:2503.22268v2",
      "section": "background",
      "text": "approaches by training a model on extensive datasets that effectively combines motion and semantic in- formation at a high level. Given a set of long-range 2D tracks, our model is designed to identify those tracks that correspond to moving objects. Once these dynamic tracks are identified, we apply a sparse-to-dense mask densifica- tion strategy, which uses an Iterative Prompting method in conjunction with SAM2 [51] to transform the sparse, point-level mask into a pixel-level segmentation. Since the primary objective is moving object segmentation, we em- arXiv:2503.22268v2 [cs.CV] 14 Apr 2025 phasize motion cues while using semantic information as secondary support. To effectively balance these two types of information, we propose two specialized modules. (1) Spatio-Temporal Trajectory Attention. Given the long-term",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S7",
      "paper_id": "arxiv:2503.22268v2",
      "section": "background",
      "text": "nature of input tracks, our model incorporates spatial atten- tion to capture relationships between different trajectories and temporal attention to monitor changes within individ- ual trajectories over time. (2) Motion-Semantic Decoupled Embedding. We implement special attention mechanisms to prioritize motion patterns and process semantic features in supplementary pathways. We trained our model on extensive datasets, including both synthetic [19, 28] and real-world data [36]. Due to the self-supervised nature of DINO features [45], our model demonstrates strong generalization capabilities, even when primarily trained on synthetic data. We evaluated our ap- proach on benchmarks [34, 43, 47, 48] that were not part of the training data, and the results show that our method significantly outperforms baseline models in diverse tasks. While",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S8",
      "paper_id": "arxiv:2503.22268v2",
      "section": "background",
      "text": "previous MOS methods leverage optical flow [6, 8, 9] to capture motion information, either by identifying different motion groups [6, 46, 53, 59] or by using learning- based models [8, 9, 18, 40, 49] to derive pixel masks from optical flow. However, optical flow is limited to short-range motion and can lose track over extended durations. Other methods [3, 7, 14, 42] rely on point trajectories as motion cues, but traditionally utilize spectral clustering on affin- ity matrices which struggle with complex motions. Though some methods also attempt to take advantage of appearance cues [24, 61] to help understand motion better, they typi- cally handle different modalities in diverse separate stages, limiting the effective integration of their complementary in- formation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S9",
      "paper_id": "arxiv:2503.22268v2",
      "section": "background",
      "text": "Addressing these limitations, our unified frame- work achieves threefold integration: long-range trajectory, DINO feature, and SAM2. This design explains the model\u2019s exceptional capability in handling challenging cases like articulated motion and reflective surfaces as shown in the Fig. 1, and the superior performance in fine-grained seg- mentation of multiple objects. In summary, we make the following contributions: \u2022 We introduce an innovative combination of long-range tracks with SAM2, which enables efficient mask densi- fication and tracking across frames. \u2022 To obtain motion labels for trajectories, we propose a",
      "page_hint": null,
      "token_count": 88,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S10",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "pled Embedding, which enables a more effective inte- gration of motion and semantic information, enhancing track-level segmentation by balancing these cues. \u2022 Extensive results on multiple benchmarks demonstrate the effectiveness of our method, particularly in fine- grained moving object segmentation. Timet=60 t=85 t=125 ABR Ours RGB RCF -All Figure 2. The effectiveness of long-range tracks . Over longer periods of time, if a moving object experiences factors such as oc- clusion or changes in lighting, it can negatively affect the tracking performance of optical-flow-based methods for that object. 2. Related Work Flow-based Moving Object Segmentation. Tradition- ally, optical flow based methods [6, 46, 53, 59] segment moving objects by grouping motion cues to create a mov- ing object mask. These",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S11",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "methods typically employ itera- tive optimization or statistical inference techniques to es- timate motion models and identify motion regions simul- taneously. Recently, numerous deep learning-based ap- proaches [8, 9, 18, 37, 40, 49, 62] have used CNN encoders or transformer to extract motion cues from optical flow, fol- lowed by decoders to produce the final segmentation. The main distinctions among these methods lie in model archi- tecture; for instance, methods that encode semantic infor- mation often utilize multiple CNN encoders to process dif- ferent data modalities separately. In general, optical-flow- based methods struggle to distinguish independent object motion from apparent motion caused by depth differences. Furthermore, strong brightness changes also adversely af- fect these methods. Additionally, optical-flow-based meth- ods are",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S12",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "limited to short temporal sequences; they perform poorly if objects move slowly or are occluded. Trajectory-based Moving Object Segmentation. Trajectory-based methods can be typically classified into two categories: two-frame and multi-frame methods. Two-frame methods [3, 14, 25] generally estimate motion parameters by solving an iterative energy minimiza- tion problem, which are recently powered with various convolutional neural network (CNN) models [54, 75]. Multi-frame methods, in contrast, often utilize spectral clustering based on affinity matrices. These matrices are derived through techniques such as geometric model fitting [2, 23, 32, 64], subspace fitting [17, 50, 55, 57], or pairwise motion affinities that integrate motion and appearance information [7, 24, 30, 42]. Recent work has 2D Tracks and Depths Spatial-Temporal Trajectory Attention Featured",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S13",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Tracks Motion-Semantic Decoupled Embedding Position Embedding Dynamic Tracks Selection Motion Encoder DINO Tracks Decoder (a) Tracks Label Prediction Grouped Dynamic Tracks \u2026 time Iterative Grouping SAM2 Predicted Fine-grained Masks SAM2 (b) Iterative Prompting Figure 3. Overview of Our Pipeline. We take 2D tracks and depth maps generated by off-the-shelf models [15, 67] as input, which are then processed by a motion encoder to capture motion patterns, producing featured tracks. Next, we use tracks decoder that integrates DINO feature [45] to decode the featured tracks by decoupling motion and semantic information and ultimately obtain the dynamic trajectories(a). Finally, using SAM2 [51], we group dynamic tracks belonging to the same object and generate fine-grained moving object masks(b). focused on the search for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S14",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "more effective motion models. For instance, [1] uses the trifocal tensor to analyze point trajectories, arguing that it provides more reliable matches over three images than fundamental matrices can over two. However, the trifocal tensor also poses challenges: it is difficult to optimize and prone to failure when the three camera positions are nearly collinear[44]. Other studies [27, 65] have proposed geometric model fusion techniques to combine different models. Some recent work has explored integrating multiple motion cues [21, 41]. For example, [24] investigates combining point trajectories and optical flow, using well-crafted geometric motion models to fuse the two affinity matrices through co-regularized multi-view spectral clustering. However, these approaches still face inherent issues due to their reliance on affinity matrices.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S15",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "They tend to capture only local similarities, leading to poor global consistency, resulting in inconsistent segmentation. Furthermore, affinity matrices is difficult to capture dynamic changes in motion features like speed and direction over time. In contrast, we address the challenge of capturing motion similarities across diverse motion types. Unsupervised Video Object Segmentation. Unsuper- vised Video Object Segmentation (VOS) aims to auto- matically identify and track salient objects in raw video footage, while semi-supervised VOS relies on first-frame ground truth annotations to segment objects in subsequent frames [47, 48]. In this work, we focus on Unsupervised VOS, referred to here simply as \u201dVOS\u201d. Recently, many ap- proaches [69, 72] have combined motion and appearance in- formation. For instance, MATNet [74] introduces",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S16",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "a motion- attentive transition model for unsupervised VOS, leverag- ing motion cues to guide segmentation with a primary fo- cus on appearance. RTNet [52] presents a method based on reciprocal transformations, using the consistency of ob- ject appearance and motion between consecutive frames to achieve segmentation. FSNet [26] employs a full-duplex strategy with a dual-path network to jointly model both ap- pearance and motion. Overall, VOS generally targets salient objects in videos, regardless of whether the object is mov- ing. Although many VOS methods incorporate motion in- formation, it is often not their primary focus. 3. Method Our objective is, given a video, to identify moving objects and generate pixel-level dynamic masks. Fig. 3 provides an overview of our pipeline.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S17",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "The central insight is that long- range tracks not only capture motion patterns that facilitate video understanding but also offer long-range prompts es- sential for promptable visual segmentation. Thus, we use long-range point tracks as motion cues, serving as the pri- mary input in Sec. 3.1, where we apply spatial-temporal at- tention to capture context-aware feature. In Sec. 3.2, we further incorporate and decouple the use of semantic in- formation with motion cues to decode features, helping the model predict the final motion labels. After identifying dy- namic tracks, we leverage these long-range tracks to prompt SAM2 [51] iteratively, as described in Sec. 3.3. 3.1. Motion Pattern Encoding Point trajectories carry valuable information for under- standing motion, and related MOS",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S18",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "methods can be typi- cally classified into two categories: two-frame and multi- frame methods. However, as discussed in Sec. 2, two-frame",
      "page_hint": null,
      "token_count": 21,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S19",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "inconsistencies and exhibit degraded performance when in- put flows are noisy. Multi-frame methods, in contrast, often utilize spectral clustering based on affinity matrices. Nev- ertheless, they remain highly sensitive to noise and struggle to handle global, dynamic, and complex motion patterns ef- fectively. To address these limitations, and inspired by Parti- cleSFM [73], we propose a method that leverages long- range point tracks [15], processed through a specialized trajectory processing model, to predict per-trajectory mo- tion labels. As illustrated in Fig. 3, our proposed net- work adopts an encoder-decoder architecture. The encoder directly processes long-range trajectory data and applies a Spatio-Temporal Trajectory Attention mechanism across trajectories. This mechanism integrates both spatial and temporal cues, capturing both local and global information",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S20",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "across time and space, in order to embed the motion pattern of each trajectory. Given that the accuracy and quality of long-range tra- jectories significantly impact model performance, we uti- lize BootsTAP [15] to generate the tracks, which provides a confidence score for each track at each time step, enabling us to mask out low-confidence points. Furthermore, due to the movement of dynamic objects and camera motion, the visibility of long-range tracks can vary over time, as they may be occluded or move out of the frame. This variabil- ity in visibility and confidence makes each trajectory data highly irregular, motivating our use of a transformer model, inspired by sequence modeling approaches in natural lan- guage processing [56, 73], to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S21",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "handle the data effectively. Our input data comprises long-range trajectories, with each trajectory consisting of normalized pixel coordinates (ui, vi), visibility \u03c1i and confidence scores ci, where i \u2208 (0, time). Masks Mi is applied to indicate points where the pixel coordinates are either invisible or low-confidence. Ad- ditionally, we integrate monocular depth maps di estimated by Depth-Anything [67], which, despite some noise, pro- vide valuable insights into the underlying 3D scene struc- ture, enhancing understanding of spatial layout and occlu- sions. To further enrich the input data and strengthen tem- poral motion cues, we compute frame-to-frame differences in both trajectory coordinates(\u2206ui, \u2206vi) and depth \u2206di for adjacent frames. Since adjacent sampling points in coordinates can lead to oversmoothing of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S22",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "spatially close features, we draw inspira- tion from NeRF [39] to address this issue. Specifically, we apply frequency transformations for positional encoding to better capture fine-grained spatial details. The final augmented trajectories pass through two MLPs to generate intermediate features, which are then fed into the transformer encoder. Given the long-range nature of the input data, we propose a Spatio-Temporal Trajectory Atten- tion for our encoderE, interleaves attention layers that oper- ate alternately across track and temporal dimensions [4, 29]. This design allows the model to capture both the tempo- ral dynamics within each trajectory and the spatial relation- ships across different trajectories. Finally, to obtain a fea- ture representation for each entire trajectory rather than in- dividual points, we",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S23",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "perform max-pooling along the tempo- ral dimension, following [73]. This process yields a sin- gle feature vector for each trajectory, naturally forming a high-dimensional featured track that implicitly captures the unique motion pattern of each trajectory. 3.2. Per-trajectory Motion Prediction Though we encoded motion pattern in Sec. 3.1, it is still challenging to distinguish moving objects based solely on motion cues, because learning to differentiate between ob- ject motion and camera motion from highly abstracted tra- jectories is difficult for the model. Providing the model with texture, appearance, and semantic information can simplify this task by helping it understand which objects are likely to move or be moved. Some approaches directly apply se- mantic segmentation models [5, 20, 68, 71]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S24",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "where poten- tially moving pixels are identified based on semantic labels. While these methods can be effective in specific scenarios, they are intrinsically limited for general moving object seg- mentation, as they depend entirely on predefined semantic classes. Recently, many MOS [61, 70] and VOS [11, 33, 35] methods combine appearance information and motion cues, but they do so in two separate stages, often using RGB im- ages to refine masks. However, relying on raw RGB data may fail to capture high-level information, and applying the two modalities in separate stages limits the effective inte- gration of their complementary information. To address these limitations, we incorporate DINO fea- tures predicted by DINO v2 [45], a self-supervised model, which helps generalize",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S25",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "the inclusion of appearance informa- tion. However, we observed that simply introducing DINO features as input makes the model overly reliant on seman- tics as shown in Fig. 8 and discussed in Sec. 4.5, reducing its ability to differentiate between moving and static objects within the same semantic category. To overcome this issue, we propose a Motion-Semantic Decoupled Embedding, en- abling the transformer decoder D to prioritize motion infor- mation while still considering semantic cues. We obtain the final embedded featured tracks P through the process described in Sec. 3.1: P = E((\u03b3(u), \u03b3(v), \u03b3(\u2206u), \u03b3(\u2206v), d,\u2206d, \u03c1, c), M). (1) We then design a transformer-based decoder, where the en- coder layer performs attention only on the embedded fea- tured",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S26",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "tracks, which contain motion information exclusively. After computing the attention-weighted feature, we con- catenate the DINO feature and pass this concatenated fea- ture through a feed-forward layer. In the decoder layer, self- attention is still applied only to the motion features; how- ever, multi-head attention is used to attend to a memory that includes semantic information. Finally, we apply a sigmoid activation function to produce the final output, yielding the predicted label for each trajectory. We then compute the loss between these predicted labels and per-track ground truth labels using a weighted binary cross-entropy loss [73]. We assign ground truth labels to each trajectory by checking if the sampled point coordinates lie within the ground truth dynamic masks. If a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S27",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "point falls inside the mask, it is labeled as dynamic. 3.3. SAM2 Iterative Prompting As depicted in Fig. 3, after obtaining the predicted label of each trajectory and filter dynamic trajectories, we use these trajectories as point prompts for SAM2[51] with an itera- tive, two-stage prompting strategy. The first stage focuses on grouping trajectories belonging to the same object and storing the trajectories of each distinct object in memory. In the second stage, this memory is used as a prompt for SAM2 [51] to generate dynamic masks. The motivation behind this approach is twofold. First, it is necessary because SAM2 requires object IDs as input. However, if we assign the same object ID to all dynamic objects (e.g., assigning 1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S28",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "to represent all dynamic objects), SAM2 would struggle to simultaneously segment multiple objects that share the same ID. Second, this method offers the benefit of achieving finer-grained segmentation. In the first stage, we select the time frame with the max- imum number of visible points and locate the densest point among all visible points in that frame. This point serves as the initial prompt for SAM2 [51], which then generates an initial mask for that frame. After generating this mask, we apply dilation to expand its boundaries, excluding all points within the expanded mask area to remove edge points and assume that these points belong to the same object. We then proceed to the next frame with the highest number",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S29",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "of visi- ble points and repeat this process until the remaining visible points across all frames are too few to process. The trajecto- ries identified as belonging to the same object are stored in memory, with unique object IDs assigned to each. We only save the points within the undilated mask for each object. In the second stage, we use this memory to refine prompt selection by locating the densest point within the stored trajectories and the two points furthest from this point. Leveraging the long-range nature of trajectories, we prompt SAM2 at regular intervals to prevent it from losing track of the object over extended distances. Since SAM2 may gen- erate partial object masks (e.g., parts of a person\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S30",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "cloth- ing), we perform post-processing on all masks to merge those that overlap internally or appear within the same mask boundaries. This results in a complete mask for each dis- tinct object. 4. Experiments 4.1. Implementation Details Training Dataset. We train our model using three datasets: Kubric[19], Dynamic Replica[28], and HOI4D [36], sam- pling them at a ratio of 35%, 35%, and 30% respectively. Kubric [19] is a synthetic dataset composed of sequences of 24 frames showing 3D rigid objects falling under grav- ity and bouncing. We generate dynamic masks for each sequence based on the motion labels of individual objects. Dynamic Replica [28] is another synthetic dataset, created for 3D reconstruction, that includes long-term tracking an- notations and object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S31",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "masks, featuring articulated models of humans and animals. We calculate dynamic masks by an- alyzing the 3D tracks to determine whether each object is in motion, providing accurate motion segmentation for this dataset. HOI4D [36] is a real-world, egocentric dataset that contains common objects involved in human-object inter- actions. This dataset provides official motion segmentation masks, making it ideal for real-world training of our model. Data Sampling. During training, we randomly sample a variable number of tracking points, enhancing the model\u2019s robustness to different track counts. For the Dynamic Replica dataset [28], which contains 300 frames, we speed up training by sampling 1/4 of the frames at regular inter- vals randomly. This approach preserves the large camera motion characteristics of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S32",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "the dataset. We find that includ- ing the Dynamic Replica dataset is essential for helping the model understand camera motion effectively. 4.2. Benchmark and metrics We evaluate our model using several established datasets for moving object video segmentation. DA VIS17-Moving[13] is a subset of the DA VIS2017 dataset[48], designed specif- ically for moving object detection and segmentation. In DA VIS17-Moving, all moving instances within each video sequence are labeled, while static objects are excluded. Following the same criteria, we created DA VIS16-Moving as a subset of the DA VIS2016 dataset [47]. Addition- ally, we report performance on other popular video ob- ject segmentation benchmarks, including DA VIS2016[47], SegTrackv2[34], and FBMS-59 [43]. For evaluation, we benchmark our moving object video segmentation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S33",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "performance using region similarity (J) and contour similarity (F) metrics, as outlined in [35, 38, 61]. 4.3. Moving Object Segmentation We selected methods that specifically target moving ob- ject segmentation as baselines [35, 38, 60, 61, 70]. For RCF-All ABR OCLR-TTA CIS Ours EM GT Figure 4. Qualitative comparison on DA VIS17-moving benchmarks. For each sequence we show moving object mask results. Our method successfully handles water reflections (left), camouflage appearances (middle), and drastic camera motion (right). RCF-AllAB ROCLR-TTAC I SOurs EMGT R G B Figure 5. Qualitative comparison on FBMS-59 benchmarks. The masks produced by us are geometrically more complete and detailed. OCLR [60], we report results for two versions: OCLR-flow, which uses only flow input, and a second",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S34",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "version OCLR- TTA that incorporates test-time adaptation on top of OCLR- flow. For RCF [35], the first stage, RCF-stage 1, focuses on motion information, while the second stage, RCF-All, fur- ther optimizes the results from the first stage. We report results for both stages. For all baselines, we apply a fully connected conditional random field (CRF) [31] to refine the masks and achieve the best possible results. Notably, for multi-object scenarios, we follow the com- mon practice [16, 60, 61, 70] of grouping all foreground objects together for evaluation purposes, which we refer to as MOS. Although our approach is capable of generat- ing highly accurate, fine-grained per-object masks, as de- tailed in Sec. 4.4, we term this second evaluation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S35",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "method as fine-grained MOS. Table 1 compares the performance of our model with several baseline methods on the MOS task. Our method achieves state-of-the-art F-scores across all datasets, and our region similarity (J) scores are either the best or second-best across multiple datasets, further val- idating the effectiveness of our approach. Fig. 4 shows our visual results on the DA VIS16-Moving dataset, where our method accurately identifies object boundaries with- out incorrectly labeling moving backgrounds. Moreover, our masks exhibit strong geometric structure, particularly in challenging scenarios with significant camera motion. Fig. 5 and Fig. 6 present qualitative results on the FBMS- 59 and SegTrack v2 benchmarks, respectively. Our method performs exceptionally well in maintaining mask geometry, and even in cases",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S36",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "where the RGB images are blurred or of Ours RGB GT CIS OCLR-TTA ABR RCF-All EM Figure 6. Qualitative comparison on SegTrack v2 benchmarks. Our method succeeds even under motion blur conditions. Table 1. Quantitative comparison on MOS task which grouping all foreground objects together for evaluation.",
      "page_hint": null,
      "token_count": 47,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S37",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Motion Appearance J&F \u2191 J \u2191 F \u2191 J \u2191 J \u2191 F \u2191 J &F \u2191 J \u2191 F \u2191 CIS [70] Optical Flow RGB 66.2 67.6 64.8 62.0 63.6 - 68.6 70.3 66.8 EM [38] Optical Flow \u2717 75.2 76.2 74.3 55.5 57.9 56.0 70.0 69.3 70.7 RCF-Stage1 [35] Optical Flow \u2717 77.3 78.6 76.0 76.7 69.9 - 78.5 80.2 76.9 RCF-All [35] Optical Flow DINO 79.6 81.0 78.3 79.6 72.4 - 80.7 82.1 79.2 OCLR-flow [60] Optical Flow \u2717 70.0 70.0 70.0 67.6 65.5 64.9 71.2 72.0 70.4 OCLR-TTA [60] Optical Flow RGB 78.5 80.2 76.9 72.3 69.9 68.3 78.8 80.8 76.8 ABR [61] Optical Flow DINO 72.0 70.2 73.7 76.6 81.9 79.6 72.5 71.8 73.2 Ours",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S38",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Trajectory DINO 89.5 89.2 89.7 76.3 78.3 82.8 90.9 90.6 91.0 ABR OCLR-TTAOurs GT RGB Figure 7. Qualitative comparison on Fine-grained MOS task which will produce per-object level masks. low quality, our reliance on long-range trajectories enables accurate identification of moving objects. 4.4. Fine-grained Moving Object Segmentation Building on the initial MOS task, this task not only iden- tifies moving objects but also classifies them within their motion context to generate fine-grained, per-object masks. We evaluate our approach for multi-moving object segmen- tation specifically on the DA VIS2017-Moving dataset. For a fair comparison, we only include baselines that claim the Table 2. Quantitative comparison on DA VIS17-Moving dataset for MOS and Fine-grained MOS tasks.",
      "page_hint": null,
      "token_count": 114,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S39",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "J \u2191 F \u2191 J&F \u2191 J \u2191 F \u2191 OCLR-flow [60] 69.9 70.0 44.4 42.1 46.8 OCLR-TTA [60] 76.0 75.3 49.1 48.4 49.9 ABR [61] 74.6 75.2 51.1 50.9 51.2 Ours 90.0 89.0 80.5 77.4 83.6 ability to perform this task. Table 2 shows that our method significantly outperforms the baselines, demonstrating its superior capability in producing accurate per-object masks. Additionally, Fig. 7 illustrates that, first, our method accu- rately identifies each object, effectively distinguishing dif- ferent objects with similar motion patterns. Second, it en- sures the completeness of each object mask, handling chal- lenging cases such as articulated human structures and oc- cluded objects while maintaining mask integrity. 4.5. Ablation Study We investigate the effectiveness of our method",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S40",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "and its var- ious components on the DA VIS17-Moving and DA VIS16- Moving datasets. The former is used for fine-grained MOS, while the latter focuses on MOS. All models are trained for a full number of epochs. We conducted several experiments to assess the impor- tance of each component. The w/o DINO configuration ex- Complete Model w/o DINO w/o Space-time Attention Complete Model w/o Decouple Embedding w/o Motion-only Encoding Figure 8. Visual comparison for the ablation study on two criti- cal and challenging cases. The top sequence shows scenarios in- volves drastic camera motion and complex motion patterns, while the bottom sequence with both static and dynamic objects of the same category. The experimental setup is detailed in Sec. 4.5.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S41",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Table 3. Quantitative comparison for the ablation study on the DA VIS17-Moving and DA VIS16-Moving benchmarks, which evaluate fine-grained MOS and MOS tasks, respectively. The ex- perimental setup is detailed in Sec. 4.5.",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S42",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "J&F \u2191 J \u2191 F \u2191 J &F \u2191 J \u2191 F \u2191 w/o Depth 69.2 65.6 72.8 82.5 78.6 86.4 w/o Tracks 19.6 14.5 24.7 20.9 9.8 31.9 w/o DINO 65.0 62.1 67.9 75.5 71.4 79.5 w/o MOE 72.0 68.7 75.4 81.8 81.0 82.7 w/o MSDE 63.0 59.3 66.7 78.2 77.3 79.1 w/o PE 66.4 64.7 68.2 82.0 81.5 82.5 w/o ST-ATT 65.5 61.9 69.1 78.3 74.3 82.4 Ours-full 80.5 77.4 83.6 89.1 89.0 89.2 cludes DINO features entirely during training, while w/o MOE (Motion-only Encoding) concatenates DINO features with motion cues before the motion encoder, allowing both the encoder and decoder layers to incorporate DINO infor- mation throughout. w/o MSDE (Motion-Semantic Decou- pled Embedding) excludes DINO features from",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S43",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "the motion encoder but concatenates them with the embedded featured tracks from the encoder output, introducing semantic infor- mation through self-attention in the tracks decoder. We also test configurations w/o depth and w/o tracks, removing spe- cific inputs to observe their impact on performance. Addi- tionally, w/o PE (Positional Embedding) omits NeRF-like positional embedding in the motion encoder, and w/o ST- ATT (Spatial-temporal Attention) replaces spatial-temporal attention with conventional attention. Table 3 presents the quantitative results. We find that ex- cluding depth as input or positional encoding impacts per- formance less than other components, but it still falls signif- icantly short of the best results. When tracks are removed and only DINO features and depth maps are used, perfor- mance",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S44",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "drops drastically, indicating that the model strug- gles to learn effectively without trajectory-based informa- tion. We further analyze the key components in two chal- lenging scenarios presented below. Drastic Camera Motion. We observed that in highly chal- lenging scenes, such as those with drastic camera movement or rapid object motion, relying solely on motion informa- tion is insufficient. As shown in the upper part of Fig. 8, the colored points represent dynamic points predicted by the model, while the hollow points indicate invisible points at that moment. In this example, without DINO feature infor- mation, the model incorrectly classifies the stationary road surface as dynamic, despite the fact that the road lacks the ability to move. This information can be",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S45",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "effectively sup- plemented by incorporating DINO features. Additionally, we found that adding spatial-temporal attention within the motion encoder is particularly beneficial in these difficult scenarios, as it provides the model with richer motion infor- mation to capture the long-range motion patterns of tracks, as illustrated in Fig. 8. Distinguishing Moving and Static Objects of the Same Category. Results show that excluding DINO features en- tirely results in a performance drop, and the manner in which these features are integrated significantly affects the model\u2019s output. Simply incorporating DINO as an input during the motion encoding stage causes the model to rely heavily on semantic information, often leading it to assume that objects of the same type share the same motion state.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S46",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "In contrast, our Motion-Semantic Decoupled Embedding ar- chitecture effectively reduces this over-reliance on seman- tics, allowing the model to differentiate between moving and static objects within the same category, as illustrated in the lower part of Fig. 8. 5. Conclusion In this work, we present a novel approach that leverages long-range tracks which departs from traditional affinity matrix-based methods. Trained on extensive datasets, our model accurately identifies dynamic tracks, which, when combined with SAM2, produce precise moving object masks. Our carefully designed model architecture is tai- lored to handle long-range motion information while effec- tively balancing motion and appearance cues. Experiments show that our method achieves state-of-the-art results across multiple benchmarks, with particularly strong performance in per-object-level segmentation. 6. Limitation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S47",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "During testing, we identified several limitations of our ap- proach, which we believe can offer valuable insights. We discuss these limitations below and leave addressing these fundamental directions for future work. Dependency on Tracking Estimators. We utilizes off-the- shelf long-range tracking estimators, whose accuracy can greatly influence overall performance, as shown in Tab. 3. Fast-Moving Objects with Brief Appearances. In long- range videos, objects moving rapidly and appearing briefly pose significant challenges. Specifically, if an object moves quickly and is captured in only a few frames, resulting in very short object tracks, our method is likely to fail. Dominant Motion vs. Minor Motion. In scenes with mul- tiple moving objects, the method may struggle to capture objects with subtle movements,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S48",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "particularly when another object exhibits more pronounced motion, causing the less dynamic object to be overlooked. Partial Segmentation. The method occasionally produces incomplete segmentation masks. For instance, when a per- son moves, if the dynamic track prompts provided to SAM2 are located on the person\u2019s clothing, segmentation might capture only the clothing rather than the entire figure, lead- ing to partial or fragmented results. Homogeneous Motion State. Our segmentation frame- work also faces difficulties when motion differentiation within the scene is limited. Specifically, when most objects share similar motion states\u2014either predominantly moving or static\u2014our approach cannot effectively distinguish indi- vidual objects, leading to segmentation failures. References [1] Federica Arrigoni, Luca Magri, and Tomas Pajdla. On the Usage of the Trifocal",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S49",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Tensor in Motion Segmentation , page 514\u2013530. 2020. 3 [2] Federica Arrigoni, Luca Magri, and Tomas Pajdla. On the Usage of the Trifocal Tensor in Motion Segmentation , page 514\u2013530. 2020. 2 [3] Daniel Barath and Jiri Matas. Progressive-x: Efficient, any- time, multi-model fitting algorithm. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pat- tern Recognition, 2019. 2, 4 [4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? Cornell University - arXiv,Cornell University - arXiv, 2021. 4 [5] Berta Bescos, Jose M. Facil, Javier Civera, and Jose Neira. Dynaslam: Tracking, mapping and inpainting in dynamic scenes. IEEE Robotics and Automation Letters , page 4076\u20134083, 2018. 4 [6] Pia Bideau and Erik",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S50",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Learned-Miller. It\u2019s moving! a prob- abilistic model for causal motion segmentation in moving camera videos. Cornell University - arXiv,Cornell Univer- sity - arXiv, 2016. 2 [7] Thomas Brox and Jitendra Malik. Object segmentation by long term analysis of point trajectories. In European con- ference on computer vision, pages 282\u2013295. Springer, 2010. 2 [8] M. B \u00a8osch. Deep learning for robust motion segmenta- tion with non-static cameras. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, 2021. 2 [9] Zhe Cao, Abhishek Kar, Christian Hane, and Jitendra Malik. Learning independent object motion from unlabelled stereo- scopic videos. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2 [10] Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S51",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "and Li Zhang. Periodic vibration gaussian: Dynamic urban scene reconstruction and real-time rendering. arXiv:2311.18561, 2023. 1 [11] Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, and Sangyoun Lee. Dual pro- totype attention for unsupervised video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19238\u201319247, 2024. 4 [12] Timoth \u00b4ee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 1 [13] Achal Dave, Pavel Tokmakov, and Deva Ramanan. To- wards segmenting anything that moves. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130, 2019. 5 [14] Andrew Delong, Anton Osokin, Hossam N. Isack, and Yuri Boykov. Fast approximate energy minimization with label",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S52",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "costs. In 2010 IEEE Computer Society Conference on Com- puter Vision and Pattern Recognition, 2010. 2, 4 [15] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, Jo\u02dcao Carreira, and Andrew Zisserman. Boot- sTAP: Bootstrapped training for tracking any point. arXiv, 2024. 3, 4, 1 [16] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusion- seg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 3664\u20133673, 2017. 6 [17] E. Elhamifar and R. Vidal. Sparse subspace clustering: Al- gorithm, theory, and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S53",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "page 2765\u20132781, 2013. 2 [18] Muhammad Faisal, Ijaz Akhter, Mohsen Ali, and Richard Hartley. Epo-net: Exploiting geometric constraints on dense trajectories for motion saliency. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pat- tern Recognition, 2019. 2 [19] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapra- gasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh- Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Rad- wan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani V ora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scal-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S54",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "able dataset generator. 2022. 2, 5 [20] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir- shick. Mask r-cnn. IEEE Transactions on Pattern Analysis and Machine Intelligence, page 386\u2013397, 2020. 4 [21] Christian Homeyer and RobertBosch Gmbh. On moving ob- ject segmentation from monocular video with transformers. 3 [22] Nan Huang, Xiaobao Wei, Wenzhao Zheng, Pengju An, Ming Lu, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang. S3gaussian: Self-supervised street gaussians for autonomous driving. arXiv preprint arXiv:2405.20323, 2024. 1 [23] Yuxiang Huang and John Zelek. Motion segmentation from a moving monocular camera. 2023. 2 [24] Yuxiang Huang, Yuhao Chen, and John Zelek. Zero-shot monocular motion segmentation in the wild by combining deep learning with geometric motion model fusion. In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S55",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , pages 2733\u2013 2743, 2024. 2, 3 [25] Hossam Isack and Yuri Boykov. Energy-based geometric multi-model fitting. International Journal of Computer Vi- sion, page 123\u2013147, 2012. 2, 4 [26] Ge-Peng Ji, Keren Fu, Zhe Wu, Deng-Ping Fan, Jianbing Shen, and Ling Shao. Full-duplex strategy for video object segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4922\u20134933, 2021. 3 [27] Yangbangyan Jiang, Qianqian Xu, Ke Ma, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. What to select: Pursu- ing consistent motion segmentation from multiple geometric models. Proceedings of the AAAI Conference on Artificial Intelligence, page 1708\u20131716, 2022. 3 [28] Nikita Karaev, Ignacio Rocco, Benjamin",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S56",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dy- namicstereo: Consistent dynamic depth from stereo videos. CVPR, 2023. 2, 5 [29] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Co- tracker: It is better to track together. In Proc. ECCV, 2024. 4 [30] Laurynas Karazija, Iro Laina, Christian Rupprecht, and An- drea Vedaldi. Learning segmentation from point trajectories, 2024. 2 [31] Philipp Kr \u00a8ahenb\u00a8uhl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. Neu- ral Information Processing Systems,Neural Information Pro- cessing Systems, 2011. 6 [32] Taotao Lai, Hanzi Wang, Yan Yan, Tat-Jun Chin, and Wan- Lei Zhao. Motion segmentation via a sparsity constraint. IEEE Transactions on Intelligent Transportation Systems , page",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S57",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "973\u2013983, 2017. 2 [33] Minhyeok Lee, Suhwan Cho, Dogyoon Lee, Chaewon Park, Jungho Lee, and Sangyoun Lee. Guided slot attention for unsupervised video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3807\u20133816, 2024. 4 [34] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and James M. Rehg. Video segmentation by tracking many figure-ground segments. In 2013 IEEE International Con- ference on Computer Vision, pages 2192\u20132199, 2013. 2, 5, 1 [35] Long Lian, Zhirong Wu, and Stella X. Yu. Bootstrapping objectness from videos by relaxed common fate and visual grouping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 14582\u201314591, 2023. 4, 5, 6, 7, 2 [36] Yunze",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S58",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human- object interaction. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 21013\u201321022, 2022. 2, 5 [37] Etienne Meunier and Patrick Bouthemy. Unsupervised mo- tion segmentation in one go: Smooth long-term model over a video, 2024. 2 [38] Etienne Meunier, Ana \u00a8\u0131s Badoual, and Patrick Bouthemy. Em-driven unsupervised learning for efficient motion seg- mentation. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 45(4):4462\u20134473, 2023. 5, 7, 2 [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S59",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "scenes as neural radiance fields for view syn- thesis. In ECCV, 2020. 4 [40] Eslam Mohamed, Mahmoud Ewaisha, Mennatullah Siam, Hazem Rashed, Senthil Yogamani, Waleed Hamdy, Mo- hamed El-Dakdouky, and Ahmad El-Sallab. Monocular in- stance motion segmentation for autonomous driving: Kitti instancemotseg dataset and multi-task baseline. In 2021 IEEE Intelligent Vehicles Symposium (IV), 2021. 2 [41] Michal Neoral and Jan \u02c7Sochman. Monocular arbitrary mov- ing object discovery and segmentation. 3 [42] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE Trans- actions on Pattern Analysis and Machine Intelligence , page 1187\u20131200, 2014. 2 [43] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation of moving objects by long term video analysis. IEEE Trans-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S60",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "actions on Pattern Analysis and Machine Intelligence, 36(6): 1187\u20131200, 2014. 2, 5, 1 [44] H. Opower. Multiple view geometry in computer vision. Op- tics and Lasers in Engineering, page 85\u201386, 2002. 3 [45] Maxime Oquab, Timoth \u00b4ee Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Rus- sell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang- Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico- las Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo- janowski. Dinov2: Learning robust visual features without supervision, 2023. 1, 2, 3, 4 [46] Anestis Papazoglou and Vittorio Ferrari. Fast object segmen- tation in unconstrained video. In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S61",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "2013 IEEE International Conference on Computer Vision, 2013. 2 [47] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 2, 3, 5, 1 [48] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar- bel\u00b4aez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 2, 3, 5, 1 [49] Mohamed Ramzy, Hazem Rashed, AhmadEl Sallab, and Senthil Yogamani. Rst-modnet: Real-time spatio-temporal moving object detection for autonomous driving. Cornell University - arXiv,Cornell University - arXiv, 2019. 2 [50] S Rao, R Tron, R Vidal, and Yi Ma. Motion segmentation in the presence of outlying,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S62",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "incomplete, or corrupted trajec- tories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(10):1832\u20131845, 2010. 2 [51] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R\u00a8adle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junt- ing Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao- Yuan Wu, Ross Girshick, Piotr Doll\u00b4ar, and Christoph Feicht- enhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 1, 3, 5 [52] Sucheng Ren, Wenxi Liu, Yongtuo Liu, Haoxin Chen, Guo- qiang Han, and Shengfeng He. Reciprocal transformations for unsupervised video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15455\u201315464, 2021. 3 [53] Hicham Sekkati and Amar Mitiche. A variational method",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S63",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "for the recovery of dense 3d structure from motion.Robotics and Autonomous Systems, 55(7):597\u2013607, 2007. 2 [54] Pavel Tokmakov, Cordelia Schmid, and Karteek Alahari. Learning to segment moving objects. International Jour- nal of Computer Vision,International Journal of Computer Vision, 2017. 2 [55] Roberto Tron and Rene Vidal. A benchmark for the com- parison of 3-d motion segmentation algorithms. In 2007 IEEE Conference on Computer Vision and Pattern Recog- nition, page 1\u20138, 2007. 2 [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, AidanN. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems,Neural Information Processing Systems, 2017. 4 [57] Ren \u00b4e Vidal. Subspace clustering. IEEE Signal Processing Magazine,IEEE Signal Processing Magazine, 2011. 2",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S64",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "[58] Qianqian Wang, Vickie Ye, Hang Gao, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruc- tion from a single video, 2024. 1 [59] Andreas Wedel, Annemarie Mei\u00dfner, Clemens Rabe, Uwe Franke, and Daniel Cremers. Detection and Segmentation of Independently Moving Objects from Dense Scene Flow, page 14\u201327. 2009. 2 [60] Junyu Xie, Weidi Xie, and Andrew Zisserman. Segmenting moving objects via an object-centric layered representation. In NeurIPS, 2022. 5, 6, 7, 2 [61] Junyu Xie, Weidi Xie, and Andrew Zisserman. Appearance- based refinement for object-centric motion segmentation. In ECCV, 2024. 2, 4, 5, 6, 7 [62] Junyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman. Moving object segmentation: All you need is sam (and flow), 2024.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S65",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "2 [63] Junyu Xie, Charig Yang, Weidi Xie, and Andrew Zisserman. Moving object segmentation: All you need is sam (and flow). In ACCV, 2024. 2 [64] Xun Xu, Loong-Fah Cheong, and Zhuwen Li. Motion seg- mentation by exploiting complementary geometric models. Cornell University - arXiv,Cornell University - arXiv, 2018. 2 [65] Xun Xu, Loong Fah Cheong, and Zhuwen Li. Motion seg- mentation by exploiting complementary geometric models. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2859\u20132867, 2018. 3 [66] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Se- ung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, and Yue Wang. Emernerf: Emergent spatial- temporal scene decomposition via self-supervision. arXiv preprint arXiv:2311.02077,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S66",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "2023. 1 [67] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 3, 4 [68] Shichao Yang and Sebastian Scherer. Cubeslam: Monoc- ular 3d object slam. IEEE Transactions on Robotics , page 925\u2013938, 2019. 4 [69] Shu Yang, Lu Zhang, Jinqing Qi, Huchuan Lu, Shuo Wang, and Xiaoxing Zhang. Learning motion-appearance co- attention for zero-shot video object segmentation. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 1564\u20131573, 2021. 3 [70] Yanchao Yang, Antonio Loquercio, Davide Scaramuzza, and Stefano Soatto. Unsupervised moving object detection via contextual information separation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S67",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "pages 879\u2013888, 2019. 4, 5, 6, 7, 2 [71] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. Ds-slam: A semantic visual slam to- wards dynamic environments. In 2018 IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS), 2018. 4 [72] Kaihua Zhang, Zicheng Zhao, Dong Liu, Qingshan Liu, and Bo Liu. Deep transport network for unsupervised video ob- ject segmentation. In Proceedings of the IEEE/CVF inter- national conference on computer vision , pages 8781\u20138790, 2021. 3 [73] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajecto- ries for localizing moving cameras in the wild. In European conference on computer vision (ECCV), 2022. 4, 5 [74]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S68",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, and Ling Shao. Motion-attentive transition for zero-shot video object segmentation. In Proceedings of the AAAI conference on artificial intelligence , pages 13066\u2013 13073, 2020. 3 [75] Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, and Ling Shao. Motion-attentive transition for zero-shot video object segmentation. In Proceedings of the AAAI conference on artificial intelligence , pages 13066\u2013 13073, 2020. 2 Segment Any Motion in Videos Supplementary Material 7. Pseudo-code for SAM2 Iterative Prompting We present the pseudo-code for the first stage of SAM2 It- erative Prompting in Algorithm 1. The first stage focuses on grouping trajectories belonging to the same object and storing the trajectories of each distinct object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S69",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "in memory. Algorithm 1 Process Invisible Trajectory with Memory 1: Initialize iteration to 0 2: Initialize memory dict as an empty dictionary 3: Set take all to False 4: if traj.shape[1] \u2264 5 then 5: Set take all to True 6: end if 7: while iteration < max iterations do 8: Set t to frame with maximum visible points 9: Extract visible points at frame t 10: Find densest point as nearest point 11: Reset predictor state and add new point 12: Reset predictor state 13: Set obj id to 1 and labels to [1] 14: Add new point using predictor to get mask 15: Dilate the mask and determine points within the mask: dilated mask 16: Determine points in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S70",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "prompt mask (visible + non- dilated): prompt mask 17: if sufficient points in mask or take all is True then 18: Increment valid obj id 19: Store object information in memory dict 20: end if 21: Remove points included in the mask from traj, visiblemask, and confidences 22: Update traj, visiblemask, and confidences with remaining points 23: Increment iteration by 1 24: if traj.shape[1] < 6 then 25: Break the loop 26: end if 27: end while 28: return memory dict 8. Additional Experiment Details Training Details. We train the model for 5 epochs, with each epoch comprising approximately 8000 steps, using the Adam optimizer with a learning rate of 1e-4 and a weight decay of 1e-4. Model Architecture. As",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S71",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "shown in the Fig 9, for the tra- jectory motion pattern encoder, we employ 4 heads for multi-head attention and a 64-dimensional feed-forward layer. For the tracks decoder, we use 8 heads for multi-head attention and a 512-dimensional feed-forward layer. Multi-Head Attention Feed Forward Add & Norm DINO Add & Norm Featured tracks Embedding \ud835\udc41 \u00d7 EncoderBlock Multi-Head Attention Featured tracks Attention Block Multi-Head Attention DecoderBlock Figure 9. Architecture of tracks decoder. Model efficiency and details. We parallelized the code during data processing. For a 50-frame video, process- ing takes 2 minutes, model inference 3 seconds, and object prompt generation requires 2 seconds per object only. For a dynamic object, 1-2 iterations are usually required. And the experimental settings are",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S72",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "discussed in Sec 4.1 and Sec 7. Training time is about 60 hours, and the hardware used for training is an NVIDIA RTX A6000. Point Trajectory. We utilize BootsTAP [15] to generate 2D tracks for query frames in video sequences. Specif- ically, query frames are selected at intervals defined by step, and 2D tracks are generated only for these frames. For training datasets, grid size specifies the sampling grid resolution, determining the spacing between sampled points, while step controls the temporal interval between query frames. During training, we randomly select one query frame and load all its associated tracks to acceler- ate the process. For the Kubric dataset with a resolution of 512\u00d7512, we set grid size to 8, generating",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S73",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "4096 points per frame, and step to 8, with the total number of tracks randomly sampled from [512, 1024, 2048, 3000, 4096]. For the HOI4D dataset with a resolution of 1920 \u00d7 1080, we set grid size to 15, generating 9216 points per frame, and step to 15, with total tracks number sampled from [1024, 1536, 2048, 4096, 5000, 6000]. For the Stereo dataset with a resolution of1280\u00d7720, we set grid size to 32, generating 920 points per frame, andstep to 8, with track counts sampled from [256, 512, 768, 920]. During in- ference, 2D tracks are also generated for each query frame. To ensure that dynamic objects appearing at different times are fully captured, tracks from all query frames",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S74",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "are loaded, and 5000 tracks are randomly selected. For the FBMS-59 dataset [43], we set grid size to 7 and step to 30, be- cause some datasets contain relatively long sequences, we select a larger step to accelerate the loading process. For SegTrack V2 [34], grid size is set to 5 and step to 8. For DA VIS-16 [47] and DA VIS-17 [48],grid size is set to 10 and step to 8. ABR OCLR-TTAOursGT RGB Figure 10. Our method demonstrates exceptional capability in generating fine-grained masks. Most previous approaches rely on the common fate assumption, where objects moving at the same speed are considered part of the same entity. Moreover, many methods lack the ability to produce fine-grained masks altogether.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S75",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "In contrast, our method can accurately distinguish and segment individual objects, even when they are closely positioned, moving simultaneously, or traveling at the same speed. Failure case. We perform well on most sequences, but struggle on cases like the \u201cpenguin\u201d in SegTrackv2 (Fig 11), where 90% of the content has similar motion. The lack of contrast and uniform motion patterns can cause the model to misinterpret object motion as camera motion, leading to a J metric of 0.014. Since SAM2 requires prompts, any fail- ure in this process results in near-zero scores, whereas the baseline still achieves the 30-50 range even when it fails. Figure 11. From left to right: input, dynamic tracks and mask. 9. Additional Experiments Comparison with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S76",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "FlowSAM [63]. We further included a new baseline experiment (see Tab. 4) to demonstrate the superior performance of our model. It is worth noting that among all the baselines, only our method and FlowSAM",
      "page_hint": null,
      "token_count": 34,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S77",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "J \u2191 F \u2191 J \u2191 F \u2191 FlowSAM YES 85.7 83.8 87.1 84.9 Ours YES 89.2 89.7 90.6 91.0 Table 4. Comparison with FlowSAM on MOS task. require human annotation\u2014our method needs human anno- tation during training, but not during inference. 10. Additional Visualizations We present additional visualizations on the three main datasets that we benchmark our method on [35, 38, 60, 61, 70]. We visualize our methods on DA VIS2016 in Fig. 12, Fig. 13 on the task of moving object segmentation. And Fig. 10 shows the result of fine-grained moving object seg- mentation on DA VIS2017. Additionally, we provide a video demonstration featur- ing featuring non-cherry-picked examples from DA VIS16- Moving, showcasing both long-range trajectory label pre-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S78",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "dictions and the final mask results. RCF-AllABPOCLR-TTACISOurs EMGT Figure 12. Our method effectively preserves the geometric integrity of articulated objects, such as human legs or camel limbs. At the same time, it can distinguish between dynamic backgrounds and foregrounds, focusing specifically on the object level. Additionally, it accurately identifies camouflage-like textures, such as a camel\u2019s head blending with the wooden fence in the background. RCF-AllABPOCLR-TTACISOurs EMGT Figure 13. Our method handles occlusion scenarios more effectively. Thanks to long-range tracks, we can accurately follow a boy temporarily obscured by trees. Furthermore, our approach addresses complex situations, such as transparent glass, by including it in the mask to ensure the completeness of the moving object mask. Additionally, for highly intricate reflections, such",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2503_22268v2:S79",
      "paper_id": "arxiv:2503.22268v2",
      "section": "method",
      "text": "as vehicle shadows, our method can accurately exclude them.",
      "page_hint": null,
      "token_count": 9,
      "paper_year": 2025,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9534448669118031,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 15,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3879,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5055,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4188,
        "empty": false
      },
      {
        "page": 4,
        "chars": 5869,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5587,
        "empty": false
      },
      {
        "page": 6,
        "chars": 2168,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2798,
        "empty": false
      },
      {
        "page": 8,
        "chars": 4985,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5656,
        "empty": false
      },
      {
        "page": 10,
        "chars": 5900,
        "empty": false
      },
      {
        "page": 11,
        "chars": 5720,
        "empty": false
      },
      {
        "page": 12,
        "chars": 4264,
        "empty": false
      },
      {
        "page": 13,
        "chars": 2099,
        "empty": false
      },
      {
        "page": 14,
        "chars": 420,
        "empty": false
      },
      {
        "page": 15,
        "chars": 467,
        "empty": false
      }
    ],
    "quality_score": 0.9534,
    "quality_band": "good"
  }
}