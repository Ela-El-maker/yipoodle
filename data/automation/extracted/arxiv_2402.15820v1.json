{
  "paper": {
    "paper_id": "arxiv:2402.15820v1",
    "title": "DART: Depth-Enhanced Accurate and Real-Time Background Matting",
    "authors": [
      "Hanxi Li",
      "Guofeng Li",
      "Bo Li",
      "Lin Wu",
      "Yan Cheng"
    ],
    "year": 2024,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Matting with a static background, often referred to as ``Background Matting\" (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate background matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These limitations manifest in the form of susceptibility to varying lighting conditions and unforeseen shadows.   In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model's output undergoes refinement through Bayesian inference, incorporating a background depth prior. The posterior prediction is then translated into a \"trimap,\" which is subsequently fed into a state-of-the-art matting algorithm to generate more precise alpha mattes. To ensure real-time matting capabilities, a critical requirement for many real-world applications, we distill the backbone of our model from a larger and more versatile BGM network. Our experiments demonstrate the superior performance of the proposed method. Moreover, thanks to the distillation operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device. This high efficiency underscores DART's immense potential for deployment in mobile applications}",
    "pdf_path": "data/automation/papers/arxiv_2402.15820v1.pdf",
    "url": "https://arxiv.org/pdf/2402.15820v1",
    "doi": null,
    "arxiv_id": "2402.15820v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.074300+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2402_15820v1:S1",
      "paper_id": "arxiv:2402.15820v1",
      "section": "body",
      "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1 DART: Depth-Enhanced Accurate and Real-Time",
      "page_hint": null,
      "token_count": 17,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S2",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "Hanxi Li1,\u2020, Guofeng Li 1,\u2020, Bo Li 2,\u22c6, Lin Wu 3, and Yan Cheng 1 1Jiangxi Normal University, Jiangxi, China 2Northwestern Polytechnical University, Shaanxi, China 3Swansea University, United Kingdom",
      "page_hint": null,
      "token_count": 29,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S3",
      "paper_id": "arxiv:2402.15820v1",
      "section": "abstract",
      "text": "to as \u201cBackground Matting\u201d (BGM), has garnered significant attention within the computer vision community due to its pivotal role in various practical applications like webcasting and photo editing. Nevertheless, achieving highly accurate back- ground matting remains a formidable challenge, primarily owing to the limitations inherent in conventional RGB images. These",
      "page_hint": null,
      "token_count": 50,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S4",
      "paper_id": "arxiv:2402.15820v1",
      "section": "limitations",
      "text": "lighting conditions and unforeseen shadows. In this paper, we leverage the rich depth information provided by the RGB-Depth (RGB-D) cameras to enhance background matting performance in real-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm to incorporate depth information. The resulting model\u2019s output undergoes refinement through Bayesian inference, incorporating a back- ground depth prior. The posterior prediction is then translated into a \u201dtrimap,\u201d which is subsequently fed into a state-of-the- art matting algorithm to generate more precise alpha mattes. To ensure real-time matting capabilities, a critical requirement for many real-world applications, we distill the backbone of our model from a larger and more versatile BGM network. Our experiments demonstrate the superior performance of the proposed method. Moreover, thanks",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S5",
      "paper_id": "arxiv:2402.15820v1",
      "section": "limitations",
      "text": "to the distillation operation, our method achieves a remarkable processing speed of 33 frames per second (fps) on a mid-range edge-computing device. This high efficiency underscores DART\u2019s immense potential for deployment in mobile applications 1 Index Terms\u2014Background matting; RGB-Depth images. I. I NTRODUCTION I MAGE Image matting is a well-established problem in computer vision, with applications spanning image and video editing, video conferencing, and more. Researchers have dedicated substantial efforts to realizing automatic and high- quality image matting under real-world conditions, resulting in advancements such as deep learning-based approaches [1\u201311]. A significant challenge in automatic image matting lies in the requirement of a \u201ctrimap,\u201d a crucial input for conventional mat- ting algorithms. Without a proper trimap, the matting problem becomes",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S6",
      "paper_id": "arxiv:2402.15820v1",
      "section": "limitations",
      "text": "ill-posed, as distinguishing between the \u201cforeground\u201d and \u201cbackground\u201d can be highly ambiguous [11\u201318]. To address this issue, pioneering work such as the \u201dBackground Matting\u201d (BGM) algorithm [5, 19] was introduced. BGM conducts image matting against a fixed background, ensuring \u2020 These authors contributed equally to this work. \u22c6 Corresponding author. 1Source code are available at https://github.com/Fenghoo/DART. Err. MapRaw \ud835\udefc\ud835\udefc Fine \ud835\udefc\ud835\udefc Finer \ud835\udefc\ud835\udefc Base Network Refine Net Post Matting Depth Information",
      "page_hint": null,
      "token_count": 71,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S7",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "Test Depth RGB Information",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S8",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "Test RGB Err. Map Correction Depth patches Trimap Gen. with Depth Info. Conventional BGM & Post Processing (Optional) Proposed Depth Enhancements Fig. 1. The illustration on the proposed depth-enhanced accurate and real-time backgournd matting (DART). Left: The conventional BGM and the optional post-matting process. Right: our depth-based enhancement approach. a well-defined foreground that can be accurately extracted even without human-labeled trimaps. However, challenges persist, especially when dealing with shadows introduced by foreground objects or unexpected lighting variations. In this paper, we present an innovative approach that leverages depth information obtained from a standard RGB-D camera to significantly improve the accuracy and robustness of image matting with static backgrounds. This novel utilization of depth data addresses some of the limitations inherent",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S9",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "in traditional methods like BGM (Background Matting) and enhances the precision and reliability of matting results. We provide a schematic overview of our proposed method in Figure 1. Our approach builds upon the foundation of the BGMv2 algorithm [19] but introduces several key modifica- tions to incorporate depth information. We refer to this method as \u201cDepth-enhanced Accurate and Real-Time BackGround Matting,\u201d or simply \u201cDART\u201d for brevity. Specifically, we utilize depth information to correct the \u201cerror map\u201d estimated by the base network of the original BGM model [19] and refine the \u201ctrimap\u201d used in the post-matting process. Addi- tionally, we replace the use of traditional RGB patches with RGB-D patches in the refining network [19]. Furthermore, we introduce the concept of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S10",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "distillation, where we derive a smaller base network [20] from the original ResNet 50- based model used in BGMv2. These modifications collectively result in significantly improved matting performance compared to state-of-the-art methods, all while maintaining exceptional arXiv:2402.15820v1 [cs.CV] 24 Feb 2024 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2 computational efficiency. Our DART algorithm can achieve a remarkable speed of 125 FPS on an affordable desktop GPU and still maintains a respectable 33 FPS on a mid-range edge- computing platform. II. T HE PROPOSED METHOD The workflow of DART is illustrated in Fig. 2. As Fig. 2, we follow BGMv2 as the base network, and then we employ a smaller network (MobileNetv2 [21]) for higher efficiency",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S11",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "and the network parameter is learned via distilling the knowledge from the base network of BGMv 2, followed by a fine-tuning on the current background. Besides, for different stages of the",
      "page_hint": null,
      "token_count": 31,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S12",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "in different manners to increase the matting accuracy and robustness. The remaining part of this section will detail the proposed depth-enhanced BGM algorithm. A. Efficient base network from distillation In the original BGMv 2 algorithm, the ResNet 50-based [22] \u201cbase network\u201d of BGMv 2 processes an input image I \u2208 ZH\u00d7W\u00d73 as the following function: \u03a6r(I) \u2192 {A\u2217 raw \u2208 R H 4 \u00d7W 4 , E\u2217 RGB \u2208 R H 4 \u00d7W 4 }, (1) where A\u2217 raw \u2208 R H 4 \u00d7W 4 denotes the raw prediction of the alpha matte while E\u2217 RGB \u2208 R H 4 \u00d7W 4 is the predicted \u201cerror map\u201d or in other words, the uncertain region of A\u2217 raw [19]. In this",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S13",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "work, we employ a MobileNetv2-based [21] network to predict the raw alpha of the test image. The similar inference process of this smaller model can be denoted as: \u03a6m(I) \u2192 {Araw \u2208 R H 4 \u00d7W 4 , ERGB \u2208 R H 4 \u00d7W 4 }. (2) where \u03a6m(I) denotes the MobileNetv2-based model which is learned via distillation. Following the SOTA distillation algorithm for segmentation [23], the knowledge transfer from \u03a6r to \u03a6m can be realized via minimizing the following loss function: Ldistill = KL(Araw, A\u2217 raw) + \u2225Araw \u2212 AGT \u2225l1 + \u2225ERGB \u2212 EGT \u2225l2 , (3) where AGT and EGT stand for the ground-truth alpha matte and error map respectively; KL(\u00b7) denotes the Kull- back\u2013Leibler divergence between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S14",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "two prediction maps. Note that in this paper we do not impose different weights for the three losses as the straightforward summation can already lead to sufficiently good results. B. Error map correction in a Bayesian style Besides the RGB images, the depth channel is also informa- tive enough to estimate the foreground region roughly. In this work, we conduct the depth-based matting in the Bayesian manner. In particular, given the (resized) depth map of the test image D \u2208 R H 4 \u00d7W 4 and the background depth map set Db = {\u2200 Di b \u2208 R H 4 \u00d7W 4 | i = 1, 2, \u00b7\u00b7\u00b7 , N} 2, one can first fill the unknown depth pixels with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S15",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "the special value \u22121. Then 2In this paper we assume that multiple static background RGB-D images are captured before matting. Note that this is trivial to realize considering that most RGB-D cameras can run faster than 30 FPS. the mean value and the standard deviation of the background depth on the coordinate [r, c] can be calculated as: D r,c b = ( 1 |K| P i\u2208K Di b(r, c), |K| > 0 dmax, |K| = 0 \u03c3r,c b = \uf8f1 \uf8f4\uf8f4\uf8f2 \uf8f4\uf8f4\uf8f3 q 1 |K|\u22121 P i\u2208K(Di b(r, c) \u2212 D r,c b )2, |K| > 1 \u03c8 \u00b7 D r,c b , |K| = 1 \u03c8 \u00b7 dmax, |K| = 0 (4) where dmax is the maximum detecting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S16",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "distance of the depth sensor; \u03c8 is a small ratio; K = {\u2200i | Di b(r, c) > 0} and | \u00b7 |denotes the set cardinality. Consequently, the conditional probability of an observed pixel depth d on the coordinate [r, c], given this pixel belongs to the foreground is calculated as: Pr,c F (d) = P(D(r, c) = d | F) = ( 1 D r,c b , d \u2208 (0, D r,c b ] 0, else (5) Similarly, the conditional probability under the background condition writes: Pr,c B (d) = P(D(r, c) = d | B) = ( N+ r,c(D r,c b , \u03c3r,c b ), d > 0 0, else (6) where N+ r,c(D r,c b , \u03c32",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S17",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "b ) denotes the estimated normal distribu- tion of the background depth value on the pixel [r, c] 3. In a Bayesian way, the posterior probability that the pixel [r, c] belongs to the foreground can be calculated as: \u02dcP r,c F (d) = P(F | D(r, c) = d) = Pr,c F (d) \u00b7 PF + PF \u00b7 \u03b6 Pr,c F (d) \u00b7 PB + Pr,c F (d) \u00b7 PF + \u03b6 (7) where P F and P B = 1 \u2212 PF are the pre-defined prior probabilities of foreground and background, respectively; \u03b6 is a small-valued parameter to ensure the foreground probability of a depth-unknown pixel equals P F . We then arrive at the foreground posterior map",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S18",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "AD \u2208 R H 4 \u00d7W 4 with each element defined as: AD(r, c) = \u02dcP r,c F (D(r, c)), \u2200r, c. (8) In practice, we post-process AD(r, c) by gaussian blurring and small region removal for more robust prediction. Recall that the RGB-based raw alpha matte defined in Equ. 2, we can compare the two foreground probability maps to generate a residual map as ED = |AD \u2212 Araw|\u25e6 \u2208 R H 4 \u00d7W 4 , (9) where | \u00b7 |\u25e6 denotes the element-wise absolute value. The residual map is used as a complement to the RGB-based error map ERGB and the corrected error map is given by: ERGBD = \u03b2 \u00b7 ED + (1 \u2212 \u03b2) \u00b7 ERGB,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S19",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "(10) where \u03b2 is the balancing parameter for fusing the two error maps. 3Note that the negative part of the distribution is truncated and the probability function is scaled so that the integral value over the domain [0, \u221e) is 1. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3 Bayesian Estimator Refine Net Trimap Generation ViTMatte \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 Base Network (MobileNetV2) Base Network \uff08ResNet50\uff09 Distill Test Depth Background Depth \ud835\udefc prediction based on depth Raw \ud835\udf36Prediction",
      "page_hint": null,
      "token_count": 82,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S20",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "Test Image \ud835\udf36Refine \u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7 RGB Patches Depth Patches Uncertain \ud835\udefc Depth Err. Map RGB Err. Map Raw \ud835\udefc (RGB) RGB-D Err. Map Post Matting (optional) Fine \ud835\udefc Trimap Finer \ud835\udefc Err. Patches Err. Map fusion Raw \ud835\udefc (Depth) Fig. 2. The workflow of the proposed DART algorithm. The three stages of DART are shown in the gray, purple, and green regions. Better view in color. C. Alpha refinement with RGB-D patches The second stage of the conventional BGM algorithm is the patch-level correction to the raw prediction. In the RGB- D scenario of this work, we propose to employ the RGB-D patch, rather than the RGB path as the input of the refinement net. Considering that the depth information",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S21",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "is relatively inde- pendent to the RGB content, the RGB-D patch could lead to a more accurate correction. This benefit is also proved empirically in the experiment part. In a mathematical way, the refining model in this work can be defined as \u2126(I, D, Araw, ERGBD) \u2192 Afine \u2208 RH\u00d7W (11) D. Post matting with depth information Given the sufficient computational or time budget, the optional post-matting can significantly increase the matting accuracy. In this paper, we employ the depth information to generate a better \u201ctrimap\u201d for the SOTA matting algorithm [24]. Recall that the refined alpha prediction is denoted as Afine which can be viewed as the \u201cprior\u201d foreground prob- ability to the next inference stage, thus the posterior",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S22",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "map considering the depth evidence is calculated as: \u02dcAr,c fine = Pr,c F (d) \u00b7 Ar,c fine Pr,c F (d) \u00b7 (1 \u2212 Ar,c fine ) + Pr,c F (d) \u00b7 Ar,c fine . (12) We then generate a trimap based on the posterior map \u02dcAfine as \u02dcAfine Gaussian Blur \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2192\u02dcA\u2020 fine Two Threshoulds \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2192T \u2208 RH\u00d7W , (13) where T is a triple-valued map with 1-valued pixels indicating the foreground objects, 0-valued ones indicating the back- ground area and 0.5-valued ones standing for the unknown region. This trimap is finally fed into the SOTA ViTMatte algorithm [24] for generating more accurate foreground masks.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S23",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "Note that this post-matting process is optional as the better performance is achieved at the cost of speed decreasing. E. Proposed Dataset As we introduced before, no RGB-D background matting dataset is available so far. We thus design and produce a specific dataset for this task. The proposed dataset, termed \u201dJXNU RGBD Background Matting\u201d or JXNU-RGBD for short, contains 12 indoor scenes, with each one involves 100 pure background RGB-D images and 5 RGB-D images with foreground objects for testing. Only the alpha matte on the test images is manually labeled for algorithm evaluation, and the labels are strictly unseen during training. Fig. 3 illustrates 8 out of the 12 scenes of the proposed dataset. 1 5 432 876 Fig.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S24",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "3. The proposed RGB-D background matting dataset. 8 scenes are illustrated here, each with one RGB image pair (test and background) and one depth map pair. The dark blue pixels of the depth map stand for the depth-unknown region. Better view in color. F . Training strategy and implementation detail Following the standard training protocol of background mat- ting [19], we train our DART model only based on real back- ground (RGB-D) images and synthetic (RGB-D) foreground objects. In particular, the ResNet 50-based base network is trained based on the training set proposed in BGMv 2 [19] augmented with the foreground objects (RGB) extracted from the X-Humans dataset [25]. Secondly, the ResNet 50 model is then fine-tuned on the proposed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S25",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "JXNU-RGBD dataset with JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4 only background images (RGB) and artificial foregrounds. Finally, the MobileNetv2-based base network is distilled from the fine-tuned larger model based upon the background images (RGB) merely from the target scene. As to the refine net of DART, we first fuse the rendered RGB-D samples from the X- Humans dataset [25] with all the background RGB-D images of JXNU-RGBD to train the raw model and then slightly fine- tune it using only the information from the target scene. As to the hyperparameters, we set \u03b2 = 0 .05 to fuse the two error map, \u03c8 = 0.01, dmax = 5460, \u03b6 = 0.001. The two thresholds",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S26",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "for generating trimap are [0.25, 0.8]. When distilling the base network, the batch size is set to 16 and the learning rates for the three submodules of the base network are [1e \u2212 4, 5e \u2212 4, 5e \u2212 4]. III. E XPERIMENT A. Basic settings In this section, we perform a series of experiments to evaluate the proposed method, compared with the BGM-V 2 [19] (the SOTA background matting method); ViTMatte [24] (the SOTA general purpose matting algorithm); HIM [8], SGHM [26] and P3M-Net [10] (three SOTA human matting algorithms). Four commonly used matting metrics, namely the sum of absolution difference (SAD), mean square error(MSE), gradi- ent error (Grad), and connectivity error (Conn) are employed for scoring the comparing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S27",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "methods. We also report the FPS numbers for each matting algorithm. Most experiments are conducted on a desktop PC with an Intel i5-13490F CPU, 32G DDR4 RAM , and an NVIDIA RTX4070Ti GPU. To evaluate the compatibility of the proposed method on the edge-computing devices, we also run the proposed method on an NVIDIA Jetson Orin NX development board, with the reimplemented code employing the TensorRT SDK [27] and the FP16 data format. B. Quantitative Results The matting accuracies of the involved methods are reported in Tab. III-B, along with the corresponding running FPS numbers. Note that ViTMatte requires a trimap as a part of its input, we thus generate a trimap for it based on the posterior map defined",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S28",
      "paper_id": "arxiv:2402.15820v1",
      "section": "background",
      "text": "in Equ. 7. TABLE I THE MATTING PERFORMANCES OF THE PROPOSED METHOD AND THE COMPARING SOTA ALGORITHMS . THE RUNNING FPS OF EACH COMPARING ALGORITHM IS ALSO ILLUSTRATED IN THE LAST COLUMN . NOTE THAT MSE IS DIVIDED BY 1000 FOR EASE OF READING . THE SPEED IS MEASURED ON THE DESKTOP ENVIRONMENT DESCRIBED IN SEC. III-A.",
      "page_hint": null,
      "token_count": 57,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S29",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "VITMatte[24] 17.71 11.16 21.67 16.60 5 P3M-Net [10] 18.78 8.60 10.9 18.57 4 SGHM [26] 6.95 2.67 8.51 6.41 12 HIM [8] 4.28 1.09 6.92 3.55 4 BGM [19] 4.78 1.86 10.05 4.67 81 DART 3.39 1.22 8.89 3.33 125 DART + ViTMatte[24] 2.90 0.61 6.02 2.42 5 According to the table, one can clearly see the superiority of the proposed DART algorithm. In particular, the DART with post-processing beats all the SOTA methods evaluated by all four metrics; the vanilla DART algorithm achieves slightly worse matting accuracy compared with its sophisticated ver- sion but enjoys a 25-time faster speed. Fig. 4 demonstrates the performances of the compared",
      "page_hint": null,
      "token_count": 108,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S30",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "the matting accuracy while the algorithm speed is represented by the marker\u2019s color (the redder, the faster) and size (the larger, the faster). From the figure, we can see the proposed DART algorithm performs fastest among all the comparing",
      "page_hint": null,
      "token_count": 39,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S31",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "Orin NX), our method can also achieve real-time speed ( 33 FPS) while still enjoying a remarkable accuracy advantage to the fast BGM-V 2 algorithm [19]. 04820 SAD 0 2 4 12 MSE DRAT-ViTMatte-PyTorch-4070Ti DRAT-PyTorch-4070Ti DRAT-TensorRT-NX BGMv2-PyTorch-4070Ti BGMv2-TensorRT-NX SGHM-PyTorch-4070Ti P3M-Net-PyTorch-4070Ti HIM-PyTorch-4070Ti Depth-ViTMatte-PyTorch-4070Ti 0 30 60 90 FPS Fig. 4. Speed and accuracy comparison of involved matting methods. Better view in color. C. Ablation study The ablation study is shown in Tab. II from where we can see a consistent increasing trend of matting error when the proposed modules are removed from the DART algorithm one by one. TABLE II THE ABLATION STUDY OF THE PROPOSED METHOD .",
      "page_hint": null,
      "token_count": 107,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S32",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "DART + ViTMatte[24] 2.9 0.61 6.02 2.42 DART 3.39 1.22 8.89 3.33 DART-NoErrorMap 3.56 1.26 9.09 3.45 DART-NoDepthRefine 4.07 1.75 10.31 3.75 DART-NoErrorMap-NoDepthRefine 4.27 1.8 10.43 3.92 IV. C ONCLUSION This paper introduces a fixed-background matting algorithm that is enhanced by depth information. By smartly exploiting the useful and complementary depth channel of an RGB-D image, the proposed DART algorithm achieves more accurate results compared with the existing SOTA matting approaches. Meanwhile, thanks to the successful distillation process, our method is fast: it runs at 125 FPS on a GPU-equipped desktop PC and 33 FPS on a mid-range edge-computing platform. To evaluate the proposed algorithm, we make a dedicated dataset, termed \u201cJXNU-RGBD\u201d for the RGB-D background matting task. To the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S33",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "best of our knowledge, this paper is the first work in the literature that explores the RGB-D matting problem with a fixed background. It paves the way to future better solutions for this new but realistic computer vision problem. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5 REFERENCES [1] X. Shen, X. Tao, H. Gao, C. Zhou, and J. Jia, \u201cDeep automatic portrait matting,\u201d in European Conference on Computer Vision. Springer, 2016, pp. 92\u2013107. [2] B. Zhu, Y . Chen, J. Wang, S. Liu, B. Zhang, and M. Tang, \u201cFast deep matting for portrait animation on mobile phone,\u201d in Proceedings of the 25th ACM international conference on Multimedia , 2017, pp. 297\u2013305. [3] Q. Chen, T.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S34",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "Ge, Y . Xu, Z. Zhang, X. Yang, and K. Gai, \u201cSemantic human matting,\u201d in Proceedings of the 26th ACM international conference on Multimedia , 2018, pp. 618\u2013626. [4] J. Liu, Y . Yao, W. Hou, M. Cui, X. Xie, C. Zhang, and X.-s. Hua, \u201cBoosting semantic human matting with coarse annotations,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 8563\u20138572. [5] S. Sengupta, V . Jayaram, B. Curless, S. M. Seitz, and I. Kemelmacher-Shlizerman, \u201cBackground matting: The world is your green screen,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 2291\u20132300. [6] J. Li, J. Zhang, S. J. Maybank, and D. Tao, \u201cBridging composite and real: towards",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S35",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "end-to-end deep image mat- ting,\u201d International Journal of Computer Vision, pp. 246\u2013 266, 2022. [7] Z. Ke, J. Sun, K. Li, Q. Yan, and R. W. Lau, \u201cModnet: Real-time trimap-free portrait matting via objective de- composition,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, 2022, pp. 1140\u20131147. [8] Y . Sun, C.-K. Tang, and Y .-W. Tai, \u201cHuman instance matting via mutual guidance and multi-instance refine- ment,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 2647\u20132656. [9] Y . Dai, B. Price, H. Zhang, and C. Shen, \u201cBoosting robustness of image matting with context assembling and strong data augmentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S36",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "pp. 11 707\u201311 716. [10] S. Ma, J. Li, J. Zhang, H. Zhang, and D. Tao, \u201cRethinking portrait matting with privacy preserving,\u201d International journal of computer vision , pp. 1\u201326, 2023. [11] J. Li, J. Zhang, and D. Tao, \u201cDeep image matting: A com- prehensive survey,\u201d arXiv preprint arXiv:2304.04672 , 2023. [12] N. Xu, B. Price, S. Cohen, and T. Huang, \u201cDeep image matting,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2970\u2013 2979. [13] X. Fang, S.-H. Zhang, T. Chen, X. Wu, A. Shamir, and S.-M. Hu, \u201cUser-guided deep human image matting using arbitrary trimaps,\u201d IEEE Transactions on Image Processing, pp. 2040\u20132052, 2022. [14] Y . Sun, C.-K. Tang, and Y .-W. Tai,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S37",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "\u201cUltrahigh resolution image/video matting with spatio-temporal sparsity,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 14 112\u201314 121. [15] G. Park, S. Son, J. Yoo, S. Kim, and N. Kwak, \u201cMat- teformer: Transformer-based image matting via prior- tokens,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022, pp. 11 696\u201311 706. [16] Q. Liu, H. Xie, S. Zhang, B. Zhong, and R. Ji, \u201cLong- range feature propagating for natural image matting,\u201d in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 526\u2013534. [17] Y . Zheng, Y . Yang, T. Che, S. Hou, W. Huang, Y . Gao, and P. Tan, \u201cImage matting with deep gaussian process,\u201d",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S38",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "IEEE Transactions on Neural Networks and Learning Systems, 2022. [18] Y . Liu, J. Xie, X. Shi, Y . Qiao, Y . Huang, Y . Tang, and X. Yang, \u201cTripartite information mining and integration for image matting,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 7555\u20137564. [19] S. Lin, A. Ryabtsev, S. Sengupta, B. L. Curless, S. M. Seitz, and I. Kemelmacher-Shlizerman, \u201cReal-time high- resolution background matting,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8762\u20138771. [20] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d in NIPS Deep Learning and Representation Learning Workshop , 2015. [21] M. Sandler, A. Howard, M. Zhu, A.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S39",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "Zhmoginov, and L.-C. Chen, \u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510\u2013 4520. [22] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recog- nition, 2016, pp. 770\u2013778. [23] C. Shu, Y . Liu, J. Gao, Z. Yan, and C. Shen, \u201cChannel- wise knowledge distillation for dense prediction,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 5311\u20135320. [24] J. Yao, X. Wang, S. Yang, and B. Wang, \u201cVitmatte: Boosting image matting with pretrained plain vision transformers,\u201d arXiv preprint arXiv:2305.15272 , 2023. [25] K. Shen, C.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2402_15820v1:S40",
      "paper_id": "arxiv:2402.15820v1",
      "section": "method",
      "text": "Guo, M. Kaufmann, J. J. Zarate, J. Valentin, J. Song, and O. Hilliges, \u201cX-avatar: Expressive human avatars,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 16 911\u201316 921. [26] X. Chen, Y . Zhu, Y . Li, B. Fu, L. Sun, Y . Shan, and S. Liu, \u201cRobust human matting via semantic guidance,\u201d in Proceedings of the Asian Conference on Computer Vision (ACCV), 2022, pp. 2984\u20132999. [27] S. Migacz, \u201c8-bit inference with tensorrt,\u201d in GPU tech- nology conference, 2017, p. 5.",
      "page_hint": null,
      "token_count": 88,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9331650924264818,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 5,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5092,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5074,
        "empty": false
      },
      {
        "page": 3,
        "chars": 3792,
        "empty": false
      },
      {
        "page": 4,
        "chars": 5245,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5441,
        "empty": false
      }
    ],
    "quality_score": 0.9332,
    "quality_band": "good"
  }
}