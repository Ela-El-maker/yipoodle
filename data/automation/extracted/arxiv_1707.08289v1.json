{
  "paper": {
    "paper_id": "arxiv:1707.08289v1",
    "title": "Fast Deep Matting for Portrait Animation on Mobile Phone",
    "authors": [
      "Bingke Zhu",
      "Yingying Chen",
      "Jinqiao Wang",
      "Si Liu",
      "Bo Zhang",
      "Ming Tang"
    ],
    "year": 2017,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Image matting plays an important role in image and video editing. However, the formulation of image matting is inherently ill-posed. Traditional methods usually employ interaction to deal with the image matting problem with trimaps and strokes, and cannot run on the mobile phone in real-time. In this paper, we propose a real-time automatic deep matting approach for mobile devices. By leveraging the densely connected blocks and the dilated convolution, a light full convolutional network is designed to predict a coarse binary mask for portrait images. And a feathering block, which is edge-preserving and matting adaptive, is further developed to learn the guided filter and transform the binary mask into alpha matte. Finally, an automatic portrait animation system based on fast deep matting is built on mobile devices, which does not need any interaction and can realize real-time matting with 15 fps. The experiments show that the proposed approach achieves comparable results with the state-of-the-art matting solvers.",
    "pdf_path": "data/automation/papers/arxiv_1707.08289v1.pdf",
    "url": "https://arxiv.org/pdf/1707.08289v1",
    "doi": null,
    "arxiv_id": "1707.08289v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.076896+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_1707_08289v1:S1",
      "paper_id": "arxiv:1707.08289v1",
      "section": "body",
      "text": "Fast Deep Matting for Portrait Animation on Mobile Phone Bingke Zhu1,2, Yingying Chen1,2, Jinqiao Wang1,2, Si Liu2,3, Bo Zhang4, Ming Tang1,2 1National Lab of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China 2University of Chinese Academy of Sciences, Beijing, China 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China 4North China University of Technology, Beijing, China {bingke.zhu, yingying.chen, jqwang, tangm}@nlpr.ia.ac.cn liusi@iie.ac.cn, zhangbo@ncut.edu.cn",
      "page_hint": null,
      "token_count": 66,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S2",
      "paper_id": "arxiv:1707.08289v1",
      "section": "abstract",
      "text": "Image matting plays an important role in image and video editing. However, the formulation of image matting is inherently ill-posed. Traditional methods usually employ interaction to deal with the image matting problem with trimaps and strokes, and cannot run on the mobile phone in real-time. In this paper, we propose a real-time automatic deep matting approach for mobile devices. By leveraging the densely connected blocks and the dilated convolution, a light full convolutional network is designed to predict a coarse binary mask for portrait image. And a feathering block, which is edge-preserving and matting adaptive, is further developed to learn the guided filter and transform the binary mask into alpha matte. Finally, an automatic portrait animation system based on fast",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S3",
      "paper_id": "arxiv:1707.08289v1",
      "section": "abstract",
      "text": "deep matting is built on mobile devices, which does not need any interaction and can realize real-time matting with 15 fps. The experiments show that the proposed approach achieves comparable results with the state-of-the-art matting solvers. KEYWORDS Portrait Matting; Real-time; Automatic; Mobile Phone",
      "page_hint": null,
      "token_count": 43,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S4",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "Image matting plays an important role in computer vision, which has a number of applications, such as virtual reality, augmented reality, interactive image editing, and image stylization [ 27]. As people are increasingly taking selfies and uploading the edited to the social network with mobile phones, real-time matting technique is in demand which can handle real world scenes. However, the formulation of image matting is still an ill-posed problem. Given an input image I, image matting problem is equivalent to decomposing it into foreground F and background B in assumption that I is blended linearly by F and B: I = \u03b1F + (1 \u2212\u03b1)B, (1) where \u03b1 is used to evaluate the foreground opacity(alpha matte). In natural image matting,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S5",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "all quantities on the right-hand side of the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM \u201917, October 23\u201327, 2017, Mountain View, CA, USA \u00a9 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-4906-2/17/10.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S6",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": ". . $15.00 https://doi.org/10.1145/3123266.3123286 (a) (b) (c) Figure 1: Examples of portrait animation. (a) The original images which are the inputs to our system. (b) The fore- grounds of the original images, which are computed by the Eq. (1). (c) The portrait animation on mobile phone. composting Eq. (1) are unknown, which makes the original matting problem ill-posed. To alleviate the difficulty of this problem, popular image matting techniques such as [8, 10, 17, 28] require users to specify foreground and background color samples with trimaps or strokes, which makes a rough distinction among foreground, background and regions with unknown opacity. Benefit from the trimaps and strokes, the interactive mattings have high accuracy and have the capacity to segment the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S7",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "elaborate hair. However, despite the high accuracy of these matting techniques, these methods rely on user involvement (label the trimaps or strokes) and require extensive computation, which restricts the application scenario. In order to deal with the interactive problem, Shen et al. [28] proposed an automatic mat- ting with the help of semantic segmentation technique. But their automatic matting have a very high computational complexity and it is still slow even on the GPU. In this paper, to solve user involvement and apply matting tech- nique on mobile phones, we propose a real-time automatic matting method on mobile phone with a segmentation block and a feather- ing block. Firstly, we calculate a coarse binary mask by a light full convolutional",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S8",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "network with dense connections. And then we de- sign a learnable guided filter to obtain the final alpha matte through a feathering block. Finally, the predicted alpha matte is calculated through a linear transformation of the coarse binary mask, whose weights are obtained from the learnt guided filter. The experiments show that our real-time automatic matting method achieves com- parable results to the general matting solvers. On the other hand, arXiv:1707.08289v1 [cs.CV] 26 Jul 2017 Down- Sampling Convolutions Up- Sampling Segmentation Block FB Convolutions Feathering Feathering Block Forward Backward Figure 2: Pipeline of our end-to-end real-time image matting framework. It includes a segmentation block and a feathering block. we demonstrate that our system possesses adaptive matting ca- pacity dividing the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S9",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "parts of head and the neck precisely, which is unprecedented in other alpha matting methods. Thus, our system is useful in many applications like real-time video matting on mobile phone. And real-time portrait animation on mobile phone can be realized using our fast deep matting technique as shown in some examples in Figure 1. Compared with existing methods, the major contributions of our work are three-fold: 1. A fast deep matting network based on segmentation block and feathering block is proposed for mobile devices. By adding the dialed convolution into the dense block structure, we propose a light dense network for portrait segmentation. 2. A feathering block is proposed to learn the guided filter and transform the binary mask into",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S10",
      "paper_id": "arxiv:1707.08289v1",
      "section": "introduction",
      "text": "alpha matte, which is edge-preserving and possesses matting adaptive capacity. 3. The proposed approach can realize real-time matting on the mobile phones and achieve comparable results with the state-of- the-art matting solvers.",
      "page_hint": null,
      "token_count": 32,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S11",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "Image Matting: For interactive matting, in order to infer the alpha matte in the unknown regions, a Bayesian matting [7] is proposed to model background and foreground color samples as Gaussian mixtures. Levin et al. [17] developed closed matting method to find the globally optimal alpha matte. He et al. [10] computed the large- kernel Laplacian to accelerate matting Laplacian computation. Sun et al. [29] proposed Poisson image mating to solve the homoge- nous Laplacian matrix. Different from the methods based on the Laplacian matrix, shared matting [8] presented the first real-time matting technique on modern GPUs by shared sampling. Benefit from the performance of deep learning, Xu et al. [34] developed an encoder-decoder network to predict alpha matte with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S12",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "the help of trimaps. Recently, Aksoy et al. [1] designed an effective inter-pixel information flow to predict the alpha matte and reach the state of the art. Different from the interactive matting, Shen et al. [28] first proposed an end-to-end convolutional network to train the automatic matting. With the rapid development of mobile devices, automatic mat- ting is a very useful technique for image editing. However, despite the high efficiency of these techniques, both interactive matting and automatic matting cannot reach real-time matting on CPUs and mobile phone. The Laplacian-based methods need to compute the Laplacian matrix and its inverse, which is an N \u00d7N symmetric matrix, where N is the number of unknowns. Neither non-iterative methods [17], nor iterative",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S13",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "methods [10] can solve the sparse linear system efficiently due to the high computing cost. The real-time image matting methods like shared matting can be real-time with the help of powerful GPUs, but these methods are still slow on CPUs and mobile phone. Semantic Segmentation: Long et al. [19] first proposed to solve the problem of semantic segmentation with fully convolutional network. ENet [ 20] is a light full convolutional network for se- mantic segmentation, which can reach real-time on GPU. Huang et al. [13] and Bengio et al. [ 14] proposed a dense network with several densely connected blocks, which make the receptive filed of prediction more dense. Specially, Chen et al. [2] developed an operation called dilated convolution,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S14",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "which makes the receptive filed more dense and bigger. Similarly, Yu et al. [35] proposed to use dilated convolution to aggregate the context information. On the other hand, semantic and multi-scale information is important in image understanding and semantic segmentation [9, 31, 32, 37]. In order to absorb more semantic and multi-scale information, Zhao et al. [36] proposed a pyramid structure with dilated convolution, Chen et al. [3] proposed an atrous spatial pyramid pooling (ASPP), and Peng et al. [21] used the large kernel convolutions to extract features. Based on these observations, we try to combine the works of Huang [13] and Chen [2] to create a light dense network,which can predict the segmentation densely in several layers. Guided Filter:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S15",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "Image filtering plays an important role in computer vision, especially after the development of convolutional neural network. He et al. [11] proposed an edge-preserving filter - guided filter to deal with the problem of gradient drifts, which is a com- mon problem in image filtering. Despite the powerful performance of convolutional neural network, it loses the information of edge when it comes to pixel-wise predicted assignments such as semantic segmentation and alpha matting due to its piecewise smoothing. 3 OVERVIEW The pipeline of our system is presented in Figure 2. The input is a color image I and the output is the alpha matte A . The network consists of two stages. The first stage is a portrait segmentation network",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S16",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "which takes an image as input and obtains a coarse binary mask. The second stage is a feathering module that refines the foreground/background mask to the final alpha matte. The first Input (128*128*3) Conv (64*64*13) Max-Pooling (64*64*3) Concat (64*64*16) Conv(dilated rate: 2) (64*64*16) Concat (64*64*32) Conv(dilated rate: 4) (64*64*16) Concat (64*64*48) Conv(dilated rate: 6) (64*64*16) Concat (64*64*64) Conv(dilated rate: 8) (64*64*16) Concat (64*64*64) Conv (64*64*2) Interp (128*128*2) Output (128*128*2) Figure 3: Diagram of our light dense network for portrait segmentation. stage provides a coarse binary mask in a fast speed with a light full convolutional network, while the second stage refines the coarse binary mask with a single filter, which reduces the error greatly. We will describe our algorithm with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S17",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "more details in the following sections. 3.1 Segmentation Block In order to segment the foreground with a fast speed, we propose a light dense network in the segmentation block. The architecture of the light dense network is presented in Figure 3. Output sizes are reported for an example input image resolution of 128 \u00d7128. The initial block is a concatenation of a 3 \u00d73 convolution and a max-pooling, which is used to down sample the input image. The dilated dense block contains four convolutional layers in different dilated rates and four densely connections. The concatenation of four convolutional layers are sent to the final convolution to obtain a binary feature maps. Finally, we interpolate the feature maps to get the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S18",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "score maps which have the same size as the original image. Specifically, our light dense network has 6 convolutional layers and 1 max-pooling layer. The motivation of this architecture is inspired by several popular full convolutional networks for semantic segmentation. Inspired by the real-time segmentation architecture of ENet [20] and Incep- tion network [30], we use the initial block in ENet to down sample the input, which can maintain more information than that of max- pooling and bring less computation cost than that of convolution. Then the down-sampling maps are sent to the dilated dense block, which is inspired by the densely connected convolution network [13] and the dilated convolution network [ 2]. Each layer of the dilated dense block",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S19",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "obtains different field of view, which is impor- tant in the multi-scale semantic segmentation. The dilated dense block can seize the foreground in variable sizes, thus obtain a better segmentation mask by the final classifier. 3.2 Feathering Block The segmentation block can produce a coarse binary mask to the alpha matte. However, the binary mask cannot represent the alpha matte due to the coarse edge. Despite fully convolution networks have been proved to be effective in the semantic segmentation [19], the methods like [34] using fully convolutional networks may suffer from the gradient drifts because of the pixel-wise smoothing in the convolutional operations. In this section, we will discuss a feathering block to refine the binary mask and solve the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S20",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "problem of gradient drifts. 3.2.1 Architecture. The convolutional operations play impor- tant roles in the classification networks. However, alpha matting is a pixel-wise predicted task, and it is unsuitable to predict the edge of the objects with the convolutional operations due to its piecewise smoothing. The state of the art method [2] relieves this problem using the conditional random field (CRF) model to refine the edge of the objects, but it costs too much computing memory and it cannot solve the problem in the convolutional neural networks intrinsically. Motivated by the gradient drifts of the convolution operations, we developed a sub-network to learn the filters of the coarse binary mask, which does not suffer from the gradient drifts. The architecture",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S21",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "of the feathering block is presented in Figure 4. The inputs to the feathering block are an image, the corresponding coarse binary mask, the square of the image as well as the product of the image and its binary mask. The design of these inputs is inspired by the guided filter [11], whose weights are designed as a function of these inputs. We concatenate the inputs and send the concatenation into the convolutional network which contains two 3 \u00d73 convolutional layers. Then we can obtain three maps corresponding to the weights and bias of the binary mask. The feathering layer can be represented as a linear transform of the coarse binary mask in sliding windows centered at each pixel: \u03b1i",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S22",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "= akSFi + bkSBi + ck , \u2200i \u2208\u03c9k , (2) where \u03b1 is the output of feathering layer represented as alpha matte, SF is the foreground score from the coarse binary mask, SB is the background score, i is the location of the pixel, and (ak , bk , ck )are the linear coefficients assumed to be constant in the k-th sliding window \u03c9k . Thus, we have: qi = ak Fi + bk Bi + ck Ii , \u2200i \u2208\u03c9k , (3) where qi = \u03b1i Ii , Fi = IiSFi , Bi = IiSBi , and I is the input image. Concat Feathering Conv + BN + ReLU Conv Figure 4: Architecture of the feathering block, which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S23",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "is used to learn the filter refining the coarse binary mask. I is the input image, S is the score maps output from the segmenta- tion network, A is the alpha matte which is the final output of the whole network, and \u2299is the hadamard product. From (3) we can get the derivative \u2207q = a\u2207F + b\u2207B + c\u2207I, (4) where q = \u03b1I, F = IS F , B = IS B . It ensures that the feathering block possesses the property of edge-preserving and matting adaptive. As discussed above, both two score mapsSF and SB would have strong responses in the area of edge because of the uncertainty in these areas. However, it is worth to note",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S24",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "that the score maps of foreground and background are allowed to have inaccurate responses when the parameters a, b, c are trained well. In this case, we hope that the parameters a, b become as small as possible, which means that the inaccurate responses are suppressed. In other word, the feathering block can preserve the edge as long as the absolute values of a, b are set to small in the area of edge while c is predominant on it. Similarly, if we want to segment the neck apart from the head, the parameters a, b, c should be set to small in the area of neck. Moreover, it is interesting that the feathering block performs like ensemble learning because",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S25",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "we can treat F, B, I as the classifiers and the parameters a, b, c as the weights for classification. When we apply the linear model to all sliding windows in the entire image, the value \u03b1i is not the same in different windows. We leverage the same strategy as He et al. [11]. After computing (ak , bk , ck )for all sliding windows in the image, we average all the possible values of \u03b1i : \u03b1i = 1 |w| \u00d5 k:i \u2208wk akSFi + bkSBi + ck = aiSFi + biSBi + ci , (5) where ai = 1 |\u03c9| \u00d5 k \u2208\u03c9i ak , bi = 1 |\u03c9| \u00d5 k \u2208\u03c9i bk , ci = 1 |\u03c9|",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S26",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "\u00d5 k \u2208\u03c9i ck . With this modification, we can still have\u2207q \u2248a\u2207F +b\u2207B +c\u2207I be- cause \u0010 ak , bk , ck \u0011 are the average of the filters, and their gradients should be much smaller than that of I near strong edges. To determine the linear coefficients, we design a sub-network to seek the solution. Specially, our network leverages a loss func- tion including two parts, which makes the alpha predictions more accurate. The first loss L\u03b1 measures the alpha matte, which is the absolute difference between the ground truth alpha values and the predicted alpha values at each pixel. And the second loss Lcolor is a compositional loss, which is the L2-norm loss function for the predicted",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S27",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "RGB foreground. Thus, we minimize the following cost function: L = L\u03b1 + Lcolor , (6) where Li \u03b1 = r\u0010 \u03b1i \u0434t \u2212\u03b1ip \u00112 + \u03b52, Li color = \u00d5 j \u2208{R, G, B } r\u0010 \u03b1i \u0434t Ii j \u2212qi j \u00112 + \u03b52, \u03b1i p = akSFi + bkSBi + ck . This loss function is chosen for two reasons. Intuitively, we hope to obtain an accurate alpha matte through the feathering block in the end, thus we use the alpha matte lossL\u03b1 to learn the parameters. On the other hand, the second loss Lcolor is used to maintain the information of the input image as much as possible. Since the score maps SF and SB would",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S28",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "lose the information of edges due to the pixel-wise smoothing, we need to use the input image to recover the lost detail of the edge. Therefore, we leverage the second loss Lcolor to guide the learning process. Despite existing deep learning methods like [6, 28, 34] used the similar loss function to solve the alpha matting problem, we have a totally different motivation and a different solution. Their loss functions are used to estimate the expression of alpha matting without considering the gradient drifts for the convolution operation, which causes the pixel-wise smooth alpha matte and loses the gradients of input image, while our loss function is edge-preserving. Through the gradient propagation, the parameters of feathering layer can be updated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S29",
      "paper_id": "arxiv:1707.08289v1",
      "section": "related_work",
      "text": "pixel-wise so as to correct the edges. It is because that the guided filter is an edge-preserving filter. However, other deep learning methods can only update the parameters in the fixed windows. As a result, their",
      "page_hint": null,
      "token_count": 36,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S30",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "results are edge-preserving. 3.2.2 Guided Image Filter. The guided filter [ 11] proposed a local linear model: qi = ak Ii + bk , \u2200i \u2208\u03c9k , (7) (a) (b) (c) (d) (e) Figure 5: Examples of the filters learnt from the feathering block. (a) The input images. (b) The foregrounds of the original images, which are calculated by the Eq. (1). (c) The weights ak of feathering block in Eq. (2) and Eq. (3). (d) The weights bk of feathering block in Eq. (2) and Eq. (3). (e) The weights ck of feathering block in Eq. (2) and Eq. (3). where q is a linear transform of I in a window \u03c9k centered at the pixel k, and (ak",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S31",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": ", bk )are some linear coefficients assumed to be constant in \u03c9k . In order to determine the linear coefficients, they minimize the following cost function in the window: E (ak , bk )= \u00d5 i \u2208\u03c9k \u0010 (ak Ii + bk \u2212pi )2 + \u03b5a2 k \u0011 . (8) Our feathering block does not use the same form like [10] be- cause we hope to obtain not only an edge-preserving filter, but also a filter with the capacity of matting adaptive. It means the filters can suppress the inaccurate response and obtain a finer matting result. As a result, the guided filter needs to rely on an accurate binary mask while our filter does not. Therefore, the feathering block",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S32",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "can be viewed as an extension of the guided filter. We extend the linear transform of the image to the linear transform of the foreground image, the background image as well as the whole image. More- over, instead of solving the linear regression to obtain the linear coefficients, we leverage a sub-network to optimize the cost func- tion, which should be much faster and reduce the computational complexity. Figure 5 shows that our feathering block is closely to the guided filter in [ 11]. Both the filter of ours and guided filter have high response on the high variance regions. However, it is obvious that there are great distinctions between our filter and guided filter. The differences between our filter",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S33",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "and guided filter can be summarized into two aspects briefly: 1. Different inputs. The inputs of guided filter are the image as well as the corresponding binary mask while our inputs are the image and the score maps of the foreground and the background. 2. Different outputs. The output of guided filter is a feathering results relied on an accurate binary mask, while our learnt guided filter outputs an alpha matte or a foreground image, which possesses the fault tolerance to the binary mask. In fact, our feathering block is matting oriented, therefore, the linear transform model of learnt guided filter in Eq. (3) is inferred from the linear transform model of matting filter in Eq. (2). Thus, our learnt",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S34",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "guided filter has multiple product terms but no constant term. Though two filters are derived in different process, they have the same properties such as edge-preserving, which has been proved in Eq. (4). Moreover, Figure 5 shows that our learnable guided filter possesses adaptive matting capacity. The general matting methods like [8, 17] would take the neck as the foreground if we define the neck in the image as unknown region, because the gradient between the face and neck is small. However, our method can divide the face and neck into the foreground and background, respectively. 3.2.3 Attention Mechanism. Our proposed feathering block is not only an extension of the guided filter, but also can be interpreted as an attention",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S35",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "mechanism. Figure 5 shows several examples of guided filters learnt from the feathering block. Intuitively, it can be interpreted as an attention mechanism, which pays different attention to various parts according to factors. Specially, from the examples in Figure 5, we can infer that the factora pays more atten- tion to the part of object\u2019s body, the factor b pays more attention to the part of background, and the factor c pays more attention to the part of object\u2019s head. Consequently, it can be inferred that the factors a and b emphasize the matting problem locally while the factor c considers the matting problem globally.",
      "page_hint": null,
      "token_count": 105,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S36",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": "4.1 Dataset and Implementation Dataset: We collect the primary dataset from [ 28], which is col- lected from Flickr. After training on this dataset, we release our app to obtain more data for training. Furthermore, we hired tens of well-trained students to accomplish the annotation work. All the ground truth alpha mattes are labelled with KNN matting [4] firstly, and refined carefully with Photoshop quick selection. Specially, we labelled the alpha matte in two ways. The first way of labelling is same as [28, 34], which labels the whole human as the foreground. The second way of labelling only labels the heads of human as the foreground. The head labels help us demonstrate that our proposed method possesses adaptive matting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S37",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": "capacity dividing the parts of head and neck precisely. After labelling process, we collect 2,000 images with high-quality mattes, and split them randomly into training and testing sets with 1,800 and 200 images respectively. For data augmentation, we adopt random mirror and random resize between 0.75 and 1.5 for all images, and additionally add random rotation between -30 and 30 degrees, and random Gaussian blur. This comprehensive data augmentation scheme prevents the network overfitting and greatly improves the performance of our system to handle new images with possibly different scale, rotation, noise and intensity. Implementation details: We setup our model training and testing",
      "page_hint": null,
      "token_count": 103,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S38",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": "Figure 2, we use a stochastic gradient descent (SGD) solver with cross-entropy loss function, batch size 256, momentum 0.99 and weight decay 0.0005. We train our model without any further post- processing module nor pre-training. The unknown weights are initialized with random values. Specially, we train our network with a three-stage strategy. Firstly, we train the light dense network with a learning rate of 1e-3 and 1e-4 in the first 10k and the last 10k iterations, respectively. Secondly, the weights in light dense network is fixed and we only train the feathering block with a learning rate set to 1e-6 which will be divided by 10 after 10k iterations. Finally, we train the whole network with a fixed learning rate",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S39",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": "of 1e-7 for 20k iterations. We found that the three-stage training strategy makes the training process more stable. All experiments on computer are performed on a system of Core E5-2660 @2.60GHz CPU and a single NVIDIA GeForce GTX TITAN X GPU with 12GB memory. We also test our approach on the mobile phone with a Qualcomm Snapdragon 820 MSM8996 CPU and Adreno 530 GPU. 4.2 Head Matting Dataset Accuracy Measure. We select the gradient error and mean squared error to measure matting quality, which can be expressed as: G \u0000A , A \u0434t \u0001 = 1 K \u00d5 i",
      "page_hint": null,
      "token_count": 99,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S40",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": "\u2207Ai \u2212\u2207A \u0434t i",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S41",
      "paper_id": "arxiv:1707.08289v1",
      "section": "experiments",
      "text": ", (9) MSE \u0000A , A \u0434t \u0001 = 1 K \u00d5 i \u0000A \u2212A \u0434t \u00012, (10) where A is the predicted matte and A \u0434t is the corresponding ground truth, K is the number of pixels in A , \u2207is the operator to compute gradients.",
      "page_hint": null,
      "token_count": 47,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S42",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "for matting and report the performance of these methods in Table 1. From the results, we can draw the conclusion that our proposed",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S43",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "portrait segmentation networks, LDN produces a coarse binary mask, which increases 3 times gradient error and 3 to 5 times mean squared error approximately, compared to the semantic segmen- tation networks. However, LDN is 20 to 60 times faster than the semantic segmentation networks on CPU. Secondly, taking our Table 1: Results on Head Matting Dataset. We compare the components of our proposed system with state-of-the-art se- mantic segmentation networks Deeplab and PSPN [2, 36]. Light Dense Network (LDN) greatly increases the speed and Feathering Block (FB) deceases the gradient error(Grad. Er- ror) and mean squared error(MSE) greatly in our system. Ad- ditionally, our feathering block has a better performance than guided filter (GF).",
      "page_hint": null,
      "token_count": 114,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S44",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "(ms) GPU (ms) Grad. Error(\u00d710\u22123) MSE (\u00d710\u22123) Deeplab101 [2] 1243 73 3.61 5.84 PSPN101 [36] 1289 76 3.88 7.07 PSPN50 [36] 511 44 3.92 8.04 LDN 27 11 9.14 27.49 Deeplab101+FB 1424 78 3.46 4.23 PSPN101+FB 1343 83 3.37 3.82 PSPN50+FB 548 52 3.64 4.67 LDN+GF 236 220 4.66 15.34 LDN+FB 38 13 3.85 7.98 Table 2: Results on Human Matting Dataset of [28]. DAPM means the approach of Deep Automatic Portrait Matting in [28]. LDN means the light dense network and FB means the feathering block. We report our speed on computer(comp.) and mobile phone(phone.) respectively.",
      "page_hint": null,
      "token_count": 97,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S45",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "/comp. (ms) GPU /comp. (ms) CPU /phone. (ms) GPU /phone. (ms) Grad. Error (\u00d710\u22123) DAPM 6000 600 - - 3.03 LDN+FB 38 13 140 62 7.40 feathering block into consideration, we find that FB can refine the portrait segmentation networks with little effort on CPU or GPU. Moreover, the feathering block decreases the gradient error and mean squared error of our proposed LDN greatly, which decreases 30% of the errors approximately. Furthermore, when compared to the guided filter, it deceases 2 times of mean squared error. Thirdly, our proposed LDN+FB structure have comparable accuracies with the combinations of segmentation networks and feathering blocks, while it is 15 to 40 times faster. Visually, we present the results of binary mask, binary",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S46",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "mask with guided filter, and alpha matte from our feathering block in Figure 6. It is obvious that the binary masks are very coarse which lose the edge shape and contain lots of isolated holes. The binary mask with guided filter can fill the holes thereby enhancing the performance of the mask, however, the alpha mattes are over transparent due to the coarse binary mask. In contrast, our proposed system achieves better performance by making the foreground clearer and remaining the edge shape. In order to illustrate the efficiency and effectiveness of the light dense block and feathering block, we discuss them separably. Firstly, comparing the four portrait segmentation networks, though our (a) (b) (c) (d) (e) Figure 6: Several",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S47",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "results from binary mask, binary mask with guided filter, and alpha matte from our feathering block. (a) The original images. (b) The ground truth foreground. (c) The foreground calculated by the binary mask. (d) The foreground calculated by the binary mask with guided filter. (e) The foreground calculated by the binary mask with our feathering block. light dense network has a low performance, it decreases the com- puting time greatly, and it can produce a coarse segmentation with 8 convolutional layers. Secondly, the feathering block increases the performances of four portrait segmentation networks. It decreases the error of our proposed light dense network greatly, which holds a comparable results with the best results and keeps a high speed. Ad- ditionally,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S48",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "comparing the performances between guided filter and feathering block, we can infer that feathering block outperforms the guided filter in accuracy while keeping a high speed. 4.3 Human Matting Dataset Furthermore, in order to demonstrate the efficiency and effective- ness of our proposed system. We train our system on the dataset of [28], which contains a training set and a testing set with 1,700 and 300 images respectively, and compare our result with that of their proposed methods. We report the performance of these automatic matting methods in Table 2. From the reported results, we find that our proposed",
      "page_hint": null,
      "token_count": 99,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S49",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "error reported in [ 28]. However, our method decreases the time cost of 5962ms on CPU and 587ms on GPU. Consequently, it can be illustrated that our method increases the matting speed greatly while keeping a comparable result with the general alpha matting",
      "page_hint": null,
      "token_count": 43,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S50",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "[28] can not run on the CPU of mobile phone because it needs a lot of time to infer the alpha matte. In contrast, our method can not only run on the GPU of mobile phone in real-time, but also has the capacity to run on the CPU of mobile phone. 4.4 Comparison with Fabby Additionally, we compare our system with Fabby1, which is a real- time face editing app. The visual results are presented in Figure 7. It is shown that we have better results than that of Fabby. Moreover, we found that Fabby cannot locate the objects such as the cases in former three columns and it fails to process some special cases like the faults in the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S51",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "last three columns. In contrast, our method can hold these cases well. From the former three columns in Figure 7, we can infer that Fabby needs to detect the objects first, and then computes the alpha matte. As a result, sometimes it is unable to calculate the alpha matte of the whole object. Conversely, our method can calculate the alpha matte of all pixels in an image and thus obtain the opacity of the whole object. Furthermore, Fabby fails to divide the similar background from foreground for the case of the fourth column, and fails to segment the foreground such as collar and shoulder in last two columns, while our method outperforms Fabby in these cases. Consequently, it can be",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S52",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "inferred that our light matting network has a more reliable alpha matte than that of Fabby. 5 APPLICATIONS Since our proposed system is fully automatic and can run fully real- time on mobile phone, a number of applications are enabled, such as fast stylization, color transform, background editing and depth- of-field. Moreover, it can not only process the image matting, but also real-time video matting due to its fast speed. Thus, we establish an app for real-time portrait animation on mobile phone, which 1The app of Fabby can be downloaded on the site www.fab.by Figure 7: The results between our system and real-time editing app Fabby. First row: The input images. Second row: The results from the app Fabby. Third",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S53",
      "paper_id": "arxiv:1707.08289v1",
      "section": "method",
      "text": "row: The results from our system. Figure 8: Examples of our real-time portrait animation app. is illustrated in Figure 8. Further, we test the speed of our method on the mobile phone with a Qualcomm Snapdragon 820 MSM8996 CPU and an Adreno 530 GPU. When tests on mobile phone, it runs 140ms on the CPU. Furthermore, after applying render script [16] to optimize the speed with GPU in Android, our approach takes 62ms on the GPU. Consequently, we can conclude that our proposed method is efficient and effective on mobile devices.",
      "page_hint": null,
      "token_count": 91,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S54",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "In this paper, we propose a fast and effective method for image and video matting. Our novel insight is that refining the coarse segmentation mask can be fast and effective than the general im- age matting methods. We developed a real-time automatic matting method with a light dense network and a feathering block. Besides, the filter learnt by the feathering block possesses edge-preserving and matting adaptive capacity. Finally, a portrait animation system based on fast deep matting is built on mobile devices. Our method does not need any interaction and can realize real-time matting on the mobile phone. The experiments show that our real-time automatic matting method achieves comparable results with the state-of-the-art matting solvers. However, our system fails to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S55",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "distin- guish tiny details in the hair areas because we have downsampled the input image. The downsampling operation would provide a more complete result but lose the detailed information. We treat this as the limitation of our method. In future, we will try to improve our method for higher accuracy by combining multiple technolo- gies like object detection [5, 18, 24, 25], image retargeting [22, 33] and face detection [12, 23, 26]. REFERENCES [1] Yag\u0131z Aksoy, Tun\u00e7 Ozan Ayd\u0131n, and Marc Pollefeys. 2017. Designing Effective Inter-Pixel Information Flow for Natural Image Matting. In Computer Vision and Pattern Recognition (CVPR), 2017 . [2] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2015. Semantic image segmentation with deep",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S56",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "convolutional nets and fully connected crfs. In International Conference on Learning Representations (ICLR), 2015 . [3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. 2017. DeepLab: Semantic Image Segmentation with Deep Convo- lutional Nets, Atrous Convolution, and Fully Connected CRFs. IEEE transactions on pattern analysis and machine intelligence (2017). [4] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. 2013. KNN matting. IEEE transactions on pattern analysis and machine intelligence 35, 9 (2013), 2175\u20132188. [5] Yingying Chen, Jinqiao Wang, Min Xu, Xiangjian He, and Hanqing Lu. 2016. A unified model sharing framework for moving object detection. Signal Processing 124 (2016), 72\u201380. [6] Donghyeon Cho, Yu Wing Tai, and Inso Kweon. 2016. Natural Image Matting Using Deep Convolutional Neural",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S57",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "Networks. InEuropean Conference on Computer Vision (ECCV), 2016 . 626\u2013643. [7] Yung-Yu Chuang, Brian Curless, David H Salesin, and Richard Szeliski. 2001. A bayesian approach to digital matting. In Computer Vision and Pattern Recognition (CVPR), 2001. Proceedings of the 2001 IEEE Computer Society Conference on , Vol. 2. IEEE, II\u2013II. [8] Eduardo SL Gastal and Manuel M Oliveira. 2010. Shared sampling for real- time alpha matting. In Computer Graphics Forum , Vol. 29. Wiley Online Library, 575\u2013584. [9] Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent, and Roberto Cipolla. 2015. SceneNet: Understanding Real World Indoor Scenes With Synthetic Data. CoRR abs/1511.07041 (2015). [10] Kaiming He, Jian Sun, and Xiaoou Tang. 2010. Fast matting using large kernel matting laplacian matrices. In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S58",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on . IEEE, 2165\u20132172. [11] Kaiming He, Jian Sun, and Xiaoou Tang. 2013. Guided image filtering. IEEE transactions on pattern analysis and machine intelligence 35, 6 (2013), 1397\u20131409. [12] Peiyun Hu and Deva Ramanan. 2017. Finding Tiny Faces. In Computer Vision and Pattern Recognition (CVPR), 2017 . [13] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. 2017. Densely connected convolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2017 . [14] Simon J\u00e9gou, Michal Drozdzal, David V\u00e1zquez, Adriana Romero, and Yoshua Bengio. 2017. The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. InWorkshop on Computer Vision in Vehicle Technology CVPR, 2017 . [15] Yangqing Jia,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S59",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolu- tional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia . ACM, 675\u2013678. [16] Seyyed Salar Latifi Oskouei, Hossein Golestani, Matin Hashemi, and Soheil Ghiasi. 2016. CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural Networks on Android. In Proceedings of the 2016 ACM on Multimedia Conference. 1201\u20131205. [17] Anat Levin, Dani Lischinski, and Yair Weiss. 2008. A closed-form solution to natural image matting. IEEE Transactions on Pattern Analysis and Machine Intelligence 30, 2 (2008), 228\u2013242. [18] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed, Cheng-Yang Fu, and Alexander C. Berg. 2016. SSD: Single",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S60",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "Shot MultiBox Detector. In ECCV. [19] Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 3431\u20133440. [20] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. 2016. Enet: A deep neural network architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147 (2016). [21] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. 2017. Large Ker- nel Matters - Improve Semantic Segmentation by Global Convolutional Network. In Computer Vision and Pattern Recognition (CVPR), 2017 . [22] Shaoyu Qi, Yu-Tseh Chi, Adrian M. Peter, and Jeffrey Ho. 2016. CASAIR: Content and Shape-Aware Image Retargeting and Its Applications. IEEE Transactions on Image Processing 25",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S61",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "(2016), 2222\u20132232. [23] Hongwei Qin, Junjie Yan, Xiu Li, and Xiaolin Hu. 2016. Joint Training of Cascaded CNN for Face Detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 3456\u20133465. [24] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: Better, Faster, Stronger. In Computer Vision and Pattern Recognition (CVPR), 2017 . [25] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2015), 1137\u20131149. [26] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A unified embedding for face recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015), 815\u2013823. [27] Xiaoyong Shen, Aaron Hertzmann,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S62",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "Jiaya Jia, Sylvain Paris, Brian Price, Eli Shecht- man, and Ian Sachs. 2016. Automatic portrait segmentation for image stylization. In Computer Graphics Forum , Vol. 35. Wiley Online Library, 93\u2013102. [28] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and Jiaya Jia. 2016. Deep Automatic Portrait Matting. InEuropean Conference on Computer Vision . Springer, 92\u2013107. [29] Jian Sun, Jiaya Jia, Chi-Keung Tang, and Heung-Yeung Shum. 2004. Poisson matting. In ACM Transactions on Graphics (ToG) , Vol. 23. ACM, 315\u2013321. [30] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex A. Alemi. 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learn- ing. In ICLR 2016 Workshop . [31] Jinqiao Wang, Ling-Yu Duan, Qingshan Liu, Hanqing Lu, and Jesse S. Jin.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S63",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "2008. A Multimodal Scheme for Program Segmentation and Representation in Broadcast Video Streams. IEEE Trans. Multimedia 10 (2008), 393\u2013408. [32] Jinqiao Wang, Wei Fu, Hanqing Lu, and Songde Ma. 2014. Bilayer Sparse Topic Model for Scene Analysis in Imbalanced Surveillance Videos. IEEE Transactions on Image Processing 23 (2014), 5198\u20135208. [33] Jinqiao Wang, Zhan Qu, Yingying Chen, Tao Mei, Min Xu, La Zhang, and Han- qing Lu. 2016. Adaptive Content Condensation Based on Grid Optimization for Thumbnail Image Generation. IEEE Trans. Circuits Syst. Video Techn. 26 (2016), 2079\u20132092. [34] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. 2017. Deep Image Matting. In Computer Vision and Pattern Recognition (CVPR), 2017 . [35] Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1707_08289v1:S64",
      "paper_id": "arxiv:1707.08289v1",
      "section": "conclusion",
      "text": "by dilated convolutions. In International Conference on Learning Representations (ICLR), 2016. [36] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. 2017. Pyramid Scene Parsing Network. In Computer Vision and Pattern Recognition (CVPR), 2017. [37] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. 2016. Semantic Understanding of Scenes through the ADE20K Dataset. CoRR abs/1608.05442 (2016).",
      "page_hint": null,
      "token_count": 62,
      "paper_year": 2017,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9555463440030542,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 9,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5163,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5287,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4810,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4301,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4323,
        "empty": false
      },
      {
        "page": 6,
        "chars": 5584,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3695,
        "empty": false
      },
      {
        "page": 8,
        "chars": 3445,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5883,
        "empty": false
      }
    ],
    "quality_score": 0.9555,
    "quality_band": "good"
  }
}