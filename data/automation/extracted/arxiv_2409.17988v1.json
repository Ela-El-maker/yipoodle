{
  "paper": {
    "paper_id": "arxiv:2409.17988v1",
    "title": "Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions",
    "authors": [
      "Weng Fei Low",
      "Gim Hee Lee"
    ],
    "year": 2024,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "The stark contrast in the design philosophy of an event camera makes it particularly ideal for operating under high-speed, high dynamic range and low-light conditions, where standard cameras underperform. Nonetheless, event cameras still suffer from some amount of motion blur, especially under these challenging conditions, in contrary to what most think. This is attributed to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure that event cameras can truly excel in such conditions where it has an edge over standard cameras, it is crucial to account for event motion blur in downstream applications, especially reconstruction. However, none of the recent works on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblur e-NeRF, a novel method to directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events generated under high-speed motion or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model proposed to account for event motion blur under arbitrary speed and lighting conditions. We also introduce a novel threshold-normalized total variation loss to improve the regularization of large textureless patches. Experiments on real and novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset will be open-sourced.",
    "pdf_path": "data/automation/papers/arxiv_2409.17988v1.pdf",
    "url": "https://arxiv.org/pdf/2409.17988v1",
    "doi": null,
    "arxiv_id": "2409.17988v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.782620+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2409_17988v1:S1",
      "paper_id": "arxiv:2409.17988v1",
      "section": "body",
      "text": "Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions Weng Fei Low and Gim Hee Lee The NUS Graduate School\u2019s Integrative Sciences and Engineering Programme (ISEP) Institute of Data Science (IDS), National University of Singapore Department of Computer Science, National University of Singapore {wengfei.low, gimhee.lee}@comp.nus.edu.sg https://wengflow.github.io/deblur-e-nerf",
      "page_hint": null,
      "token_count": 48,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S2",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "them ideal for high-speed, high dynamic range & low-light environments, where standard cameras underperform. However, event cameras also suf- fer from motion blur, especially under these challenging conditions, con- trary to what most think. This is due to the limited bandwidth of the event sensor pixel, which is mostly proportional to the light intensity. Thus, to ensure event cameras can truly excel in such conditions where it has an edge over standard cameras, event motion blur must be accounted for in downstream tasks, especially reconstruction. However, no prior work on reconstructing Neural Radiance Fields (NeRFs) from events, nor event simulators, have considered the full effects of event motion blur. To this end, we propose, Deblure-NeRF, a novel method to directly",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S3",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "and effectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions. The core component of this work is aphysically-accurate pixel bandwidth model that accounts for event motion blur. We also introduce a threshold-normalized total variation loss to better regularize large textureless patches. Experiments on real & novel realistically simulated sequences verify our effectiveness. Our code, event simulator and synthetic event dataset are open-sourced. Keywords: Neural Radiance Field\u00b7 Motion blur\u00b7 Event camera",
      "page_hint": null,
      "token_count": 74,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S4",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "Event cameras offer a complementary approach to visual sensing, commonly achieved with frame-based cameras. Instead of capturing intensity images at a fixed rate, event cameras asynchronously detect changes in log-intensity per pixel and output a stream ofevents, each encoding the time instant, pixel location and direction of change. Such a stark contrast in design philosophy enable event cameras to offer many attractive properties over standard cameras,e.g. high dynamic range, high temporal resolution, low latency and low power [7]. These desirable properties make event cameras particularly ideal for appli- cations that involve high-speed motion,High Dynamic Range(HDR)/low-light arXiv:2409.17988v1 [cs.CV] 26 Sep 2024 2 W. F. Low and G. H. Lee (b)Significantlymotion-blurredevents(left),generatedunderhigh-speedandlow-lightconditions(right). (a)Minimallymotion-blurredevents(left),generatedunderlow-speedandbright-lightconditions(right). E2VID +NeRFRobust e-NeRF Deblur e-NeRF(Ours)Target(c)NovelviewssynthesizedfromNeRFsthatarereconstructedusingseverelymotion-blurredeventsgeneratedunderhigh-speedandlow-lightconditions. Fig. 1:Existing works on NeRF",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S5",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "reconstruction from moving event cameras heavily rely on (a). In contrast, Deblure-NeRF is able to directly and effectively reconstruct blur-minimal NeRFs from (b), as shown in (c). scenes and/or a strict power budget, such as in robotics, augmented reality, surveillance and mobile imaging. Since these are exactly the operating condi- tions where standard cameras underperform, event cameras meaningfully com- plements standard cameras. This is clearly demonstrated in the recent success of image deblurring [14,26,49,56], attributed to the addition of an event camera. Nonetheless, event cameras also suffer from motion blur [12,27,52], especially under high speed or low light, albeit much less severe than standard cameras. This is contrary to what most think, as it has not been widely documented and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S6",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "discussed in the computer vision community. In general, motion blur of events are manifested as a time-varying latency on the event generation process. In severe cases, a significant \u201closs or introduction of events\u201d may also occur, especially the former. This leads to artifacts such as event trails and blurring of edges, when visualizing events in 2D as their image-plane projection, as shown in Figs. 1 and 2. Event motion blur can be attributed to the limited bandwidth of the event sensor pixel, which is mostly proportional to the incident light intensity [3,5,8,9,12,24,31]. It bounds the minimum event detection latency, maximum frequency of change detectable and hence maximum event generation rate. Therefore, to ensure event cameras can truly excel under conditions",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S7",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "of high- speed or low-light, where it has an edge over standard cameras, it is crucial to account for event motion blur in downstream tasks, especially reconstruction. While recent works on reconstructingNeural Radiance Fields(NeRFs) [32] from events [13,18,28,41], and possibly images [1,18,30,38], have shown impressive results, none of them accounted for event motion blur, which limits their perfor- Deblur e-NeRF 3 extremely motion blurred. Both effects reduce the number of generated brightness change events (to about 2 per edge) and increase their timing jitter.v2e models these effects to produce realistic low-light synthetic DVS events. pixel very dark photocurrent I p + I dark or Y s dark current I dark moderately bright photoreceptor V p or L lp Time (ms)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S8",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "ON OFF \ufb01nite aperture \"motion blur\" Figure 2. Simulated DVS pixel photoreceptor and resulting ON and OFF events under moderate and extremely low illumination. Both photocurrent and dark current include shot noise which is proportional to the mean current. Code to reproduce: https://git.io/JOWbG 3.2. Motion blur For frame-based video, motion blur is simply a low- pass box \ufb01lter imposed by the \ufb01nite integration time for the frame. It should be obvious from Fig. 2 that a DVS pixel does not respond instantly to an edge: The \ufb01nite re- sponse time of the photoreceptor blurs the edge. The transi- tion from one brightness level to another is like the response of an RC lowpass \ufb01lter. The bigger the step, the longer",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S9",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "it takes for the pixel to settle to the new brightness value. The result is that a passing edge will result in an extended series of events as the pixel settles down to the new value. This \ufb01nite response time over which the pixel continues to emit events is the equivalent \u201cmotion blur\u201d of DVS pixels. Un- der bright indoor illumination, typical values for the pixel motion blur are on the order of 1 ms. Under very low il- lumination, the equivalent pixel motion blur can extend for tens of milliseconds. Fig. 3 shows measured DA VIS346 [ 33] DVS motion blur of a moving edge under bright and dark conditions. Users typically view DVS output as 2D frames of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S10",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "histogrammed event counts collected over a \ufb01xed integration time (Fig. 3A and C). The frame integration time low-pass \ufb01lters the DVS output stream just like conventional video cameras. The additional DVS motion blur can be easily observed by lining up the events in a 3D space-time view of the event cloud that A B D C 500X darkerBright 2D (x,y,p) 24 ms 3D (x,y,t) 24 ms edge events aligned White bar motion 420 pix/s 10 pix 10 pix \u22641 pix 7 pix \u224815 ms DVS motion blur Frame motion blur Figure 3. Measured motion blur of real DVS outputs for a moving white bar (speed: 420 pixels/s) on a dark background. compensates for the motion of the edge (Figs. 3B",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S11",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "and D). In this view, the DVS motion blur appears as a thickened edge. In Fig. 3B, the motion blur of the leading white edge is less than 1 pixel ( i.e., less than 2 ms), but in Fig. 3D, the blur is about 7 pixels or 15 ms. 3.3. Latency Quick response time is a clear advantage of DVS cam- eras, and they have been used to build complete visually servoed robots with total closed-loop latencies of under 3 ms [ 4, 3]. But it is important to realize the true range of achievable response latency. For example, high-speed USB computer interfaces impose a minimum latency of a few hundred microseconds [ 4]. latency jitter DVS OFF events time",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S12",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "LED o\ufb00 A B home o\ufb03ce night street moonlit surgery cloudy day sunny day 920 us\u00b16% 4 ms\u00b128% 15 us\u00b13.3% 400 us\u00b143% 10u 100u 1m 10m 1 10 100 1000 10,000 chip illuminance [lux] 200 2k 20k 200k scene illuminance [lux] latency [s] biased for speed nominal biases 1 st 2 nd Figure 4. Real DVS latency measurements to turning off blink- ing LED.A: de\ufb01nition of response latency. B: measured data. Adapted from [ 17], with scene illumination axis based on [ 5]. Added to these computer and operating system laten- cies are the DVS sensor chip latencies, which are illustrated in Fig. 4 with real DVS data. This experiment recorded the response latency to a blinking LED turning off.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S13",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "The horizontal axis in Fig. 4B is in units of lux (visible pho- tons/area/time): The upper scale is for chip illumination, Fig. 2:Event motion blur from a white bar moving on a black background (From [12]) mance. Moreover, none of the existing event simulators [12,15,39] model the full non-linear behaviour of the pixel bandwidth under arbitrary lighting conditions. They at most model the 1st-order behavior exhibited under extreme low light. Contributions. We propose Deblure-NeRF, a novel method to directly and ef- fectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed motion or low-light conditions. Specifically, we introduce a physically-accurate pixel bandwidth model to ac- count for event motion blur under arbitrary speed & lighting conditions. We also present a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S14",
      "paper_id": "arxiv:2409.17988v1",
      "section": "introduction",
      "text": "discrete-time variant of the model, a numerical solution to its transient response & an importance sampling strategy, to enable its computational imple- mentation. We incorporate them as part of the event generation model to recon- struct blur-minimal NeRFs viaAnalysis-by-Synthesis [16], which also supports the joint optimization of unknown pixel bandwidth model parameters. We also introduce a novelthreshold-normalized total variation lossto better regularize large textureless patches in the scene. Ambiguities in the reconstruction are re- solved by performing the proposedtranslated-gamma correction, which takes the pixel bandwidth model into consideration. Experiments on new event sequences, simulated with an improved ESIM [28,39] using our pixel bandwidth model, & realsequencesfromEDS[11],clearlyvalidatetheeffectivenessofDeblur e-NeRF. Our code, event simulator & synthetic event dataset are open-sourced.",
      "page_hint": null,
      "token_count": 117,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S15",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "Image Motion Deblurring.Image motion blur can be simply modeled as an average of incident light intensity over the exposure time of the image [37], which is a unity-gainLinear Time-Invariant(LTI) Low-Pass Filter (LPF). Thus, the severity of image motion blur is invariant to lighting, unlike event motion blur. This model may be used in anAnalysis-by-Synthesis framework [16] to deblur images [35]. However, a simplified model of spatially filtering images with a time- varyingblurkernel[37]ismorecommonlyusedforthistask[2,19,20,36,42,50,51]. With the rise of deep learning, state-of-the-art image/video deblurring methods [43,44,47,53,54] are generally deep image-to-image translation networks. Despite the rich literature on image motion deblurring, there are no known methods for effective event motion deblurring. While concurrent work [52] pro- 4 W. F. Low and G. H. Lee",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S16",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "posed to correct event timestamps for motion blur-induced latency (Sec. 1), the method cannot handle any blur-induced \u201closs or introduction of events\u201d and does not generalize to different cameras and bias settings. Similar generalization issues also plague concurrent work [27] on night-time events-to-video reconstruction. NeRF from Motion-Blurred Images. A na\u00efve way to reconstruct a blur- minimal Neural Radiance Field(NeRF) [32] from motion-blurred images is to first deblur them using an existing method. Recent works have shown superior performance by integrating either the full [22,46] or simplified [21,29] image motion blur model into an Analysis-by-Synthesis framework, where both NeRF and blur model parameters are jointly optimized, similar to our work. However, there are no known works on NeRF reconstruction from motion-blurred",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S17",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "events. 3 Preliminaries: Robust e-NeRF Robust e-NeRF [28] is the state-of-the-art for reconstructing NeRFs with event cameras, particularly from temporally sparse and noisy events generated under non-uniform motion. The method consists of 2 key components: a realistic event generation model and a pair of normalized reconstruction losses. After training, NeRF renders aregamma correctedto resolve ambiguities in the reconstruction. Event Generation Model.An Event, denoted ase = (u, p, tprev, tcurr ), with polarity p \u2208 {\u22121, +1} is generated at timestamptcurr when the change in in- cident log-radiancelog L at a pixelu, measured relative to a reference value at timestamp tref , shares the same sign asp and possesses a magnitude given by the Contrast Threshold associated to polarity p, Cp.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S18",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "Following the generation of an event, the pixel will be momentarily deactivated for an amount of time determined by theRefractory Period \u03c4 and then reset at the end. This event generation model, as illustrated in Fig. 3, can be succinctly described by: \u2206 log L := log L(u, tcurr ) \u2212 log L(u.tref ) = pCp , where tref = tprev + \u03c4 . (1) Training. To reconstruct a NeRF from anEvent Stream E = {e}, provided by a calibrated event camera with known trajectory, a batch of eventsEbatch is sampled randomly fromE for optimization of the following total training loss: L = 1 |Ebatch| X e\u2208Ebatch \u03bbdiff \u2113diff (e) + \u03bbgrad\u2113grad(e) . (2) The threshold-normalized difference loss\u2113diff with weight\u03bbdiff",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S19",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "acts as the main reconstruction loss. It enforces themean contrast threshold\u00afC = 1 2 (C\u22121 + C+1) normalized squared consistency between the predicted log-radiance difference \u2206 log \u02c6L := log \u02c6L(u, tcurr ) \u2212 log \u02c6L(u, tref ), given by NeRF renders, and the ob- served log-radiance difference\u2206 log L = pCp from an event (Eq. (1)), as follows: Deblur e-NeRF 5 \u2113diff (e) =",
      "page_hint": null,
      "token_count": 65,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S20",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "\u2206 log \u02c6L \u2212 pCp \u00afC !2 . (3) The target-normalized gradient loss\u2113grad with weight \u03bbgrad acts as a smooth- nessconstraintforregularizationoftexturelessregions.Itrepresentsthe Absolute Percentage ErrorAPE (\u02c6y, y) = |\u02c6y\u2212y/y| between the predicted log-radiance gradi- ent \u2202 \u2202t log \u02c6L(u, t), computed using auto-differentiation, and the finite difference approximation of the target log-radiance gradient\u2202 \u2202t log L(u, t) \u2248 pCp tcurr \u2212tref , at a timestamp tsam sampled betweentref and tcurr , as follows: \u2113grad(e) = APE \u0012 \u2202 \u2202t log \u02c6L(u, tsam), pCp tcurr \u2212 tref \u0013 . (4) Gamma Correction. Since event cameras mainly provide observations of changes in log-radiance, not absolute log-radiance, the predicted log-radiance log \u02c6L fromthereconstructedNeRFisonlyaccurateuptoanoffsetpercolorchan- nel. There will be an additional channel-consistent scale ambiguity, when only",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S21",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "the Contrast Threshold RatioC+1/C\u22121 is known during reconstruction. Nonethe- less, these ambiguities can be resolved post-reconstruction, given a set of refer- ence images. Specifically,ordinary least squarescan be used to perform anaffine correction onlog \u02c6L, or equivalently agamma correction on \u02c6L, as follows: log \u02c6Lcorr = a \u2299 log \u02c6L + b , (5) where a and b are the correction parameters. 4 Our Method We first introduce the physically-accurate pixel bandwidth model proposed to model the motion blur of events (Sec. 4.1), which extends the event genera- tion model of Robuste-NeRF (Sec. 3) . Subsequently, we detail how to synthe- size motion-blurred (effective) log-radiance incident at a pixel (Sec. 4.2), thus event motion blur, for optimization of a blur-minimal NeRF",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S22",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "from motion-blurred events (Sec. 4.3). Lastly, we present an enhanced variant of gamma correction (Sec. 3) that takes the pixel bandwidth model into consideration (Sec. 4.4). 4.1 Pixel Bandwidth Model Event motion blur is attributed to the limited bandwidth of the sensor pixel ana- log circuit, which also bounds the minimum event detection latency, maximum frequency of change detectable and hence maximum event generation rate. Event Pixel Circuit. Fig. 4 shows the core analog circuit of a typical event sensor pixel (in particular, that of the DVS128 [25]), which consists of 4 stages. 6 W. F. Low and G. H. Lee \ud835\udc61!\"# \ud835\udc61\ud835\udc5d=+1\ud835\udc61$!\"%\ud835\udc61&'!! \ud835\udc36()\ud835\udc36*)\ud835\udc86=\ud835\udc96,\ud835\udc5d,\ud835\udc61$!\"%,\ud835\udc61&'!! log\ud835\udc3f(\ud835\udc96,\ud835\udc61) \ud835\udf0f Fig. 3: Robust e-NeRF event generation model [28] IEEE TRANSACTIONS ON ELECTRON DEVICES, VOL.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S23",
      "paper_id": "arxiv:2409.17988v1",
      "section": "related_work",
      "text": "64, NO. 8, AUGUST 2017 3239 Temperature and Parasitic Photocurrent Effects in Dynamic Vision Sensors Yuji Nozaki and Tobi Delbruck, Fellow, IEEE",
      "page_hint": null,
      "token_count": 22,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S24",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "tocurrent on event-based dynamic vision sensors (DVS) is important because of their application in uncontrolled robotic, automotive, and surveillance applications. This paper considers the temperature dependence of DVS threshold temporal contrast (TC), dark current, and back- ground activity caused by junction leakage. New theory shows that if bias currents have a constant ratio, then ideally the DVS threshold TC is temperature independent, but the presence of temperature dependent junction leakage currents causes nonideal behavior at elevated tempera- ture. Both measured photodiode dark current and leakage induced event activity follow Arhenius activation.This paper also de\ufb01nes a new metric for parasitic photocurrent quan- tum ef\ufb01ciency and measures the sensitivity of DVS pixels to parasitic photocurrent. Index Terms \u2014 CMOS image sensors, dark",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S25",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "current, junc- tion leakage, photocurrent, vision sensor. I. I NTRODUCTION D YNAMIC vision sensors (DVS) and related sensors output asynchronous temporal contrast (TC) address events that signal local pixel-level brightness change [1]\u2013[9]. Because DVS have sparse, qui ck, and high dynamic range output, they can overcome the limited dynamic range and latency-power tradeoff of frame-based cameras, and are being developed for applications in surveillance, robotics, and sci- enti\ufb01c imaging [11], [12]. So far, there has been no study of temperature dependence of DVS sensor variants. Because of the applications of DVS in uncontrolled environments, the main purpose of this paper is to model and measure the effect of temperature on DVS. In addi- tion, unintended photocurrent in MOS transistor source/drain",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S26",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "junctions (parasitic photocurrent) causes event activity in the presence of strong dc lighting. Th is effect is closely related to junction leakage current temp erature dependent effects. Manuscript received April 4, 2017; revised May 12, 2017; accepted June 9, 2017. Date of publication June 29, 2017; date of current version July 21, 2017. This work was supported in part by the European projects VISUALISE under Grant FP7-ICT -600954, in part by SEEBETTER under Grant FP7-ICT -270324, and in part by the Tokyo Tech Exchange Pro- gram of the Tokyo Institute of Technology. The review of this paper was arranged by Editor A. Lahav. (Corresponding author: Tobi Delbruck.) Y. N o z a k i wa s w i t h",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S27",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "t h e I n s t i t u t e o f N e u r o i n fo r m a t i c s, C H - 8 0 5 7 Z \u00fc r i c h , Switzerland, and inilabs GmbH, CH-8057 Z\u00fcrich, Switzerland. He is now with the Tokyo Institute of Technology, Kanagawa 226-8503, Japan (e-mail: nozaxi0327@gmail.com). T. Delbruck is with the Institute of Neuroinfor matics, University of Zurich, CH-8057 ETH Z\u00fcrich, Switzerland, and also with inilabs GmbH, CH-8057 Z\u00fcrich, Switzerland (e-mail: tobi@ini.uzh.ch). Color versions of one or more of the \ufb01gures in this paper are available online at http://ieeexplore.ieee.org. Digital Object Identi\ufb01er 10.1109/TED.2017.2717848 Fig. 1. DVS pixel and operation. (A) \u2013 (E)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S28",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "Analog part of original DVS pixel circuit. The digital circuits that communicate with the peripheral AER readout circuits are not shown; (F) Principle of operation. For the DAVIS240C, C 1 = 130 fF , and C 2 = 6f F( C 1 / C 2 = 22 ). For the DVS128, C 1 = 467 fF and C 2 = 24 fF ( C 1 / C 2 = 20 ). (Adapted from [1]). This paper is organized as follows. Section II reviews DVS pixel circuit operation. Section III models the effects of temperature and parasitic photocurrent on the DVS pixel. Section IV compares measurem ents with theory. Section V concludes the paper. II. DVS P IXEL C IRCUIT The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S29",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "analog part of the original DVS pixel circuit ( Fig. 1 ) consists of six stages: part A is a continuous-time photore- ceptor circuit that transduces from a photocurrent (plus dark current) I p + I dark to produce a voltage V p that logarithmically increases with light intensity. Part B is a source follower buffer that isolates the photoreceptor from the next stage. Part C is as w i t c h e d - c a p a c i t o rd i f f e r e n c i n ga m p l i \ufb01 e rt h a ta m p l i \ufb01 e st h e change in log intensity from",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S30",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "the value memorized after the last event was sent. Part D are the two voltage comparators that detect increases or decreas es in log intensity that exceed threshold values. Part E generates the reset pulse, including ar e f r a c t o r yp e r i o d ,w h e nt h ep i x e lr e c e i v e sr o wa n dc o l u m n acknowledge signals RA and CA. Part F shows the principle of operation: Reset momentarily connects switch M r ,w h i c h balances circuit C and memorizes V sf across C 1 .I nr e s p o n s e to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S31",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "a change in the continuous-time logarithmic photoreceptor This work is licensed under a Creative Commons Attrib ution 3.0 License. For more information, see h ttp://creativecommons.o rg/licenses/by/3.0/ diffdiffI= Fig. 4:Core analog circuit of a typical event sensor pixel (Adapted from [34]) \ud835\udc3f!\"# \ud835\udc3f$%&' log++ \ud835\udc3f log\ud835\udc3f 1st-orderLTI LPF2nd-order NLTI LPFlog\ud835\udc3f( 1st-orderLTI LPFlog\ud835\udc3f!) log\ud835\udc3f$\")) Logarithmic PhotoreceptorSource Follower BufferDifferencing Amplifier Fig. 5:Overview of the proposed pixel bandwidth model The Logarithmic Photoreceptor(Stage A) contains aPhotodiode (PD) that transduces radiance (more accurately, irradiance) incident at the pixel toSignal Photocurrent Ip proportionally, and an active feedback loop that outputs a volt- ageVp proportionaltothelogarithmofthe PhotocurrentI = Ip+Idark [10,25,34]. A smallDark CurrentIdark flows through the photodiode, even in the dark. Next, Vp is buffered with aSource Follower(Stage B)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S32",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "to isolate the sensitive photoreceptor from rapid transients in subsequent stages [25,34]. TheDifferenc- ing Amplifier(StageC)thenamplifiesthechangeinsourcefollowerbufferoutput Vsf from the reset/reference voltage level, & outputs a voltageVdiff to be com- pared with both ON & OFF thresholds for event detection (Stage D) [10,25,34]. When Vdiff exceeds either thresholds, an event is generated and the differencing amplifier is held in reset for a duration of the refractory period\u03c4 [10,25,34]. Model. The design of the pixel analog circuit entails that event cameras in fact respond to changes ineffective log-radiancelog L = log (Lsig + Ldark ) instead, where Lsig is the actual incident radiance signal andLdark is the black level. Similar to standard image sensors, the black level is defined as the dark current-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S33",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "equivalent incident radiance, which is exponentially sensitive to temperature [34] and effectively limits the dynamic range of the sensor [10]. More precisely, as the pixel bandwidth is mainly limited by the first 3 stages of the analog circuit [3,5,8,9,24,31], event cameras effectively measure changes in thelow-pass-filtered/motion-blurred effective log-radiancelog Lblur .Thisexplains the discussed motion blur of events and event sensor dynamic performance limit. We accurately model the band-limiting behavior of the pixel with a unity- gain 4th-order Non-Linear Time-Invariant (NLTI) Low-Pass Filter (LPF) in Deblur e-NeRF 7 state-space form, with inputu = log L, statex = [ \u2202 log Lp/\u2202t log Lp log Lsf log Ldiff ]\u22a4 and outputy = [ log Lsf log Ldiff ]\u22a4, as follows: \u02d9x(t) = A",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S34",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "(u(t)) x(t) + B (u(t)) u(t) y(t) = C x(t) , (6) where A(u) = \uf8ee \uf8ef\uf8ef\uf8f0 \u22122\u03b6(u)\u03c9n(u) \u2212\u03c92 n(u) 0 0 1 0 0 0 0 \u03c9c,sf \u2212\u03c9c,sf 0 0 0 \u03c9c,diff \u2212\u03c9c,diff \uf8f9 \uf8fa\uf8fa\uf8fb, B(u) = \uf8ee \uf8ef\uf8ef\uf8f0 \u03c92 n(u) 0 0 0 \uf8f9 \uf8fa\uf8fa\uf8fb, C = \u00140 0 1 0 0 0 0 1 \u0015 . Specifically, the pixel bandwidth model is formed by a cascade of: 1. A unity-gain 2nd-order NLTI LPF, with inputlog L, state[ \u2202 log Lp/\u2202t log Lp ]\u22a4 and output log Lp, that models the transient response of the logarithmic photoreceptor [3,5,8,9,24,25,31]. Similar to itsLinear Time Invariant(LTI) counterpart, this 2nd-order filter is characterized by itsDamping Ratio\u03b6 and Natural Angular Frequency\u03c9n.However,theyarenotconstants,butcomplex non-linear functions",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S35",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "of its input [3,5,24,25] (more details in the supplement). The bandwidth of this filter is mostly proportional to the exponential of its input exp u = L, which explains the susceptibility to motion blur under low-light. However, black levelLdark limits the minimum pixel bandwidth. 2. A unity-gain 1st-order LTI LPF, with inputlog Lp and state/outputlog Lsf , that models the transient response of the source follower buffer [9,24,31]. It is characterized by its constant bandwidth/Cutoff Angular Frequency\u03c9c,sf , thatisproportionaltothe source follower buffer bias currentIsf (Fig.4)[24]. 3. Another unity-gain 1st-order LTI LPF, with input log Lsf , state/output log Ldiff and cutoff angular frequency\u03c9c,diff > \u03c9c,sf [24], that models the transient response of the differencing amplifier [24,31]. as illustrated in Fig.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S36",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "5. We also model the steady-state behavior of the differencing amplifier reset mechanism as a reset of the amplifier LPF state/outputlog Ldiff to its input log Lsf , at the end of the refractory period (i.e. reference timestamptref ). These 2 models allow the motion-blurred effective log-radiancelog Lblur to be derived as: log Lblur (t) = log Ldiff (t) + logLdelta(tref ) e\u2212\u03c9c,diff (t\u2212tref ) , t\u2265 tref , (7) where log Ldelta = log Lsf \u2212 log Ldiff . 4.2 Synthesis of Motion-Blurred Effective Log-Radiance The pixel bandwidth model proposed in Sec. 4.1 provides a means to accurately synthesize motion-blurred effective log-radiance log Lblur , thus simulate event 8 W. F. Low and G. H. Lee \ud835\udc99\ud835\udc58+1=\ud835\udc34!\ud835\udc58 \ud835\udc99\ud835\udc58+\ud835\udc35!\ud835\udc58 \ud835\udc62\ud835\udc58+\ud835\udc35*!\ud835\udc58",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S37",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "\ud835\udc62\ud835\udc58+1\ud835\udc9a\ud835\udc58=\ud835\udc36! \ud835\udc99\ud835\udc58\ud835\udc9a\ud835\udc58\u2248.\ud835\udc980\ud835\udc56 \ud835\udc62\ud835\udc56\" #$\"! \ud835\udc99\u0307\ud835\udc61=\ud835\udc34\ud835\udc58 \ud835\udc99\ud835\udc61+\ud835\udc35\ud835\udc58 \ud835\udc62\ud835\udc61\ud835\udc9a\ud835\udc61=\ud835\udc36 \ud835\udc99\ud835\udc61 Solve \ud835\udc99\u0307\ud835\udc61=\ud835\udc34\ud835\udc61 \ud835\udc99\ud835\udc61+\ud835\udc35\ud835\udc61 \ud835\udc62\ud835\udc61\ud835\udc9a\ud835\udc61=\ud835\udc36 \ud835\udc99\ud835\udc61 LinearizeContinuous-Time NLTI SystemContinuous-Time LTV System Discrete-Time LTV SystemDiscrete-Time LTV Solution Discretize Fig. 6:Overview of deriving the numerical solution of the pixel bandwidth model motion blur, given the pixel-incident log-radiance signallog Lsig. However, the continuous-time and non-linear nature of the model (Eq. (6)) prohibits its direct computational implementation, for use in a simulator or for NeRF reconstruction usinganAnalysis-by-Synthesisframework(Sec.4.3).Adiscrete-timecounterpart of the model that operates on discrete-time input samples is necessary (Fig. 6). Discrete-Time Model. Assume the discrete-time sequence of inputsu[k] = log L[k] is sampled at timestampstk, where the time intervals between succes- sive samples\u03b4tk = tk+1 \u2212tk may possibly be irregular. We derive a discrete-time model from the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S38",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "continuous-time, non-linear pixel bandwidth model by first lin- earizing [6] the 4th-order NLTI LPF (Eq. (6)) at different steady-state operating points (\u00afx[k], \u00afu[k]) = \u0010 [ 0 u[k+1] u[k+1] u[k+1] ]\u22a4, u[k + 1] \u0011 , for each time interval (tk, tk+1]. Then, we discretize the linearized model assumingFirst-Order Hold (FOH) [6] (i.e. piecewise-linear) inputsu. This yields a discrete-time 4th-order Linear Time-Varying(LTV) LPF, in non-standard state-space form, as follows: x[k + 1] = Ad[k] x[k] + Bd[k] u[k] + eBd[k] u[k + 1] y[k] = Cd x[k] , (8) where Ad[k] = \u03a6[k], Bd[k] = \u03931[k] \u2212 \u03932[k], eBd[k] = \u03932[k], Cd = C and: \uf8ee \uf8f0 \u03a6[k] \u03931[k] \u03932[k] 0 I I 0 0 I \uf8f9 \uf8fb = exp",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S39",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "\uf8eb \uf8ed \uf8ee \uf8f0 A (u[k + 1])\u03b4tk B (u[k + 1])\u03b4tk 0 0 0 I 0 0 0 \uf8f9 \uf8fb \uf8f6 \uf8f8 . Numerical Solution. The discrete-time model presented can be directly inte- grated in an existing event simulator,e.g. ESIM [39] & its improved variant in- troducedinRobust e-NeRF,tosynthesize log Lblur &thussimulateeventmotion blur (more details in the supplement), assuming some appropriate initial state x[k0], e.g. the steady-state on the initial inputu[k0]. However, this cannot be done when the appropriatex[k0] is not well defined for an arbitrarylog Lblur [k] of interest,e.g. during NeRF reconstruction. We tackle this issue with the (nu- merical) solution to the transient response of the discrete-time model below: Deblur e-NeRF 9 y[k] = Cd \" \u03c6(k0, k)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S40",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "x[k0] + k\u22121X i=k0 \u03c6(i + 1, k) \u0010 Bd[i] u[i] + eBd[i] u[i + 1] \u0011# , (9) where thestate transition matrix\u03c6(m, n) = Qn\u2212m j=1 Ad[n \u2212 j]. As the linearized, thus discretized, model isasymptotically stable, the mag- nitude of eigenvalues ofAd[k], for all k, must be smaller than 1. This entails that limk\u2212k0\u2192\u221e \u03c6(k0, k) = 0. Thus, for a sufficiently long numerical integration time interval(tk0 , tk], y[k] can be approximated as thezero-state responseof the model, which is just a weighted sum of past & present inputsu between k0 & k: y[k] \u2248 kX i=k0 w[i] u[i] \u2248 kX i=k0 \u02c6w[i] u[i] , (10) where w[i] = \uf8f1 \uf8f4\uf8f4\uf8f2 \uf8f4\uf8f4\uf8f3 Cd\u03c6(k0 + 1, k)Bd[k0] , if",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S41",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "i = k0 Cd \u0010 \u03c6(i + 1, k)Bd[i] + \u03c6(i, k) eBd[i \u2212 1] \u0011 , if i = k0 + 1, . . . , k\u2212 1 Cd eBd[k \u2212 1] , if i = k \u02c6w[i] = w[i] \u2298 kX j=k0 w[j] , and\u2298 denotesHadamard/element-wisedivision.Itcanbeshownthat limk\u2212k0\u2192\u221ePk i=k0 w[i] = 1. Thus, we use thesum-normalized weights \u02c6w[i] in practice, as they are corrected for the bias due to a finite integration interval(tk0 , tk]. In general, these weights tend to be larger for larger input (i.e. higher effective log-radiance) samples with timestamps closer to the output timestamptk. Importance Sampling. Often times, we are interested in computinglog Lblur at some desired timestamptk, given only a function for sampling inputsu",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S42",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "= log L and a fixed sampling budget. To this end, we propose to infer the optimal input sample timestamps, represented by the random variableTi \u2208 (\u2212\u221e, tk], by sampling them from the followingtransformed exponential distribution: Ti \u223c Exp (tk \u2212 ti; \u03c9c,dom,min) = \u03c9c,dom,min e\u2212\u03c9c,dom,min (tk\u2212ti) , (11) where \u03c9c,dom,min is the minimum possible dominant cutoff angular frequency of the pixel, achieved under extreme low-light whenL = Ldark . The suggested proposal distribution coarsely approximates the distribution represented by \u02c6w[i] over the interval(tk0 , tk], as it corresponds to the weight function, derived from the zero-state response of the continuous-time dominant pole-approximatedmodelunderextremelow-light.Thus,itgenerallyconcentrate samples at relevant parts of the input (i.e. with large weight), achieving a similar goal asimportance sampling.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S43",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "It works best under low-light, when event cameras aremost susceptibleto motionblur.More detailsareavailablein thesupplement. 10 W. F. Low and G. H. Lee 4.3 Training We employ the same training procedure used in Robuste-NeRF (Sec. 3) to optimize a blur-minimal NeRF from motion-blurred events in an Analysis-by- Synthesis manner, with a few exceptions. Specifically, we apply our training losses on the predicted motion-blurred effective log-radiancelog \u02c6Lblur , which was synthesized from NeRF renders\u02c6L using the proposed numerical solution to the pixel bandwidth model (Eqs. (7) and (10)) with importance sampling (Eq. (11)). Moreover,weadopta Huber-norm(\u03b4 = 1)variantofthethreshold-normalized difference loss\u2113diff (Eq. (3)), which is less sensitive to outliers, with weight\u03bbdiff . We also propose thethreshold-normalized total variation loss\u2113tv , as a replace- ment for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S44",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "the target-normalized gradient loss\u2113grad (Eq. (4)), with weight\u03bbtv . Fundamental Limitations. When the black levelLdark is unknown, the inci- dent radiance signalLsig and Ldark cannot be unambiguously disentangled from just the observation of effective radianceL = Lsig +Ldark given by events. Thus, we can only reconstruct a NeRF with volume renders\u02c6L that represent predicted effective radiance, not predicted incident radiance signal as suggested by Robust e-NeRF. This is enabled by the assumption that the temperature of the event camera remains effectively constant over the entire duration of the given event steam, so that the dark current & thus black level remains effectively stationary. Furthermore, as the pixel bandwidth depends on theabsolute effective radi- ance L, the predicted effective radiance\u02c6L is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S45",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "theoretically gamma-accurate (i.e. gamma correction of\u02c6L is unnecessary), assuming known pixel bandwidth model parameters, contrary to what is suggested by Robuste-NeRF. However, in prac- tice, \u02c6L is generally only gamma-accurate if theL associated to the events has a significant impact on the pixel transient response. In other words, the gamma- accuracy of\u02c6L greatly depends on the severity of event motion blur, hence camera speed, scene illumination, scene texture complexity, camera used and its settings. Threshold-Normalized Total Variation Loss.This loss penalizes themean contrast threshold\u00afC = 1 2 (C\u22121 +C+1) normalized total variationof the predicted motion-blurred effective log-radiancelog \u02c6Lblur , on a subinterval(tstart, tend] sam- pled between the interval(tref , tcurr ] given by an event, as follows: \u2113tv (e) =",
      "page_hint": null,
      "token_count": 119,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S46",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "\u03b4 log \u02c6Lblur \u00afC",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S47",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": ", (12) where \u03b4 log \u02c6Lblur := log \u02c6Lblur (u, tend) \u2212 log \u02c6Lblur (u, tstart). Similar to\u2113grad (Eq. (4)), this loss acts as a smoothness constraint for reg- ularization of textureless regions in the scene. However, it imposes a stronger bias to enforce the uniformity oflog \u02c6Lblur between event intervals, which greatly helps with reconstructing large uniform patches. It can also effectively generalize to arbitrary threshold values due to the normalization, similar to\u2113diff (Eq. (3)). JointOptimizationofPixelBandwidthModelParameters. Ourmethod does not strictly rely on thepixel bandwidth model parameters\u2126 to be known Deblur e-NeRF 11 as a priori, as it generally supports their joint optimization withNeRF param- eters \u0398. However, their prior knowledge generally facilitates a more accurate reconstruction. The parameters\u2126, which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S48",
      "paper_id": "arxiv:2409.17988v1",
      "section": "abstract",
      "text": "include the parameters of non-linear damping ratio function\u03b6 & natural angular frequency function\u03c9n, and angular cutoff frequencies\u03c9c,sf and \u03c9c,diff (Eq. (6)), depend on the pixel circuit design, semiconductor manufacturing process & user-defined event camera bias settings. 4.4 Translated-Gamma Correction To eliminate the unknown black level offset and resolve potential gamma-inacc- uracies in the predicted effective radiance\u02c6L, we propose aTranslated-Gamma Correction on \u02c6L post-reconstruction, using a set of reference images, as follows: \u02c6Lsig,corr = b \u2299 \u02c6La \u2212 c , (13) where a, b and c are the correction parameters, viaLevenberg-Marquardtnon- linear least squares optimization. The translation/offset correction is done inde- pendently per color channel to account for channel-varying spectral sensitivities.",
      "page_hint": null,
      "token_count": 110,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S49",
      "paper_id": "arxiv:2409.17988v1",
      "section": "experiments",
      "text": "We conduct a series of novel view synthesis experiments, both on synthetic (Sec. 5.1) and real event sequences (Sec. 5.2), to verify that our method, De- blure-NeRF, can indeed directly and effectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions, using a physically-accurate pixel bandwidth model. Metrics. We adopt the commonly used PSNR, SSIM [48] and AlexNet-based LPIPS [55] to evaluate the similarity between the target and translated-gamma- corrected (Sec. 4.4) synthesized novel views, for all methods in each experiment. Baselines. We benchmark our method, Deblure-NeRF, against the state-of- the-art, Robust e-NeRF [28], and a na\u00efve baseline of E2VID [40] (a seminal events-to-video reconstruction method)+ NeRF [32] (as well as 2 otherimage blur-aware baselines in the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S50",
      "paper_id": "arxiv:2409.17988v1",
      "section": "experiments",
      "text": "supplement). The setup of anevent motion blur- aware baseline is hindered by the lack of relevant works. The implementation of allmethodsemployacommonNeRFbackbone[23]toallowforafaircomparison. Datasets. We perform the synthetic experiments on the default set of sequences released by Robuste-NeRF, and a novel set similarly simulated on the \u201cReal- istic Synthetic 360\u25e6\u201d scenes [32]. However, our sequences involve event motion blur due to fast camera motion and/or poor scene illumination. Such sequences were simulated with our proposed event simulator, which incorporates the pixel bandwidth model presented in Secs. 4.1 and 4.2 in the improved ESIM [39] event simulator introduced in Robuste-NeRF. Similar to the sequences in the Robust e-NeRF synthetic event dataset, the events are generated from a virtual event camera moving in a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S51",
      "paper_id": "arxiv:2409.17988v1",
      "section": "experiments",
      "text": "hemi-/spherical spiral motion about the object at the origin. 12 W. F. Low and G. H. Lee Fig. 7:Pixel bandwidth of DVS128 [25] with nominal biases Ontheotherhand,weconducttherealexperimentsonthe 08_peanuts_runn- ing and11_all_characters sequences of the EDS dataset [11], which are gener- ally 360\u25e6 captures of a table of objects in an office room under moderate lighting. These sequences were chosen as they involve some high-speed camera motion. 5.1 Synthetic Experiments The synthetic experiments serve as the main benchmark to assess all methods, as they enable controlled tests under diverse realistic conditions with precise ground truth, a task that would otherwise be infeasible using real sequences. As with the default sequences in Robuste-NeRF [28], we simulate ours with symmetric contrast thresholds of 0.25",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S52",
      "paper_id": "arxiv:2409.17988v1",
      "section": "experiments",
      "text": "(i.e. C\u22121 = C+1 = 0 .25), zero pixel- to-pixel threshold standard deviation and refractory period (i.e. \u03c3Cp = 0, \u03c4= 0), and provide camera poses at1 kHz. Moreover, the pixel bandwidth model parameters that we adopt correspond to that of the DVS128 [25] event camera with nominal biases (provided in jAER [4]). Fig. 7 illustrates the light-dependent behaviour of its pixel bandwidth, which is mostly proportional to the scene illuminance Esc, thus radiance incident at a pixelLsig. Unless otherwise stated, the virtual event camera revolves the object 4 times with uniform 2 revolution per second speed about the object vertical axis. All sequences are also simulated under a scene illuminance Esc of 1 000 lux by default, which corresponds",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S53",
      "paper_id": "arxiv:2409.17988v1",
      "section": "experiments",
      "text": "to standard office lighting [3,5]. Under such a condition, the pixel bandwidth spans around50\u20132500Hz,dependingontheincidentradiance.Duetolimitedresources, our method is only trained with1/8\u00d7 the batch size of our baselines, by default. Upper Bound Performance.To quantify the upper bound performance, we evaluate all methods on motion blur-free event sequences, which is effectively given by a pixel with infinite bandwidth. For this purpose, we remove the pixel bandwidth model from our method and train it with the same batch size as the Deblur e-NeRF 13 Table 1: Upper bound perfor- mance without event motion blur",
      "page_hint": null,
      "token_count": 90,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S54",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "E2VID + NeRF 19.49 0.847 0.268 Robust e-NeRF 28.48 0.944 0.054 Deblur e-NeRF 29.43 0.953 0.043 Table 2:Quantitative results of the real exps. 08_peanuts_running 11_all_characters Method PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 E2VID + NeRF 14.85 0.690 0.595 13.12 0.695 0.627 Robust e-NeRF 18.00 0.677 0.507 15.91 0.677 0.552 Deblur e-NeRF 18.27 0.695 0.503 16.53 0.710 0.511 Table 3:Effect of camera speed.\u2020Trained with1/8\u00d7 the batch size of baselines. v = 0.125\u00d7 v = 1\u00d7 v = 4\u00d7 Method PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 E2VID + NeRF 18.58 0.849 0.259 18.85 0.839 0.278 17.82 0.804 0.328 Robust e-NeRF 28.31 0.943 0.050 26.11",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S55",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "0.924 0.074 22.18 0.861 0.122 Deblur e-NeRF\u2020 28.71 0.948 0.048 28.41 0.947 0.049 27.48 0.939 0.061 Table 4:Effect of scene illuminance.\u2020Trained with1/8\u00d7 the batch size of baselines. Esc = 100 000lux Esc = 1 000lux Esc = 10lux Method PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 E2VID + NeRF 19.27 0.846 0.268 18.85 0.839 0.278 17.24 0.804 0.354 Robust e-NeRF 27.62 0.942 0.055 26.11 0.924 0.074 22.72 0.870 0.129 Deblur e-NeRF\u2020 28.73 0.948 0.047 28.41 0.947 0.049 28.62 0.935 0.059 Table 5:Collective effect of camera speed and scene illuminance.\u2020Trained with1/8\u00d7 the batch size of baselines. v = 0.125\u00d7, Esc = 100 000lux v = 1\u00d7, Esc = 1 000lux",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S56",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "v = 4\u00d7, Esc = 10lux",
      "page_hint": null,
      "token_count": 6,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S57",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "Cp & \u03c4 Opt. \u2126 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 E2VID + NeRF \u2212 \u2212 19.19 0.844 0.281 18.85 0.839 0.278 15.37 0.799 0.436 \u00d7 \u2212 28.27 0.944 0.057 26.11 0.924 0.074 18.42 0.814 0.255Robust e-NeRF \u2713 \u2212 28.28 0.944 0.051 26.31 0.923 0.075 18.51 0.812 0.254 \u00d7 \u00d7 29.00 0.950 0.043 28.41 0.947 0.049 26.15 0.904 0.134Deblur e-NeRF\u2020 \u00d7 \u2713 28.19 0.943 0.046 26.07 0.930 0.067 25.59 0.896 0.156 baselines. The main difference between Robuste-NeRF and our method, under such a setting, is the replacement of the target-normalized gradient loss\u2113grad with the threshold-normalized total variation loss\u2113tv . Thus, the quantitative results reported in Tab. 1 verifies",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S58",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "the effectiveness of our proposed\u2113tv over\u2113grad. Moreover, the qualitative results shown in Fig. 8, particularly at the back of the chair, clearly shows the strength of\u2113tv in regularizing large textureless patches. Effect of Camera Speed.To investigate the effect of event motion blur due to high-speed camera motion, we evaluate all methods on 3 sets of sequences simulated with camera speedsv that are0.125\u00d7, 1\u00d7 & 4\u00d7 of the default setting, respectively. As event motion blur may lead to a significant \u201closs or introduction of events\u201d (Sec. 1), we also quantify the average number of events relative to 14 W. F. Low and G. H. Lee that of its corresponding blur-free sequence, across all sequences in the set. This translates to93.36%, 100.95%",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S59",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "& 95.21% for v = 0.125\u00d7, 1\u00d7 & 4\u00d7, respectively. The quantitative results in Tab. 3 clearly underscores the significance of in- corporating a pixel bandwidth model, as our method significantly outperforms all baselines, especially under high-speed motion, despite being trained with 1/8\u00d7 the batch size of baselines. The results also display our robustness to event motion blur, as our performance remains relatively unperturbed under varying camera speeds, & remains close to our upper bound performance. Qualitative re- sults in Fig. 8 further validate our effectiveness in reconstructing a blur-minimal NeRF, as our method is free from artifacts such as floaters and double edges. Effect of Scene Illuminance. To assess the effect of event motion blur due to poor scene",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S60",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "illumination, we benchmark all methods on 3 sets of sequences simulatedwithsceneilluminance Esc of100 000,1 000 &10lux,whichcorrespond to sunlight, office light & street light, respectively [3]. Furthermore, the pixel bandwidth spans around 1200\u20134000Hz, 50\u20132500Hz & 20\u2013170Hz, respectively, depending on the incident radiance. They also have82.49%, 100.95% & 17.22% the number of events of the blur-free set, respectively. The quantitative results given in Tab. 4 undoubtedly verifies the importance of modeling event motion blur, as our method once again outperform other works, while remaining close to the upper bound performance. It also reveals our astonishing robustness to blur-induced \u201closs of events\u201d, particularly underEsc = 10lux. Collective Effect. To assess the effect of event motion blur due to both high speed and low",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S61",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "light, which resemble challenging real-world conditions, we bench- mark all methods on 3 sets of sequences with different difficulty levels:easy (v = 0.125\u00d7, Esc = 100 000 lux), medium (v = 1\u00d7, Esc = 1 000 lux) and hard (v = 4\u00d7, Esc = 10lux), which have93.36%, 100.95% and 7.14% the number of events of the blur-free set, respectively. We also evaluate Robuste-NeRF with jointly optimized contrast thresholdsCp & refractory period\u03c4 to validate the im- portance of modeling event motion blur. We benchmark our method with jointly optimized pixel bandwidth model parameters\u2126, poorly initialized to4\u00d7 their true value, to assess the robustness of joint optimization, thus auto-calibration. The quantitative and qualitative results given in Tab. 5 and Fig. 8, respec-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S62",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "tively, generally reflect the results of the 2 previous experiments, thus a similar conclusion can be drawn. In addition,Cp & \u03c4 clearly cannot compensate for the lackofapixelbandwidthmodel,astheirjointoptimizationhasvirtuallynoeffect on Robuste-NeRF. The quantitative results also reveal the feasibility of jointly optimizing \u2126, especially under severe motion blur. While our performance de- teriorates as the difficulty increases, which suggests a limit to our robustness to event motion blur, we show in the supplement that increasing our batch size to that of the baselines significantly improves our performance, hence robustness. 5.2 Real Experiments To account for unknown event camera intrinsic parameters, we train Robuste- NeRF and our method with jointly optimized contrast thresholds and refractory Deblur e-NeRF 15 E2VID +NeRFRobust e-NeRFDeblur e-NeRF(Ours)Target \ud835\udc63=4\u00d7,\ud835\udc38!\"=10\ud835\udc59\ud835\udc62\ud835\udc65Upper Bound",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S63",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "Performance08_peanuts_running11_all_characters\ud835\udc63=4\u00d7 Fig. 8:Qualitative results of synthetic & real experiments, w/o jointly optimized con- trast thresholds, refractory period and pixel bandwidth model parameters, if applicable period. We also train our method with jointly optimized pixel bandwidth model parameters,initializedfromDVS128[25]fastbiases(providedinjAER[4]),using the same batch size as the baselines. While the 2 sequences only occasionally in- volve fast camera motion under moderate office lighting, the quantitative results reported in Tab. 2 demonstrate our superior performance. This is also supported qualitatively in Fig. 8, where the table & walls are visibly more uniform, and less blooming artifacts are observed around objects, compared to Robust e-NeRF.",
      "page_hint": null,
      "token_count": 99,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S64",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "In this paper, we introduce Deblure-NeRF, a novel method to directly and ef- fectively reconstruct blur-minimal NeRFs from motion-blurred events, generated under high-speed or low-light conditions. The core component of this work is a physically-accurate pixel bandwidth model that accounts for event motion blur. We also propose a threshold-normalized total variation loss to better regular- ize large textureless patches. Despite its accomplishments, Deblure-NeRF still inherits the limitations of Robuste-NeRF and other works,e.g. assumption of known camera trajectory. Moreover, since the synthesis of motion-blurred effec- tivelog-radianceatagiventimestamprequiresmultiplepastandpresentsamples of effective log-radiance, given by NeRF renders, our reconstruction also incurs 16 W. F. Low and G. H. Lee a higher computational and memory cost. Joint optimization of pixel bandwidth model parameters is also sometimes",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S65",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "unstable. We leave these as future work. Acknowledgements. This research / project is supported by the National Research Foundation, Singapore, under its NRF-Investigatorship Programme (Award ID. NRF-NRFI09-0008), and the Tier 1 grant T1-251RES2305 from the Singapore Ministry of Education. References 1. Cannici, M., Scaramuzza, D.: Mitigating motion blur in neural radiance fields with events and frames. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 2. Cho, S., Lee, S.: Fast motion deblurring. ACM Trans. Graph. (2009) 3. Delbruck, T., Mead, C.A.: Analog VLSI Phototransduction by continuous-time, adaptive, logarithmic photoreceptor circuits. Vision Chips: Implementing vision algorithms with analog VLSI circuits (1995) 4. Delbruck, T.: Frame-free dynamic digital vision. In: Proceedings of Intl. Symp. on Secure-Life",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S66",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Electronics, Advanced Electronics for Quality Life and Society (2008) 5. Delbruck, T.: Investigations of analog VLSI visual transduction and motion pro- cessing. Doctoral Thesis, California Institute of Technology (1993) 6. Franklin, G., Powell, J., Workman, M.: Digital Control of Dynamic Systems. Addison-Wesley series in electrical and computer engineering: Control engineer- ing, Addison-Wesley (1998) 7. Gallego, G., Delbr\u00fcck, T., Orchard, G., Bartolozzi, C., Taba, B., Censi, A., Leutenegger, S., Davison, A.J., Conradt, J., Daniilidis, K., Scaramuzza, D.: Event- based vision: A survey. IEEE Transactions on Pattern Analysis and Machine In- telligence (2020) 8. Graca, R., Delbruck, T.: Unraveling the paradox of intensity-dependent dvs pixel noise. In: International Image Sensor Workshop (IISW) (2021) 9. Graca, R., McReynolds, B., Delbruck, T.: Optimal biasing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S67",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "and physical limits of dvs event noise. In: International Image Sensor Workshop (IISW) (2023) 10. Gra\u00e7a, R., McReynolds, B., Delbruck, T.: Shining light on the dvs pixel: A tuto- rial and discussion about biasing and optimization. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2023) 11. Hidalgo-Carri\u00f3, J., Gallego, G., Scaramuzza, D.: Event-Aided Direct Sparse Odom- etry. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR) (2022) 12. Hu, Y., Liu, S.C., Delbruck, T.: v2e: From Video Frames to Realistic DVS Events. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops (2021) 13. Hwang, I., Kim, J., Kim, Y.M.: Ev-nerf: Event based neural radiance field. In: IEEE/CVF",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S68",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Winter Conference on Applications of Computer Vision (WACV) (2023) 14. Jiang, Z., Zhang, Y., Zou, D., Ren, J., Lv, J., Liu, Y.: Learning event-based motion deblurring. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020) 15. Joubert, D., Marcireau, A., Ralph, N., Jolley, A., van Schaik, A., Cohen, G.: Event Camera Simulator Improvements via Characterized Parameters. Frontiers in Neu- roscience (2021) Deblur e-NeRF 17 16. Kato, H., Beker, D., Morariu, M., Ando, T., Matsuoka, T., Kehl, W., Gaidon, A.: Differentiable Rendering: A Survey (2020), arXiv:2006.12057 [cs] 17. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: 3rd In- ternational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S69",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Track Proceedings (2015) 18. Klenk, S., Koestler, L., Scaramuzza, D., Cremers, D.: E-nerf: Neural radiance fields from a moving event camera. IEEE Robotics and Automation Letters (2023) 19. Krishnan, D., Fergus, R.: Fast image deconvolution using hyper-laplacian priors. In: Proceedings of the 22nd International Conference on Neural Information Pro- cessing Systems (2009) 20. Krishnan, D., Tay, T., Fergus, R.: Blind deconvolution using a normalized sparsity measure. In: CVPR 2011 (2011) 21. Lee, D., Lee, M., Shin, C., Lee, S.: Dp-nerf: Deblurred neural radiance field with physical scene priors. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023) 22. Lee, D., Oh, J., Rim, J., Cho, S., Lee, K.: Exblurf: Efficient radiance fields for extreme motion blurred images.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S70",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "In: 2023 IEEE/CVF International Conference on Computer Vision (ICCV) (2023) 23. Li, R., Gao, H., Tancik, M., Kanazawa, A.: Nerfacc: Efficient sampling accelerates nerfs. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2023) 24. Lichtsteiner, P.: An AER temporal contrast vision sensor. Doctoral Thesis, ETH Zurich (2006) 25. Lichtsteiner, P., Posch, C., Delbruck, T.: A 128\u00d7 128 120 dB 15\u00b5s latency asyn- chronous temporal contrast vision sensor. IEEE Journal of Solid-State Circuits (2008) 26. Lin, S., Zhang, J., Pan, J., Jiang, Z., Zou, D., Wang, Y., Chen, J., Ren, J.: Learning event-drivenvideodeblurringandinterpolation.In:ComputerVision\u2013ECCV2020 (2020) 27. Liu, H., Peng, S., Zhu, L., Chang, Y., Zhou, H., Yan, L.: Seeing motion at nighttime with an event camera. In: Proceedings of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S71",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 28. Low, W.F., Lee, G.H.: Robust e-nerf: Nerf from sparse & noisy events under non- uniform motion. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2023) 29. Ma,L.,Li,X.,Liao,J.,Zhang,Q.,Wang,X.,Wang,J.,Sander,P.V.:Deblur-NeRF: Neural Radiance Fields From Blurry Images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 30. Ma, Q., Paudel, D.P., Chhatkuli, A., Van Gool, L.: Deformable neural radiance fields using rgb and event cameras. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2023) 31. McReynolds, B.J., Graca, R.P., Delbruck, T.: Experimental methods to predict dynamic vision sensor event camera performance. Optical Engineering (2022) 32. Mildenhall, B., Srinivasan, P.P., Tancik, M.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S72",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In: European Conference on Computer Vision (ECCV) 2020 (2020) 33. M\u00fcller, T., Evans, A., Schied, C., Keller, A.: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM Transactions on Graphics (2022) 34. Nozaki, Y., Delbruck, T.: Temperature and Parasitic Photocurrent Effects in Dy- namic Vision Sensors. IEEE Transactions on Electron Devices (2017) 18 W. F. Low and G. H. Lee 35. Park, H., Mu Lee, K.: Joint estimation of camera pose, depth, deblurring, and super-resolution from a blurred image sequence. In: Proceedings of the IEEE In- ternational Conference on Computer Vision (ICCV) (2017) 36. Perrone, D., Favaro, P.: Total variation blind deconvolution: The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S73",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "devil is in the details. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014) 37. Potmesil,M.,Chakravarty,I.:Modelingmotionblurincomputer-generatedimages. SIGGRAPH Comput. Graph. (1983) 38. Qi, Y., Zhu, L., Zhang, Y., Li, J.: E2nerf: Event enhanced neural radiance fields from blurry images. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2023) 39. Rebecq, H., Gehrig, D., Scaramuzza, D.: ESIM: an Open Event Camera Simulator. In: Proceedings of The 2nd Conference on Robot Learning, PMLR (2018) 40. Rebecq, H., Ranftl, R., Koltun, V., Scaramuzza, D.: High speed and high dynamic range video with an event camera. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021) 41. Rudnev, V., Elgharib, M., Theobalt, C., Golyanik, V.: Eventnerf: Neural radiance",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S74",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "fields from a single colour event camera (2022), arXiv:2206.11896 [cs] 42. Shan,Q.,Jia,J.,Agarwala,A.:High-qualitymotiondeblurringfromasingleimage. ACM Trans. Graph. (2008) 43. Son, H., Lee, J., Lee, J., Cho, S., Lee, S.: Recurrent video deblurring with blur- invariant motion estimation and pixel volumes. ACM Trans. Graph. (2021) 44. Tao, X., Gao, H., Shen, X., Wang, J., Jia, J.: Scale-recurrent network for deep im- age deblurring. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (2018) 45. Wang, C., Gao, D., Xu, K., Geng, J., Hu, Y., Qiu, Y., Li, B., Yang, F., Moon, B., Pandey, A., Aryan, Xu, J., Wu, T., He, H., Huang, D., Ren, Z., Zhao, S., Fu, T., Reddy, P., Lin, X., Wang, W., Shi, J., Talak, R., Cao, K., Du,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S75",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Y., Wang, H., Yu, H., Wang, S., Chen, S., Kashyap, A., Bandaru, R., Dantu, K., Wu, J., Xie, L., Carlone, L., Hutter, M., Scherer, S.: PyPose: A library for robot learning with physics-based optimization. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023) 46. Wang, P., Zhao, L., Ma, R., Liu, P.: Bad-nerf: Bundle adjusted deblur neural radiance fields. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2023) 47. Wang, Z., Cun, X., Bao, J., Zhou, W., Liu, J., Li, H.: Uformer: A general u-shaped transformer for image restoration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 48. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S76",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "from error visibility to structural similarity. IEEE Transactions on Image Processing (2004) 49. Xu, F., Yu, L., Wang, B., Yang, W., Xia, G.S., Jia, X., Qiao, Z., Liu, J.: Mo- tion deblurring with real events. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 2583\u20132592 (October 2021) 50. Xu, L., Jia, J.: Two-phase kernel estimation for robust motion deblurring. In: Pro- ceedings of the 11th European Conference on Computer Vision: Part I (2010) 51. Xu, L., Zheng, S., Jia, J.: Unnatural l0 sparse representation for natural image de- blurring. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2013) Deblur e-NeRF 19 52. Yang, Y., Liang, J., Yu, B., Chen, Y., Ren, J.S.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S77",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Shi, B.: Latency correction for event-guideddeblurringandframeinterpolation.In:ProceedingsoftheIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2024) 53. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.: Restormer: Efficient transformer for high-resolution image restoration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) 54. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H., Shao, L.: Multi-stage progressive image restoration. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) (2021) 55. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable ef- fectiveness of deep features as a perceptual metric. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 56. Zhang, X.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S78",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Yu, L.: Unifying motion deblurring and frame interpolation with events. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022) Supplementary Material for Deblur e-NeRF: NeRF from Motion-Blurred Events under High-speed or Low-light Conditions Weng Fei Low and Gim Hee Lee The NUS Graduate School\u2019s Integrative Sciences and Engineering Programme (ISEP) Institute of Data Science (IDS), National University of Singapore Department of Computer Science, National University of Singapore {wengfei.low, gimhee.lee}@comp.nus.edu.sg https://wengflow.github.io/deblur-e-nerf A Logarithmic Photoreceptor Model. As mentioned in Sec. 3.1, we model the radiance-dependent band- limiting behavior of the logarithmic photoreceptor with the following unity-gain 2nd-order Non-Linear Time-Invariant (NLTI) Low-Pass Filter (LPF) with input up = log L, statexp = [ \u2202 log Lp/\u2202t log Lp",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S79",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "]\u22a4 and outputyp = log Lp: \u02d9xp(t) = Ap (up(t)) xp(t) + Bp (up(t)) up(t) yp(t) = Cp xp(t) , (14) where Ap(u) = \u0014\u22122\u03b6(u)\u03c9n(u) \u2212\u03c92 n(u) 1 0 \u0015 , Bp(u) = \u0014\u03c92 n(u) 0 \u0015 , Cp = \u00020 1\u0003 . The derivation of this model follows closely that of the small signal model for the original adaptive variant of the logarithmic photoreceptor circuit [3,5], but we account for the absence of an adaptive element in the circuit. The radiance-dependent damping ratio\u03b6 and natural angular frequency\u03c9n are, respectively, given by: \u03b6(u) = \u03c4out + \u03c4in(u) + (Aamp + 1)\u03c4mil(u) 2 p \u03c4out (\u03c4in(u) + \u03c4mil(u)) (Aloop + 1) , (15) \u03c9n(u) = s Aloop + 1 \u03c4out (\u03c4in(u)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S80",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "+ \u03c4mil(u)) , (16) where Aamp and Aloop are the amplifier and total loop gains of the photoreceptor circuit, respectively, and\u03c4out is the time constant associated to the output node of the photoreceptor circuit and inversely proportional to thephotoreceptor bias current Ipr (Fig. 3). Furthermore,\u03c4in and \u03c4mil are, respectively, the radiance- dependent time constants associated to the input node andMiller capacitance of the photoreceptor circuit, given by: Deblur e-NeRF 21 \u03c4in(u) = CinVT \u03ba exp u = CinVT \u03baL , (17) \u03c4mil(u) = CmilVT \u03ba exp u = CmilVT \u03baL , (18) where Cin and Cmil are the (lumped) parasitic capacitance on the photodiode and Miller capacitance in the photoreceptor circuit, respectively,VT is the ther- mal voltage, and\u03ba is the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S81",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "signal photocurrentIp to incident radiance signalLsig ratio governed by the photodiode. Behavior under Extreme Low Light.As \u03c4out \u226a \u03c4in + \u03c4mil under extreme low light, the model described above reduces to a unity-gain 1st-order NLTI LPF with inputu\u02c6p = log L, statex\u02c6p = output y\u02c6p = log Lp: \u02d9x\u02c6p(t) = A\u02c6p (u\u02c6p(t)) x\u02c6p(t) + B\u02c6p (u\u02c6p(t)) u\u02c6p(t) y\u02c6p(t) = C\u02c6p x\u02c6p(t) , (19) where A\u02c6p(u) = \u2212\u03c9c,\u02c6 p(u), B\u02c6p(u) = \u03c9c,\u02c6 p(u) and C\u02c6p = 1. The cutoff angular frequency of this non-linear filter: \u03c9c,\u02c6 p(u) = Aloop + 1 \u03c4in(u) + (Aamp + 1)\u03c4mil(u) (20) is directly proportional to the effective radianceL = Lsig + Ldark . Nonethe- less, it remains very much smaller than the radiance-independent cutoff angular",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S82",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "frequencies of the source follower buffer\u03c9c,sf and differencing amplifier\u03c9c,diff . Therefore, this rather simple 1st-order model forms the dominant pole approx- imation of the full 4th-order pixel bandwidth model under extreme low-light, which is relatively accurate. Furthermore, whenL(t) \u2248 Ldark , we can further approximate this model with its linearized variant, which has a constant cutoff angular frequency of\u03c9c,\u02c6 p(log Ldark ) = \u03c9c,dom,min (cf. Eq. (11)). Fundamental Limitations. The logarithmic photoreceptor 2nd-order NLTI LPF is characterized byAamp, Aloop, \u03c4out, Cin VT/\u03ba = \u03c4inL and Cmil VT/\u03ba = \u03c4milL. When the unknown logarithmic photoreceptor model parameters are jointly op- timized, the predicted effective radiance\u02c6L from the reconstructed NeRF is only accurate up a scale, since \u03c4in and \u03c4mil are invariant",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S83",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "to the common scale of L, Cin VT/\u03ba and Cmil VT/\u03ba. This further necessitates a translated-gamma correction (Sec. 3.4) of\u02c6L post-reconstruction. B Event Simulator AsalludedinSecs.3.2and4,oureventsimulatorextendstheimprovedESIM[39] eventsimulatorintroducedinRobuste-NeRFwiththeproposedpixelbandwidth model, particularly the discrete-time model given by Eq. (8). We appropriately 22 W. F. Low and G. H. Lee (b) \ud835\udc38!\"=100 000 \ud835\udc59\ud835\udc62\ud835\udc65(b) \ud835\udc38!\"=1 000 \ud835\udc59\ud835\udc62\ud835\udc65 (d) \ud835\udc38!\"=10 \ud835\udc59\ud835\udc62\ud835\udc65(a) Scene Fig. 6:Our simulated events transformed to the scene plane. initialize the state of the 4th-order NLTI LPF with the steady-state \u00afx[k0] = [ 0 u[k0] u[k0] u[k0] ]\u22a4 on the initial input effective log-radianceu[k0] = log L[k0]. Fig. 6 depicts our simulated events on a simple planar scene of shapes under various scene illuminanceEsc. It can be observed that as the scene illumina- tion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S84",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "improves, the events become more localized around the edges in the scene. Moreover, the spreading or blurring of negative events in red is more severe than that of positive events in blue. This happens because negative events involve a transition from a high to low effective log-radiancelog L, where the latter is as- sociated to a low pixel bandwidth. All these observations validate the accuracy of our event simulator, as they conform to the expected behavior of an event sensor pixel. C Implementation Details Deblur e-NeRF. The implementation of our method is based on Robuste- NeRF [28]. In particular, we adopt the same NeRF model architecture, parame- terization of positive-to-negative contrast threshold ratioC+1/C\u22121 and refractory period for joint optimization, NerfAcc",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S85",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "[23]/Instant-NGP [33] parameters, train- ing schedule, learning rates and constant-rate camera pose interpolation. Nonetheless, since our method can theoretically reconstruct a NeRF with gamma-accurate predicted effective radiance \u02c6L, particularly under unknown contrast thresholds, we also parameterize the mean contrast threshold \u00afC = 1 2 (C\u22121 + C+1), which defines the scale of the contrast thresholds, viaSoftPlus to ensure that it is always positive during its joint optimization. Such a parame- terization of the contrast thresholds is optimal in the sense that the normalized predictions, which are\u2206 log \u02c6Lblur/\u00afC for \u2113diff and \u03b4 log \u02c6Lblur/\u00afC for \u2113tv , and normalized targets, which arepCp/\u00afC for \u2113diff and 0 for \u2113tv , are invariant toC+1/C\u22121 and \u00afC, respectively. Furthermore, we parameterize the pixel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S86",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "bandwidth model parameters as A\u22121 amp, A\u22121 loop, \u03c4out, Cmil VT/\u03ba = \u03c4milL, \u03c4sf = \u03c9\u22121 c,sf and \u03c4diff = \u03c9\u22121 c,diff , which gener- ally has values smaller than 1, viaSoftPlus as well for joint optimization. Note that we do not parameterizeCin VT/\u03ba = \u03c4inL, but keep it fixed at an arbitrary positive value, as the predicted effective radiance is only accurate up to a scale when pixel bandwidth model parameters are jointly optimized (Appendix A). Deblur e-NeRF 23 This helps to clamp down on this gauge freedom during joint optimization, and yields a minimal parameterization of\u03c4in and \u03c4mil up to an arbitrary scale. Care must be taken to ensure that the predefinedCin VT/\u03ba is larger than the minimum",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S87",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "effective radiance\u03f5 = 0.001 the NeRF model can output. We adopt a sample sizek \u2212 k0 + 1 of 30 for importance sampling of inputs u = log L in all experiments. Moreover, we sample the optimal input sample timestamps Ti from the transformed exponential distribution given by Eq. (11), but truncated in practice to a finite support of(tk0 , tk] such that its cumula- tive probability is exactly0.95. The sampling is done using a variant of inverse transform sampling, where instead of uniformly sampling the interval(0, 1] (and then applying the inverse cumulative distribution function), we directly take k\u2212k0 +1 = 30 evenly-spaced samples in the same interval. This helps to prevent significant under/over-representation of inputsu around certain time",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S88",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "regions in the computation of the outputy[k], due to randomness. Apart from that, since we assumeu is stationary prior the start of the event sequence, we assign input samples with timestamps prior the start to have the same value as the initial input. Furthermore, we sample each subinterval(tstart, tend] between the interval (tref , tcurr ] for use in\u2113tv , by first sampling the length of the subintervaltend \u2212 tstart from a triangular distribution with a support of [0, tcurr \u2212 tref ) and a mode of 0, then sampling tstart from a uniform distribution with a support of [0, (tcurr \u2212 tref ) \u2212 (tend \u2212 tstart)). Joint optimization of the pixel bandwidth model parameters is done with the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S89",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "same learning rate of 0.01 as the NeRF model parameters. Moreover, we train our method with loss weights of\u03bbdiff = 1 and \u03bbtv = 0.1, as well as a batch size (defined relative to Robuste-NeRF) of 217 = 131 072, by default. However, we observed that our loss values for the threshold-normalized dif- ference loss\u2113diff , under thehard setting (v = 4\u00d7, Esc = 10lux) in the collective effect synthetic experiment, is\u223c 100\u00d7 smaller than that of other settings, but the loss values for threshold-normalized total variation loss\u2113tv (Eq. (12)) re- mains in the same order. This will cause the total training lossL (Eq. (2), but with \u03bbtv \u2113tv (e) instead of\u03bbgrad\u2113grad(e)) to be inappropriately dominated by the regularization loss",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S90",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "\u2113tv , instead of the primary reconstruction loss\u2113diff , if the default loss weights are used. Thus, we adopt\u03bbtv = 0.001, which is100\u00d7 smaller than the default, under the hard setting to rebalance both losses. Apart from that, we also adopt\u03bbtv = 0.01, which is 10\u00d7 smaller than the default, under the Esc = 10 lux setting in the synthetic experiment studying the effect of scene illuminance, due to similar observations. As we employ the Adam [17] optimizer, which is invariant to diagonal rescaling of gradients hence loss, this is equivalent to a loss weight \u03bbdiff of 100\u00d7 or 10\u00d7 larger than that of the default, while maintaining\u03bbtv at the default. Baselines. We employ the official implementation of Robuste-NeRF in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S91",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "our experiments. However, we adopt\u03bbgrad = 0.00001 and \u03bbgrad = 0.0001, which are 100\u00d7 and 10\u00d7 smaller than the default at\u03bbgrad = 0.001, under thehard (v = 24 W. F. Low and G. H. Lee 4\u00d7, Esc = 10lux) andEsc = 10lux settings in synthetic experiment, respectively, due to similar observations made in our method. Moreover, we implement E2VID + NeRF according to how it is done for the experiments in Robuste-NeRF. Translated-Gamma Correction. To account for the time-varying sensor gain (i.e. ISO) and exposure time of the captured reference images, particularly during evaluation, we additionally scale each correction with the known gain- exposure product of the corresponding reference image. Theoptimalcorrectionparameters a, b andc areoptimizedusingtheLevenberg- Marquardt algorithm with a Trust",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S92",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "Region strategy to determine the optimal damping factor at each iteration. We adopt the implementation provided by PyPose [45], as well as its default hyperparameters. Furthermore, we appropri- ately initialize the optimization withc = 0 and the solution ofa and b given by gamma correction (Eq. (5)). The optimization is performed until the sum of squared correction errors has converged, up to a maximum of 20 iterations. D Interpretation of Real Quantitative Results Note that care must be taken when interpreting the quantitative results of the real experiments presented in Tab. 2 and Tab. 10, since they are not truly indica- tive of theabsolute performance of all methods, but likely only indicative of their relative performance. This is due to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S93",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "the fact that the target novel views, given by a separate standard camera, suffer from motion blur, rolling shutter artifacts, and saturation, as a result of a significantly smaller dynamic range compared to an event camera. Furthermore, the target novel views are not raw images that does not depend on the unknownCamera Response Function(CRF), and are grayscale images converted from RGB images provided by the camera, which might not reflect the spectral sensitivity of the monochrome event camera. E Additional Experiment Results E.1 Per-Scene Breakdown Tab. 6 and Figs. 7 and 8 show the quantitative and qualitative results of all methods, respectively, for all synthetic scene sequences simulated with the hard setting (v = 4 \u00d7, Esc = 10 lux).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S94",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "The results clearly demonstrate our superior performance in reconstructing a blur-minimal NeRF from motion-blurred events. E.2 Ablation on Pixel Bandwidth Model To further ascertain the role of the proposed pixel bandwidth model, we evaluate our method with and without the pixel bandwidth model incorporated, under the same settings as the synthetic experiment in studying the collective effect, without joint optimization of pixel bandwidth model parameters. The quanti- tative results given in Tab. 7 undoubtedly verifies the importance of the pixel bandwidth model in accounting for event motion blur. Deblur e-NeRF 25 Table 6:Per-synthetic scene breakdown under the hard setting.\u2020Trained with1/8\u00d7 the batch size of baselines. Synthetic Scene Metric Method Chair Drums Ficus Hotdog Lego Materials Mic Mean E2VID + NeRF",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S95",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "16.67 15.00 16.25 17.53 14.75 11.65 15.72 15.37 Robust e-NeRF 21.64 17.41 21.80 15.05 18.28 15.68 19.11 18.42PSNR \u2191 Deblur e-NeRF\u2020 27.39 22.14 29.10 23.69 27.69 24.49 28.53 26.15 E2VID + NeRF 0.835 0.776 0.840 0.842 0.719 0.726 0.854 0.799 Robust e-NeRF 0.836 0.758 0.864 0.849 0.754 0.772 0.862 0.814SSIM \u2191 Deblur e-NeRF\u2020 0.902 0.839 0.944 0.904 0.896 0.890 0.951 0.904 E2VID + NeRF 0.374 0.498 0.310 0.391 0.509 0.589 0.380 0.436 Robust e-NeRF 0.216 0.336 0.146 0.287 0.279 0.295 0.228 0.255LPIPS \u2193 Deblur e-NeRF\u2020 0.107 0.231 0.120 0.168 0.105 0.116 0.093 0.134 chair E2VID +NeRFRobust e-NeRFDeblur e-NeRF(Ours)Target drumsficus Fig. 7:Synthesized novel views onchair, drums and ficus under the hard setting E.3 Effect of Reduced Batch Size To assess the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S96",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "true impact of training with a reduced batch size, we also bench- mark our method with a batch size of1/8\u00d7 and 1\u00d7 that of our baselines, under 26 W. F. Low and G. H. Lee hotdog E2VID +NeRFRobust e-NeRFDeblur e-NeRF(Ours)Target legomaterials mic Fig. 8:Synthesized novel views onhotdog, lego, materials and mic under the hard setting the same settings as the experiment in Appendix E.2, but only on thelego scene. The quantitative results reported in Tab. 8 provide a glimpse into the true strength of our method, as significant improvements can be observed as the batch size increases to that of the baselines. E.4 Ablation on Input Sample Size We perform a cost-benefit analysis on the input sample sizek \u2212 k0",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S97",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "+ 1 of our method on thelego scene under the hard setting (v = 4\u00d7, Esc = 10lux). The quantitative results presented in Tab. 9 suggests that our default input sample size of 30 strikes the best balance between cost and performance. Note that the computational and memory cost of our method is proportional to the input Deblur e-NeRF 27 Table 7:Ablation on pixel bandwidth model v = 0.125\u00d7, Esc = 100 000lux v = 1\u00d7, Esc = 1 000lux v = 4\u00d7, Esc = 10luxPixel Bandwidth Model PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 \u00d7 28.75 0.948 0.048 26.98 0.934 0.061 18.31 0.822 0.245 \u2713 29.00 0.950 0.043",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S98",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "28.41 0.947 0.049 26.15 0.904 0.134 Table 8:Effect of reduced batch size on thelego scene v = 0.125\u00d7, Esc = 100 000lux v = 1\u00d7, Esc = 1 000lux v = 4\u00d7, Esc = 10lux Batch Size,\u00d7 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 1/8 29.44 0.940 0.045 28.42 0.938 0.048 27.69 0.896 0.105 1 31.27 0.953 0.030 30.43 0.950 0.038 30.72 0.948 0.037 Table 9:Ablation of input sample size onlego under the hard setting Input Sample Size PSNR\u2191 SSIM \u2191 LPIPS \u2193 1 18.46 0.765 0.273 5 22.64 0.807 0.21 15 26.41 0.875 0.125 30 27.69 0.896 0.105 50 28.18 0.902 0.097 75 28.21 0.903 0.096 sample size,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S99",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "as alluded in Sec. 5, and an input sample size of1 is equivalent to having the pixel bandwidth model removed. E.5 Results on 07_ziggy_and_fuzz_hdr Apart from08_peanuts_running and 11_all_characters, we also benchmark all methods on the07_ziggy_and_fuzz_hdr sequence from the EDS dataset, which involves a HDR scene with occasional high-speed camera motion. The quantitativeandqualitativeresultsgiveninTab.10andFig.9onceagaindemon- strates our superior performance, as the objects on the table are clearly more well-defined and the table surface, wall and curtains are much smoother, while preserving details and color accuracy of the scene. E.6 Comparison with Image Blur-Aware Baselines While motion blur in standard and event cameras are vastly different, and thus incomparable, we provide additional quantitative results of 2 otherimage blur- aware baselines: E2VID+ MPRNet [54] (a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S100",
      "paper_id": "arxiv:2409.17988v1",
      "section": "conclusion",
      "text": "seminal image deblurring method) + NeRF and E2VID + Deblur-NeRF [29] (a seminal NeRF with image blur model), for selected synthetic experiments (i.e. upper bound performance and 28 W. F. Low and G. H. Lee 07_ziggy_and_fuzz_hdr E2VID +NeRFRobust e-NeRFDeblur e-NeRF(Ours)Target Fig. 9:Synthesized novel views on the07_ziggy_and_fuzz_hdr scene Table 10:Quantitative results on the07_ziggy_and_fuzz_hdr scene",
      "page_hint": null,
      "token_count": 53,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S101",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "E2VID + NeRF 14.96 0.691 0.556 Robust e-NeRF 18.02 0.631 0.464 Deblur e-NeRF 18.47 0.648 0.440 Table 11:Comparison with image blur-aware baselines built upon E2VID. Simulation Settings / Real Scene E2VID + NeRF E2VID + MPRNet + NeRF E2VID + Deblur-NeRF PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 No event motion blur 19.49 0.847 0.268 19.44 0.851 0.267 19.84 0.839 0.291 v = 0.125\u00d7, Esc = 100 000lux 19.19 0.844 0.281 19.18 0.849 0.260 19.15 0.841 0.288 v = 1\u00d7, Esc = 1 000lux 18.85 0.839 0.278 18.86 0.843 0.269 18.73 0.818 0.317 v = 4\u00d7, Esc = 10lux 15.37 0.799 0.436 15.44 0.794 0.439 15.42 0.783 0.472 07_ziggy_and_fuzz_hdr",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2409_17988v1:S102",
      "paper_id": "arxiv:2409.17988v1",
      "section": "method",
      "text": "14.96 0.691 0.556 14.96 0.691 0.552 14.85 0.680 0.504 08_peanuts_running 14.85 0.690 0.595 14.81 0.690 0.604 14.91 0.682 0.517 11_all_characters 13.12 0.695 0.627 13.10 0.695 0.624 12.95 0.689 0.576 collective effect) and the real experiment, to make our experiments more com- plete. From the results reported in Tab. 11, it is evident that the incorporation of an image blur or deblur model is unable to account for event motion blur, as the performance is virtually the same with or without it. This reinforces the importance for our physically-accurate pixel bandwidth model to account for event motion blur under arbitrary speed and lighting conditions.",
      "page_hint": null,
      "token_count": 103,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9549710939914803,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 28,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 2575,
        "empty": false
      },
      {
        "page": 2,
        "chars": 2421,
        "empty": false
      },
      {
        "page": 3,
        "chars": 6309,
        "empty": false
      },
      {
        "page": 4,
        "chars": 2692,
        "empty": false
      },
      {
        "page": 5,
        "chars": 2255,
        "empty": false
      },
      {
        "page": 6,
        "chars": 7384,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2318,
        "empty": false
      },
      {
        "page": 8,
        "chars": 2325,
        "empty": false
      },
      {
        "page": 9,
        "chars": 2318,
        "empty": false
      },
      {
        "page": 10,
        "chars": 3033,
        "empty": false
      },
      {
        "page": 11,
        "chars": 2788,
        "empty": false
      },
      {
        "page": 12,
        "chars": 2038,
        "empty": false
      },
      {
        "page": 13,
        "chars": 2746,
        "empty": false
      },
      {
        "page": 14,
        "chars": 3318,
        "empty": false
      },
      {
        "page": 15,
        "chars": 1664,
        "empty": false
      },
      {
        "page": 16,
        "chars": 3142,
        "empty": false
      },
      {
        "page": 17,
        "chars": 3373,
        "empty": false
      },
      {
        "page": 18,
        "chars": 3359,
        "empty": false
      },
      {
        "page": 19,
        "chars": 1126,
        "empty": false
      },
      {
        "page": 20,
        "chars": 1817,
        "empty": false
      },
      {
        "page": 21,
        "chars": 2181,
        "empty": false
      },
      {
        "page": 22,
        "chars": 2508,
        "empty": false
      },
      {
        "page": 23,
        "chars": 3284,
        "empty": false
      },
      {
        "page": 24,
        "chars": 2751,
        "empty": false
      },
      {
        "page": 25,
        "chars": 1116,
        "empty": false
      },
      {
        "page": 26,
        "chars": 885,
        "empty": false
      },
      {
        "page": 27,
        "chars": 1954,
        "empty": false
      },
      {
        "page": 28,
        "chars": 1564,
        "empty": false
      }
    ],
    "quality_score": 0.955,
    "quality_band": "good"
  }
}