{
  "paper": {
    "paper_id": "arxiv:1811.11745v2",
    "title": "Learning to Synthesize Motion Blur",
    "authors": [
      "Tim Brooks",
      "Jonathan T. Barron"
    ],
    "year": 2018,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "We present a technique for synthesizing a motion blurred image from a pair of unblurred images captured in succession. To build this system we motivate and design a differentiable \"line prediction\" layer to be used as part of a neural network architecture, with which we can learn a system to regress from image pairs to motion blurred images that span the capture time of the input image pair. Training this model requires an abundance of data, and so we design and execute a strategy for using frame interpolation techniques to generate a large-scale synthetic dataset of motion blurred images and their respective inputs. We additionally capture a high quality test set of real motion blurred images, synthesized from slow motion videos, with which we evaluate our model against several baseline techniques that can be used to synthesize motion blur. Our model produces higher accuracy output than our baselines, and is significantly faster than baselines with competitive accuracy.",
    "pdf_path": "data/automation/papers/arxiv_1811.11745v2.pdf",
    "url": "https://arxiv.org/pdf/1811.11745v2",
    "doi": null,
    "arxiv_id": "1811.11745v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.783131+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_1811_11745v2:S1",
      "paper_id": "arxiv:1811.11745v2",
      "section": "body",
      "text": "Learning to Synthesize Motion Blur Tim Brooks Jonathan T. Barron Google Research",
      "page_hint": null,
      "token_count": 12,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S2",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "We present a technique for synthesizing a motion blurred image from a pair of unblurred images captured in succes- sion. To build this system we motivate and design a differen- tiable \u201cline prediction\u201d layer to be used as part of a neural network architecture, with which we can learn a system to regress from image pairs to motion blurred images that span the capture time of the input image pair. Training this model requires an abundance of data, and so we design and exe- cute a strategy for using frame interpolation techniques to generate a large-scale synthetic dataset of motion blurred images and their respective inputs. We additionally capture a high quality test set of real motion blurred images, synthe-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S3",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "sized from slow motion videos, with which we evaluate our model against several baseline techniques that can be used to synthesize motion blur. Our model produces higher ac- curacy output than our baselines, and is signi\ufb01cantly faster than baselines with competitive accuracy. 1. Introduction Though images are often thought of as capturing a sin- gle moment in time, all images in fact capture a duration of time: an image begins when a camera starts collecting light, and ends when that camera stops collecting light. If the camera or the scene move while light is being collected, the resulting image will exhibit motion blur. That blur may indicate the speed of a subject or may serve to separate a subject from",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S4",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "the background, depending on the relative mo- tion of the camera and the subject (see Figure 1(b)). Motion blur is a valuable cue for image understanding. Given a single image containing motion blur, one can es- timate the relative direction and magnitude of scene mo- tion that resulted in the observed blur [7, 8]. This motion estimate may be semantically meaningful [32], or may be used by a deblurring algorithm to synthesize a sharp im- age [5, 9, 17, 23]. Recent work has relied on deep learn- ing for removing motion blur and inferring the underlying motion of the scene [6, 11, 30]. Deep learning techniques tend to need an abundance of training data to work well, (a) A pair",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S5",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "of input images. (b) Our model\u2019s output. Figure 1. In (a) we present two images of a subject moving across the image plane. Our system uses these images to synthesize the motion blurred image in (b), which conveys a sense of motion and separates the subject from the background. 1 arXiv:1811.11745v2 [cs.CV] 20 Jun 2019 and so to train these techniques one must generate large amounts of synthetic training data by synthetically blurring sharp images. These techniques also tend to use synthetic data (usually sharp images convolved by real or synthetic \u201ccamera shake\u201d kernels) for quantitative evaluation, using real motion-blurred images only to produce qualitative vi- sualizations. Naturally, the ability of these learned models to generalize to real images depends",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S6",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "on the realism of their synthetic training data. In this paper, we treat the inverse of this well-studied blur removal task as a \ufb01rst class prob- lem. We present a fast and effective way to synthesize the training data necessary to train a motion deblurring algo- rithm, and we quantitatively demonstrate that our technique generalizes from our synthetic training data to real motion- blurred imagery. Talented photographers sometimes use motion blur for artistic effect (Figure 2(a)). But composing an artful motion-blurred photograph is a dif\ufb01cult process, typically requiring a tripod, manual camera settings, perfect timing, expert skill, and much trial and error. As a result, for ca- sual photographers motion blur is likely to manifest as an unwanted artifact (Figure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S7",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "2(b)). Because of the dif\ufb01culty in using motion blur effectively, most consumer cameras are designed to take images with as little motion blur as possi- ble \u2014 though if noise is a concern some motion blur is un- avoidable, especially in low-light environments or in scenes with signi\ufb01cant motion [12]. Artistic control over motion blur is therefore out of reach for most casual photographers. By allowing motion blurred images to be synthesized from the conventional unblurred images that are captured by stan- dard consumer cameras, our technique allows non-experts to create motion blurred images in a post-capture setting. This is analogous to how recent progress in depth estima- tion has enabled post-capture on-device depth-of-\ufb01eld ma- nipulation, also known as \u201cPortrait",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S8",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "Mode\u201d [2, 4, 31]. Motion blur is also an important tool in cinematography, where \ufb01lmmakers will carefully adjust the shutter angle of their camera to create a particular \u201c\ufb01lm look\u201d. As in pho- tography, this requires expert domain knowledge and skill- ful execution. Our system (or indeed any system that oper- ates on pairs of frames) can be used to manipulate the mo- tion blur of video sequences after the fact, by independently processing all pairs of adjacent frames in the input video. Motion blur synthesis has been extensively studied in the rendering community [22], though these methods typically require perfect knowledge of scene velocities and depths as inputs. We instead target the most general form of this prob- lem,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S9",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "and assume the only inputs available to our system are unblurred input images, as is the case in most general vision and imaging contexts. To enable the varied image understanding and image ma- nipulation tasks that require a method for creating motion blur, we present an algorithm that takes two sharp images (a) Artful motion blur. (b) Unwanted motion blur. Figure 2. Capable photographers can use motion blur to produce striking photographs, as in (a). But for most casual photographers, motion blur is more likely to manifest as an unwanted artifact in an image that was intended to be completely sharp, as in (b). taken one after the other, as shown in Figure 1(a), and syn- thesizes a corresponding motion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S10",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "blurred image, such as in Figure 1(b). The synthesized image resembles an image captured over the time spanned by the input images \u2014 the image \u201cstarts\u201d at the \ufb01rst input image, and \u201cends\u201d at the sec- ond input image. To achieve this, we adapt recent advances in machine learning to the task of predicting line kernels for motion blurring image pairs. We build upon the recent success of convolutional neu- ral networks [16] and end-to-end training on tasks similar to ours, such as optical \ufb02ow [13, 29, 33] and frame interpola- tion [14, 24, 25, 26]. We use state-of-the-art frame interpo- lation to synthesize training data for our motion blur model, and demonstrate that our model, trained directly on the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S11",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "task of synthesizing motion blur, produces improved results on real images over baselines derived from optical \ufb02ow and frame interpolation techniques. Though frame interpola- tion achieves closer accuracy, our technique is signi\ufb01cantly faster, and is thereby better suited to the online synthesis of training data in a deep learning context, and is easier to deploy in a consumer-facing rendering or smartphone- photography setting. The remainder of this paper is structured as follows: In Section 2 we discuss the nature of motion blur as a func- tion of linear motion and motivate our novel line prediction layer. In Section 3 we de\ufb01ne a deep neural network archi- tecture based on our line prediction layer. In Section 4 we construct a synthetic",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S12",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "dataset that is used for training, and a real-world dataset that is used for evaluation. In Section 5 we evaluate the performance of our model compared to its ablations and variants, and to techniques in the literature that can be adapted to the task of synthesizing motion blur. 2. Problem Formulation We aim to take two adjacent images from a camera, say from a video or from a \u201cburst\u201d of photos [12], and from them synthesize a motion blurred image that spans the du- ration between the input images. That is, letting I1 be the image exposed for the duration [s1,t1] and I2 be the image exposed for the duration[s2,t2] (where s1 <t1 <s2 <t2), we synthesize the long exposure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S13",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "photograph I1\u21922, which spans the duration [s1,t2]. Similar to the assumptions of optical \ufb02ow, which de- scribes motion between two frames in terms of per-pixel velocity vectors, we assume locally linear motion between the two input images. We further assume that each pixel in the motion blurred image can be linearly interpolated from pixels lying on lines drawn from the corresponding pixel in each of the input images. While these assumptions are not always valid\u2014for example, in the case of objects that are rotating or oscillating\u2014we will demonstrate that this sim- ple linear model is suf\ufb01ciently expressive to produce high quality results. Our neural network architecture uses a novel \u201cline pre- diction\u201d layer, which we de\ufb01ne here. For each pixel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S14",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "in our images Ii (i\u2208{1,2}) we predict a line, where one endpoint of that line is at the pixel\u2019s location(x,y) and the other end- point is at (x+ \u2206x i (x,y),y + \u2206y i (x,y))\u2014the pixel\u2019s loca- tion when advected by some predicted offset\u2206i. The line is composed of N evenly-spaced discrete samples, for which we also predict Wi(x,y,n ), a weighting for each sample. Our \ufb01nal predicted image I1\u21922 is de\ufb01ned as the weighted average of the two input images according to the discrete samples along all lines: I1\u21922(x,y) = \u2211 i\u2208{1,2} N\u22121\u2211 n=0 Wi(x,y,n )\u00d7 (1) Ii ( x+ ( n N \u22121 ) \u2206x i (x,y),y + ( n N \u22121 ) \u2206y i (x,y) ) ,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S15",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "where Ii(x,y) is the result of bilinear interpolation of Ii at any continuous location (x,y). We refer to this approach as \u201cline prediction\u201d, analo- gously to the \u201ckernel prediction\u201d literature [3, 21, 25]. Our model can be thought of as a form of kernel prediction, as the weighted average in Equation 1 can be rasterized into a per-pixel convolution with a discrete kernel composed of the sum of the weighted bilinear interpolation kernels used in line prediction\u2014though reformulating the blur in this way makes it signi\ufb01cantly more expensive to compute. For our line prediction technique to work properly, we must reason about the relationship between our line off- sets \u2206i and our sampling density. Since the standard deep learning techniques",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S16",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "we use for estimating the parameters of our line prediction layer have dif\ufb01culty producing variable- length outputs, the number of estimated line samples N is \ufb01xed. However, if the motion estimated at a given pixel is signi\ufb01cantly greater than the number of samples available to reconstruct our predicted line, then our resulting motion (a) Temporal undersampling (b) Temporal supersampling Figure 3. Temporal sampling is critical to the construction of our model and our training data. If a motion blurred image is syn- thesized using signi\ufb01cantly fewer samples than the maximum dis- placement of any pixel across those samples, then that synthesized image may be temporally undersampled. This results in discon- tinuous artifacts along the direction of the motion, as in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S17",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "(a). If the sampling density is suf\ufb01ciently large with respect to image resolu- tion and object motion then the synthesized images will not exhibit any such artifacts, as in (b). blurred image will be temporally undersampled, and will therefore contain artifacts from these \u201cgaps\u201d when synthe- sizing motion blur. See Figure 3 for a visualization of this sampling issue. For this reason, when determining a value for N, we must impose a bound on the magnitude of our line endpoint displacements (\u2206x i (x,y),\u2206y i (x,y)). We only ad- dress the task of synthesizing motion blurred images whose maximal displacement is 32 pixels in length, and we set N = 17 . We found that we are able to use",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S18",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "half as many samples as our maximum displacement because the kernel used by bilinear interpolation effectively pre\ufb01lters the con- volution induced by our line prediction. This limit on pixel displacement and sampling density is analogous to the sim- ilar limits of kernel prediction-based video frame interpola- tion techniques with regard to their kernel sizes. Our decision to have our network predict a set of sam- pling weights Wi(x,y,n ) may seem unusual, as techniques from the graphics literature tend to assign uniform weights to pixels when rendering motion blur [20]. These learned weights allow our algorithm to handle complex motions and occlusions, and to hedge against certain failure modes. For example, by emitting a weight of 0, our model can",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S19",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "ignore certain pixels during integration, which may be necessary if the pixel of interest moves behind an occluder on its path towards its location in the other frame. Because our syn- thesis happens simultaneously in both the \u201cforward\u201d and \u201cbackward\u201d direction, our model can use these weights to smoothly transition across images or to selectively draw from one image but not the other, further improving its ability to reason about occlusion. Though our model is constrained to linear motion, these weights can be used to model an object as moving at a non-constant speed along Figure 4. A visualization of our architecture, which takes as input a concatenation of our two input images and uses a U-Net convolutional neural network",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S20",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "to predict the parameters for our line prediction layer. its line. For example, if an object accelerates towards its destination, our model can synthesize a more accurate mo- tion blur (without introducing any temporal undersampling issues) by giving early samples higher weights than later samples. 3. Model Architecture Our model is built around the U-Net architecture of [28], which feeds into our line prediction layer whose output is used to synthesize a motion blurred image. The input to our model is simply the concatenation of our two input images. See Figure 4 for a visualization of our architecture. The U-Net architecture, which has been used success- fully for the related task of frame interpolation [14, 26], is a fully-convolutional encoder/decoder",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S21",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "model with skip con- nections from each encoder to its corresponding decoder of the same spatial resolution. Our encoder consists of \ufb01ve hi- erarchies (sets of layers operating at the same scale) each containing three \u2018conv\u2019 layers, and where all but the last hi- erarchy are followed by a max pooling layer that downsam- ples the spatial resolution by a factor of 2 \u00d7. Our decoder consists of four hierarchies, each with three conv layers that are followed by a bilinear upsampling layer that increases spatial resolution by a factor of 2 \u00d7. Each conv layer uses 3\u00d73 kernels and is followed by leaky ReLU activation [19]. We train our model end-to-end by minimizing the L1 loss between our model\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S22",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "predicted motion blurred image and our ground-truth motion blurred images. Our data aug- mentation and training procedure will be described in more detail in Section 5. We experimented with pretraining our line prediction model using optical \ufb02ow training data, as prescribed in [33], but this did not appear to improve per- formance or signi\ufb01cantly speed up convergence. Our model is implemented using TensorFlow [1]. 4. Dataset Training or evaluating our model requires that we pro- duce ground truth data of the following form: two input images, and an output image wherein the camera has in- tegrated light from the start of the \ufb01rst image to the end of the second image. Because large neural networks require an abundance of data,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S23",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "for training we present our own synthetic data generation technique based around video frame inter- polation, which we use to synthesize motion blurred images from conventional, abundantly available video sequences (Sec 4.1). We take sets of adjacent video frames, synthesiz- ing many intermediate images between those frames, and average all resulting frames to make a single synthetic mo- tion blurred image (where the original two frames can then be used as input to our algorithm). These synthesized mo- tion blurred images look reasonable and are easy to generate in large quantities, but they may contain artifacts due to mis- takes in the underlying video frame interpolation technique and so have questionable value as a \u201ctest set\u201d. Therefore, for evaluation, where",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S24",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "data \ufb01delity is valued more highly than quantity, we use a small number of real slow-motion video sequences. The \ufb01rst and last frames of each sequence are used as input to our algorithm, and the sum of all frames in the sequence is used as the \u201cground-truth\u201d motion blurred image (Sec 4.2). 4.1. Synthetic Training Data We manually created our own dataset directly from pub- licly available videos, as this gives us precise control over things like downsampling and the amount of motion present in the scene, while allowing us to select for interesting, high-frequency scene content. To construct this dataset, we \ufb01rst extract sets of adjacent triplets from carefully chosen video sequences, and then use those triplets to train",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S25",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "a video frame interpolation algorithm. This video frame interpo- lation algorithm is then applied recursively to all triplets, which allows us to synthesize a 33 frame interpolated se- quence from each triplet that can then be averaged to pro- duce a synthetically motion blurred image. These images are then treated as \u201cground truth\u201d when training our model. We downloaded \u223c30,000 Creative Commons licensed 1080p videos from YouTube in categories that tend to have signi\ufb01cant amounts of motion, such as \u201cWildlife,\u201d \u201cExtreme Sports,\u201d and \u201cPerforming Arts.\u201d We then downsampled each video by a factor of 4 \u00d7using bicubic interpolation to remove compression artifacts, and then center-cropped each sequence to a resolution of 270\u00d7270. From these video se- quences, we extracted triplets",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S26",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "of adjacent frames that satisfy the following properties: 1. High frequency image content : Focusing training on images with interesting gradient information tends to im- prove training for image synthesis tasks such as our own, as shown in [10]. We therefore rejected any triplet whose aver- age gradient magnitude (computed using Sobel \ufb01lters) over all pixels was less than13 (assuming images are in[0,255]). 2. Suf\ufb01cient motion: Scenes without motion are unlikely to provide much signal during training. Therefore, for each triplet we estimated per-pixel motion across adjacent frames (using the fast optical \ufb02ow technique of [18]) and only ac- cepted triplets where at least 10% of each pixel\u2019s \ufb02ow had a magnitude (\u221e-norm) of at least 8 pixels. 3. Limited",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S27",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "motion: Our learned model and many of the baseline models we compare against have outputs with lim- ited spatial support, and we would like our training data to lie entirely within the receptive \ufb01eld of our models. We therefore discarded any triplet that contained a \ufb02ow esti- mate with a magnitude (\u221e-norm) of more than 16. 4. No abrupt changes : Signi\ufb01cant and rapid changes across adjacent frames in our video data are often due to cuts or other kinds of video editing, or global changes in brightness or illumination. To address this, we warp each frame in each triplet according to its estimated motion and discard triplets with an average L1 distance of more than13 (assuming images are in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S28",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "[0,255]). 5. Approximately linear motion: Our model architecture is only capable of estimating and applying a linear motion blur. Images that are not expressible using linear blurs will therefore likely not contribute much signal during training. We therefore compare the \u201cforward\u201d \ufb02ow between the sec- ond and third frame to the negative of the \u201cbackward\u201d \ufb02ow from the \ufb01rst and second frame, and discard any triplets with a mean disagreement of >0.8 pixel widths. Note that (5) represents a kind of \u201cco-design\u201d of our al- gorithm and our training data, in that we craft our dataset to complement the assumptions of our model. To evaluate a broader generalization of our model, we do not impose this constraint on our \u201creal\u201d",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S29",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "testing dataset. To ensure diversity, we extract no more than 50 triplets from each video, and no more than a single triplet from a given scene within each video. This process resulted in Figure 5. Here we show randomly chosen input/output pairs from our synthetic training dataset. To generate this data we identify triplets (shown in the \ufb01rst three columns) of adjacent frames that satisfy our criteria for motion and image content, use those triplets to train a video frame interpolation model, and apply that model re- cursively on each triplet to generate intermediate frames which are then averaged to synthesize a single motion blurred image (shown in the last column). When training our motion blur model, we use the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S30",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "\ufb01rst and last images of each triplet as input and the averaged image as ground-truth. >300,000 unique triplets, of which 5% are set aside for validation with the remaining 95% used for training. This training/validation split is carefully constructed such that all triplets generated from any given video are assigned to ei- ther the training or validation split \u2014 no video\u2019s triplets are present in both the training or validation splits. With this dataset we then train a video frame interpola- tion network based on [26], which will shortly be used to produce the \ufb01nal motion blur training data we are pursu- ing. Our frame interpolation network is the same model as described in Section 3, but using a separable",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S31",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "kernel predic- tion layer of 33\u00d733 learned kernels instead of our line pre- diction layer. Our training procedure is described in more detail in Section 5. The need to train this frame interpo- lation model is why we chose to extract triplets from our video sequences as opposed to just two frames, as the mid- dle frame of each triplet can be used as ground-truth during this training stage (but will be ignored when training our motion blur model). After training, this frame interpolation model takes two frames as input, and from them synthesizes an output frame that should lie exactly in between the two input frames. We apply this network to our triplet of video frames, \ufb01rst using the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S32",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "\ufb01rst and second frames of the triplet as input to the network to synthesize an in-between frame, then using the second and third frames to synthesize another in-between frame. We then apply this same process recur- sively using the real and newly-interpolated frames as input. This is done 4 times, resulting in a 33 frame sequence of interpolated frames. These frames are all then averaged to produce a synthetically motion blurred image. Note that our recursive interpolation process yields 15 frames between each image in our triplet. Because our previously-described data collection procedure omitted adjacent frames with a motion of more than 16 pixels, this means that we should expect our interpolated images to have a motion of less than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S33",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "one pixel width per frame. This means that our resulting motion blurred images should not suffer from temporal un- dersampling. See Figure 5 for some examples of our syn- thetic training data. 4.2. Real Test Data For evaluation purposes we would like a small, high- quality dataset that is not vulnerable to the artifacts that may be introduced by frame interpolation algorithms, and is as close as possible to a real in-camera motion blurred im- age. Although it is easy to acquire motion blurred images by themselves, acquiring the two input images alongside that motion blurred image is not possible with conventional camera sensors. We therefore capture a series of short slow motion videos, where the \ufb01rst and last frames",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S34",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "of each video are used as input to our system, and the per-pixel mean of all frames is used as the ground-truth motion blurred image. Our dataset was gathered by a photographer using the Pana- sonic LUMIX GH5s, which records videos at 240fps. The photographer was instructed to photograph subjects that are well-suited to an artistic use of motion blur: people walk- ing or running, vehicles moving, falling water, etc. Im- ages were bicubicly downsampled by 2 \u00d7to help remove demosaicing and compression artifacts, and center-cropped to 512\u00d7512 pixels. From each video we selected a span of frames such that the total motion across the span is no more than 32 pixels. Any sequences that exhibited any tem- poral undersampling",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S35",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "were removed. For each sequence we generated a single motion blurred image by simply averag- ing the frames, and we set aside the \ufb01rst and last frame of each sequence for use as input to our model. Each sequence has a variable length of frames, as we saw no need to omit frames from each sequence if they happened to be tempo- rally super-sampled. Our \ufb01nal dataset consists of 21 diverse sequences. See Figure 8 and the appendix for examples. 5. Experiments Our motion blur models, as well as our frame interpola- tion model used to generate our synthetic data, were trained distributedly over 8 NVIDIA Tesla P100 GPUs for 3.5M iterations on batches of size 16 using the Adam",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S36",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "optimiza- tion algorithm [15] with a learning rate of\u03b1= 0.00002 and momentum decay rates \u03b21 = 0 .9 and \u03b22 = 0 .998. Dur- ing training we performed data augmentation by randomly extracting a 256\u00d7256 crop from each image, and then ran- domly applying a horizontal \ufb02ip, vertical \ufb02ip, and a 90\u25e6 rotation. Training to convergence took \u223c2.5 days. We evaluate our model against \ufb01ve baseline algorithms: A \u201cnaive\u201d baseline that is simply the mean of the two in- put images (see Figure 7(a)), the non-learned and non-deep optical \ufb02ow algorithm of [27], the state-of-the-art learned \ufb02ow method of [29], the video frame interpolation work of [26] (which improves upon [25]), and the state-of-the-art video interpolation work of [14]. We",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S37",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "additionally evaluate against three ablated versions of our model: 1. Direct Prediction: instead of using line prediction our network directly estimates the motion blurred image, by re- placing our line prediction model with a single 1 \u00d71 conv layer that produces a 3 channel output. 2. Uniform Weight: we use uniform weights for each sample along lines rather than learning weights (i.e., all Wi(x,y,n ) = 1/2N). 3. Kernel Prediction: instead of using line prediction we use the separable kernel prediction of [26], by replacing our line prediction layer with a single 1 \u00d71 conv layer at the end of our network that produces a 65\u00d765 separable kernel (represented as a 65\u00d71 and 1\u00d765 kernel) at each pixel. Our \u201ckernel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S38",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "prediction\u201d model has an inherent limitation, as separable kernels are limited in their ability to represent angled blur kernels. For example, the matrix corresponding to a blur kernel of a diagonal line is full-rank and cannot be represented well as a rank-1 matrix, so equivalently, the ker- nel cannot be represented well by a separable kernel. This limitation can be addressed by using non-separable kernels as in [25], however, the large kernels needed for our applica- tion require extreme amounts of memory that far exceeded the limits of our GPUs when we attempted to use this ap- proach for training. To generate motion blurred comparisons from our op- tical \ufb02ow baselines, we employed the same line blurring scheme as our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S39",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "\u201duniform weight\u201d model, and bilinearly sam- ple N evenly-spaced values from the input images along lines corresponding to the optical \ufb02ow \ufb01elds. These sam- pled images are then averaged to produce a motion blurred image. We found that both \ufb02ow algorithms bene\ufb01ted signif- icantly (a PSNR improvement of \u223c5) from using the nega- tive backward \ufb02ow instead of the forward \ufb02ow to produce motion blur, so we adopted that strategy when evaluating our baseline \ufb02ow techniques. More sophisticated strategies for gathering and scattering in forward and backward direc- tions of object velocities have been used to synthesize mo- tion blur in the graphics literature [20, 22], but these tech- niques assume that perfect scene geometry is known and so (a)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S40",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "Input image 1 (b) Input image 2 (c) Non-input intermediate frames (d) Ground-truth motion blur (e) PWC-Net [29] (f) EpicFlow [27] (g) SepConv [26] (h) Super SloMo [14] (i) Ours (direct pred.) (j) Ours (uniform weight) (k) Ours (kernel pred.) (l) Our Model Figure 6. Results for one scene from our test dataset. The ground truth image (d) is the sum of the input images (a) & (b) and of the frames between those two images (c). We programmatically select the three non-overlapping 32 \u00d732 sub-images with maximal variance across all frames in (c) and present crops of those regions, rendered with nearest-neighbor interpolation and sorted by their y-coordinates. We compare our model (l) against four baselines (e)-(h), and three",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S41",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "ablations (i)-(k). See the appendix for additional results. cannot be used for our task. Comparisons against frame interpolation baselines were conducted by recursively running frame interpolation on the input image pair for5 iterations, which results in a33-frame sequence \u2014 a suf\ufb01ciently dense sampling given the limit of 32-pixel displacements in our real test set. The result- ing synthetic slow motion sequences were then averaged to produce a motion blurred image. Algorithm PSNR SSIM Runtime (ms)Naive Baseline28.06\u00b14.05 0.888\u00b10.087 -PWC-Net [29]29.93\u00b13.47 0.938\u00b10.057 39.5EpicFlow [27]30.07\u00b13.49 0.940\u00b10.057 96.3\u00d7106 SepConv [26]32.91\u00b14.60 0.954\u00b10.054 10.9\u00d7104 Super SloMo [14]33.64\u00b14.66 0.958\u00b10.048 13.7\u00d7102 Ours (direct pred.)33.97\u00b14.53 0.961\u00b10.044 34.7Ours (uniform weight)33.88\u00b14.68 0.959\u00b10.050 42.8Ours (kernel pred.)33.73\u00b14.31 0.961\u00b10.045 65.5Our Model 34.14\u00b14.65 0.963\u00b10.045 43.7 Table 1. Performance on our real test dataset, in which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S42",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "we com- pare our model to three of its ablated variants and \ufb01ve baseline algorithms. We primarily evaluate our model on the real test dataset described in Section 4.2, shown in Table 1. We report the mean PSNR and SSIM for the dataset, and note that our model produces the highest value of both out of all base- lines and ablations. Though at \ufb01rst glance the difference be- tween models may appear small, the unusually high PSNR of the \u201cnaive\u201d baseline serves to anchor these scores and suggests that small variations in scores are meaningful. The two optical \ufb02ow baselines are the lowest-performing tech- niques, with the two video frame interpolation techniques performing nearly as well as ours. However, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S43",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "gap in run- time between our model and the baseline techniques is quite substantial, as our model is 30\u00d7- 2,500\u00d7faster. This is par- tially due to our compact architecture and the fact that line prediction is amenable to a fast implementation, but is also because video frame interpolation techniques must predict a 33 frame sequence that is then averaged to produce a single image, and so necessarily suffer a 33\u00d7speed decrease. The reported runtimes of our model, its ablations, and the technique of SepConv [26] are the mean of 1000 runs on a GeForce GTX 1080 Ti, at our test set image reso- lution of 512 \u00d7512. The runtimes of PWC-Net [29] and Super SloMo [14] were reported by the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S44",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "authors of those papers, who graciously ran their code on our data using a NVIDIA Pascal TitanX (a faster GPU than the one used for our model). The runtime for EpicFlow [27] was ex- trapolated from the numbers cited in the paper, which were produced on a 3.6Ghz CPU. Reported times for the opti- cal \ufb02ow methods are underestimates of their true runtimes, as we only measure the time taken to generate their \ufb02ow \ufb01elds, and do not include the time taken to render images from those \ufb02ow \ufb01elds. The reduced performance of our \u201cuniform weight\u201d abla- tion appears to be due to its dif\ufb01culty in handling occlusions and motion boundaries, which appear to particularly bene\ufb01t from the learned sample",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S45",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "weights. This can be seen in Fig- ure 8(l), where our model appears to use its learned weights to blur around the occlusions of the basketball net webbing. The output of our model super\ufb01cially resembles an op- tical \ufb02ow algorithm, in that the line endpoint \u2206x i (x,y) pre- dicted at each pixel can be treated as a \ufb02ow vector. Though this is an oversimpli\ufb01cation (our model actually predicts a weighting for a set of points along this line and those weights may be zero, effectively shortening or shifting this line) it is illustrative to visualize our output as a \ufb02ow \ufb01eld and compare it to optical \ufb02ow algorithms, as in Figure 7. Because our model is trained solely for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S46",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "the task of syn- thesizing motion blur, its \u201c\ufb02ow\u201d often looks irregular and inaccurate compared to optical \ufb02ow algorithms, which are trained or designed to minimized end point error of with re- spect to scene motion. This difference manifests itself in a number of ways: our model assigns a near-zero \u201c\ufb02ow\u201d to pixels in large \ufb02at regions of the image, because blurring a \ufb02at region looks identical to not blurring a \ufb02at region and so our training loss is agnostic in these \ufb02at regions. Also, our model attempts to model the motion of things like shadows, which optical \ufb02ow algorithms are trained to ignore as they do not represent motion of the underlying physical object. This disconnect between apparent motion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S47",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "in an image and true motion in world geometry may explain why our opti- cal \ufb02ow baselines perform poorly on our task. This differ- ence between our model\u2019s learned \u201c\ufb02ow\u201d and explicit opti- cal \ufb02ow techniques is analogous to prior work on learning monocular depth cues using defocus blur as a supervisory cue [?]. As our test-set performance demonstrates, our model performs well on diverse cases, including a variety of scene content, types of motion, duration of blurs, and amounts of blur in input frames. However, our model is limited in its inability to handle motions larger than those in the training dataset (32 pixels) and (similarly to other techniques) its in- ability to render nonlinear motion blur. In the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S48",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "supplemental video we present results in which (a) Input images, averaged (b) Our Model\u2019s \u22061(x, y) (c) EpicFlow [27] (d) PWC-Net [29] Figure 7. A subset of our model\u2019s output can be visualized by using the endpoint of each pixel\u2019s predicted line as a \ufb02ow vector. Here we render our model\u2019s \u201c\ufb02ow \ufb01eld\u201d alongside two optical \ufb02ow algorithms. Our \u201c\ufb02ow \ufb01elds\u201d tend to look irregular, highlighting the difference between training for accurate motion blur synthesis and training for accurate motion estimation. our system has been used to add motion blur to video se- quences, by running on all pairs of adjacent video frames. 6. Conclusion We have presented a technique for synthesizing motion blurred images from pairs of unblurred images.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S49",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "As part of our neural network architecture we have proposed a novel line prediction layer, which is motivated by the optical prop- erties of motion blur, and which is capable of producing accurate motion blur even when faced with occlusion and complex motion. We have described a strategy for using frame interpolation techniques to generate a large-scale syn- thetic dataset for use in training our motion blur synthesis model. We additionally captured a ground truth test set of real motion blurred images with their corresponding input images, and with that we have demonstrated that our pro- posed model outperforms prior work in terms of accuracy and speed. Our approach is fast, accurate, and uses readily available imagery from videos or",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S50",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "\u201cbursts\u201d as input, and so provides a path for enabling motion blur manipulation in consumer photography applications, and for synthesizing the realistic training data needed by deblurring or motion estimation algorithms. References [1] Mart \u00b4\u0131n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe- mawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Mur- ray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete War- den, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Ten- sor\ufb02ow: A system for large-scale machine learning. OSDI, 2016. 4 [2] Apple. Use portrait mode on your iphone. https:// support.apple.com/en-us/HT208118, 2017. 2 [3] Steve Bako, Thijs V ogels, Brian Mcwilliams, Mark Meyer, Jan Nov \u00b4aK, Alex",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S51",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "Harvill, Pradeep Sen, Tony Derose, and Fabrice Rousselle. Kernel-predicting convolutional networks for denoising monte carlo renderings. SIGGRAPH, 2017. 3 [4] Jonathan T. Barron, Andrew Adams, YiChang Shih, and Car- los Hern\u00b4andez. Fast bilateral-space stereo for synthetic de- focus. CVPR, 2015. 2 [5] Benedicte Bascle, Andrew Blake, and Andrew Zisserman. Motion deblurring and super-resolution from an image se- quence. ECCV, 1996. 1 [6] Ayan Chakrabarti. A neural approach to blind motion de- blurring. ECCV, 2016. 1 [7] Ayan Chakrabarti, Todd E. Zickler, and William T. Freeman. Analyzing spatially-varying blur. CVPR, 2010. 1 [8] Shengyang Dai and Ying Wu. Motion from blur. CVPR, 2008. 1 [9] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T. Roweis, and William T. Freeman. Removing camera shake",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S52",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "from a single photograph. SIGGRAPH, 2006. 1 [10] Micha \u00a8el Gharbi, Gaurav Chaurasia, Sylvain Paris, and Fr\u00b4edo Durand. Deep joint demosaicking and denoising. SIG- GRAPH Asia, 2016. 5 [11] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian Reid, Chunhua Shen, Anton van den Hengel, and Qinfeng Shi. From motion blur to motion \ufb02ow: A deep learning solu- tion for removing heterogeneous motion blur. CVPR, 2017. 1 [12] Samuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T. Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. Burst photography for high dynamic range and low-light imaging on mobile cameras.SIGGRAPH Asia, 2016. 2 [13] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu- tion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S53",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "of optical \ufb02ow estimation with deep networks. CVPR, 2017. 2 [14] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik G. Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. CVPR, 2018. 2, 4, 6, 7, 8, 11, 12, 13, 14 [15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 6 [16] Yann Lecun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L.D. Jackel. Backpropagation ap- plied to handwritten zip code recognition. Neural computa- tion, 1989. 2 [17] Anat Levin. Blind motion deblurring using image statistics. NIPS, 2006. 1 [18] Ce Liu. Beyond Pixels: Exploring New Representations and Applications for Motion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S54",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "Analysis. PhD thesis, MIT, 2009. 5 [19] Andrew L. Maas, Awni Y . Hannun, and Andrew Y . Ng. Rec- ti\ufb01er nonlinearities improve neural network acoustic models. ICML, 2013. 4 [20] Morgan McGuire, Padraic Hennessy, Michael Bukowski, and Brian Osman. A reconstruction \ufb01lter for plausible mo- tion blur. ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, 2012. 3, 6 [21] Ben Mildenhall, Jonathan T. Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll. Burst denoising with kernel prediction networks. CVPR, 2018. 3 [22] Fernando Navarro, Francisco J. Sern, and Diego Gutierrez. Motion Blur Rendering: State of the Art. Computer Graph- ics Forum, 2011. 2, 6 [23] Shree K Nayar and Moshe Ben-Ezra. Motion-based motion deblurring. TPAMI, 2004. 1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S55",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "[24] Simon Niklaus and Feng Liu. Context-aware synthesis for video frame interpolation. CVPR, 2018. 2 [25] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter- polation via adaptive convolution. CVPR, 2017. 2, 3, 6 [26] Simon Niklaus, Long Mai, and Feng Liu. Video frame inter- polation via adaptive separable convolution. ICCV, 2017. 2, 4, 5, 6, 7, 11, 12, 13, 14 [27] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow. CVPR, 2015. 6, 7, 8, 11, 12, 13, 14 [28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. MICCAI, 2015. 4 [29] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S56",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "optical \ufb02ow using pyramid, warping, and cost volume. CVPR, 2018. 2, 6, 7, 8, 11, 12, 13, 14 [30] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn- ing a convolutional neural network for non-uniform motion blur removal. CVPR, 2015. 1 [31] Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori Kanazawa, Robert Carroll, Yair Movshovitz- Attias, Jonathan T. Barron, Yael Pritch, and Marc Levoy. Synthetic depth-of-\ufb01eld with a single-camera mobile phone. SIGGRAPH, 2018. 2 [32] Jacob Walker, Abhinav Gupta, and Martial Hebert. Dense optical \ufb02ow prediction from a static image. ICCV, 2015. 1 [33] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William T Freeman. Video enhancement with task-oriented \ufb02ow. arXiv, 2017. 2, 4 A.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S57",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "Additional Results Because our synthetic dataset contains a validation set, we report performance of our model and its ablations in Ta- ble 2. We do not report the performance of our baseline techniques, as their performance on this synthetic data is unlikely to be meaningful when compared to our real test dataset, and also because some of our baselines needed to be run by the respective authors of each paper whom we did not wish to burden by requesting they process 15000 images in addition to our test set. In the table we see that the relative ordering of our model with respect to its abla- tions is consistent with their ordering in our test-set, though absolute performance is consistently",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S58",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "higher. Algorithm PSNR SSIM Ours (direct pred.) 35.371 0.9854 Ours (kernel pred.) 36.762 0.9873 Ours (uniform weight) 37.217 0.9866 Our Model 37.673 0.9881 Table 2. Performance of our model and its ablations on the valida- tion set of our synthetic dataset. See Figures 8-11 for additional results on our real dataset, in which we compare our model against a set of ablations as well as a set of optical \ufb02ow and video frame interpolation methods that could also be used to synthesize motion blurred images. (a) Input image 1 (b) Input image 2 (c) Non-input intermediate frames (d) Ground-truth motion blur (e) PWC-Net [29] (f) EpicFlow [27] (g) SepConv [26] (h) Super SloMo [14] (i) Ours (direct pred.) (j) Ours",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S59",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "(uniform weight) (k) Ours (kernel pred.) (l) Our Model Figure 8. Results for one scene from our test dataset. The ground truth image (d) is the sum of the input images (a) & (b) and of the frames between those two images (c). We programmatically select the three non-overlapping 32 \u00d7 32 sub-images with maximal variance across all frames in (c) and present crops of those regions, rendered with nearest-neighbor interpolation and sorted by their y-coordinates. We compare our model (l) against four baselines (e)-(h), and three ablations (i)-(k). Note that all techniques are unable to accurately blur the spinning wheel, which violates our model\u2019s and optical \ufb02ow\u2019s assumption of linear motion. (a) Input image 1 (b) Input image 2",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S60",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "(c) Non-input intermediate frames (d) Ground-truth motion blur (e) PWC-Net [29] (f) EpicFlow [27] (g) SepConv [26] (h) Super SloMo [14] (i) Ours (direct pred.) (j) Ours (uniform weight) (k) Ours (kernel pred.) (l) Our Model Figure 9. Additional results in the same format as Figure 8. (a) Input image 1 (b) Input image 2 (c) Non-input intermediate frames (d) Ground-truth motion blur (e) PWC-Net [29] (f) EpicFlow [27] (g) SepConv [26] (h) Super SloMo [14] (i) Ours (direct pred.) (j) Ours (uniform weight) (k) Ours (kernel pred.) (l) Our Model Figure 10. Additional results in the same format as Figure 8. (a) Input image 1 (b) Input image 2 (c) Non-input intermediate frames (d) Ground-truth motion blur (e) PWC-Net",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1811_11745v2:S61",
      "paper_id": "arxiv:1811.11745v2",
      "section": "abstract",
      "text": "[29] (f) EpicFlow [27] (g) SepConv [26] (h) Super SloMo [14] (i) Ours (direct pred.) (j) Ours (uniform weight) (k) Ours (kernel pred.) (l) Our Model Figure 11. Additional results in the same format as Figure 8.",
      "page_hint": null,
      "token_count": 37,
      "paper_year": 2018,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9544314628652555,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 14,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 2527,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5319,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5177,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4041,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4892,
        "empty": false
      },
      {
        "page": 6,
        "chars": 5815,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2935,
        "empty": false
      },
      {
        "page": 8,
        "chars": 4639,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5212,
        "empty": false
      },
      {
        "page": 10,
        "chars": 1209,
        "empty": false
      },
      {
        "page": 11,
        "chars": 900,
        "empty": false
      },
      {
        "page": 12,
        "chars": 320,
        "empty": false
      },
      {
        "page": 13,
        "chars": 321,
        "empty": false
      },
      {
        "page": 14,
        "chars": 321,
        "empty": false
      }
    ],
    "quality_score": 0.9544,
    "quality_band": "good"
  }
}