{
  "paper": {
    "paper_id": "arxiv:2403.15789v1",
    "title": "In-Context Matting",
    "authors": [
      "He Guo",
      "Zixuan Ye",
      "Zhiguo Cao",
      "Hao Lu"
    ],
    "year": 2024,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "We introduce in-context matting, a novel task setting of image matting. Given a reference image of a certain foreground and guided priors such as points, scribbles, and masks, in-context matting enables automatic alpha estimation on a batch of target images of the same foreground category, without additional auxiliary input. This setting marries good performance in auxiliary input-based matting and ease of use in automatic matting, which finds a good trade-off between customization and automation. To overcome the key challenge of accurate foreground matching, we introduce IconMatting, an in-context matting model built upon a pre-trained text-to-image diffusion model. Conditioned on inter- and intra-similarity matching, IconMatting can make full use of reference context to generate accurate target alpha mattes. To benchmark the task, we also introduce a novel testing dataset ICM-$57$, covering 57 groups of real-world images. Quantitative and qualitative results on the ICM-57 testing set show that IconMatting rivals the accuracy of trimap-based matting while retaining the automation level akin to automatic matting. Code is available at https://github.com/tiny-smart/in-context-matting",
    "pdf_path": "data/automation/papers/arxiv_2403.15789v1.pdf",
    "url": "https://arxiv.org/pdf/2403.15789v1",
    "doi": null,
    "arxiv_id": "2403.15789v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.073637+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2403_15789v1:S1",
      "paper_id": "arxiv:2403.15789v1",
      "section": "body",
      "text": "In-Context Matting He Guo Zixuan Ye Zhiguo Cao Hao Lu * School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, China {hguo01,hlu}@hust.edu.cn \u2026 Target imagesReference\u2026 Target imagesReference\u2026 Target imagesReference\u2026 \u2026 \u2026 AnimalHumanNatural Images AutoMatting In-ContextMatting Auxiliary-InputBasedMatting N*image + 1*prompt + 1*model      N*predictions N*image + M*model      N*predictionsN*image + N*prompt      N*predictions Mask Points Scribbles Figure 1. In-Context Matting. This novel task setting for image matting enables automatic natural image matting of target images of a certain object category conditioned on a reference image of the same category, with user-provided priors such as masks and scribbles on the reference image only. Notice that, our approach exhibits remarkable cross-domain matting quality.",
      "page_hint": null,
      "token_count": 110,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S2",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "We introduce in-context matting, a novel task setting of image matting. Given a reference image of a certain fore- ground and guided priors such as points, scribbles, and masks, in-context matting enables automatic alpha estima- tion on a batch of target images of the same foreground cat- egory, without additional auxiliary input. This setting mar- ries good performance in auxiliary input-based matting and ease of use in automatic matting, which finds a good trade- off between customization and automation. To overcome the key challenge of accurate foreground matching, we intro- duce IconMatting, an in-context matting model built upon a pre-trained text-to-image diffusion model. Conditioned on inter- and intra-similarity matching, IconMatting can *Corresponding author make full use of reference context to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S3",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "generate accurate tar- get alpha mattes. To benchmark the task, we also intro- duce a novel testing dataset ICM- 57, covering 57 groups of real-world images. Quantitative and qualitative results on the ICM-57 testing set show that IconMatting rivals the accuracy of trimap-based matting while retaining the au- tomation level akin to automatic matting. Code is avail- able at https://github.com/tiny-smart/in- context-matting. 1. Introduction Image matting has been a long-standing problem in vision and graphics [3]. It typically requires estimating an accurate alpha matte by solving a so-called matting equation I = \u03b1F + (1\u2212 \u03b1)B , (1) 1 arXiv:2403.15789v1 [cs.CV] 23 Mar 2024 where I is the 3-channel RGB image, F, B, and \u03b1 are the 3-channel foreground, the 3-channel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S4",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "background, and the 1-channel alpha matte. The matting equation, however, is highly ill-posed, due to the need to infer 7 unknowns from 3 observations. Prior art has come up with different ways to reduce un- certainties in matting such as using trimaps [7, 17, 19\u2013 21, 34, 40], scribbles [41], or a even known background [18, 31]. These approaches are given the name auxiliary input- based matting [15] in modern matting literature. Indeed, these matting models, particularly trimap-based ones, have achieved remarkable accuracy. They are in some way user- unfriendly as an auxiliary input should be provided with each image in practice, which significantly harms matting efficiency and user experience. Recently another stream of work attempts to abandon any auxiliary",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S5",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "input com- pletely and forms a new paradigm called automatic mat- ting [4, 10, 12, 13, 22, 33, 45]. Despite their inherent ad- vantages, these automatic matting models are narrowed to specific object categories, such as humans [4, 10, 22, 33], animals [13], and salient objects [12, 45]. This can render poor generalization in natural scenes, infeasibility to tackle general object categories, and unawareness of foreground of interest. Hence, there seems an obvious gap between accuracy and efficiency and between customization and automation. An interesting question is that: Can the auxiliary input- based matting be optimized to enhance the efficiency, while also maintaining guidance for matting targets with suffi- cient automation, thereby harmonizing the two existing mat- ting paradigms? In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S6",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "this paper, we introduce in-context matting, a novel task setting of image matting, where a reference input can provide guidance for a batch of target images with simi- lar foregrounds. The alpha mattes of the target batch are predicted by leveraging the contextual information from the reference input. Fig. 1 provides an example, where a cat marked in the reference image enables the extraction of cats from all target images, regardless of the background or do- main. This novel task setting relieves users from providing auxiliary input for each image. Instead, by specifying the matting target only in a single reference image, the alpha mattes of the entire batch images could be predicted. Given the reference guidance, in-context matting can",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S7",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "also gather sufficient contextual information, leading to higher accu- racy and adaptability than fully automatic matting. This setting therefore combines the most notable features of au- tomatic and auxiliary input-based matting, finding a good trade-off between them. Technically, we confront a primary challenge inherent to in-context matting, i.e., how to leverage the reference input to accurately identify the corresponding target foreground. While SegGPT [37] has explored image segmentation with contextual information, its in-context coloring framework is not suitable for image matting. In this work, we approach this challenge as a problem of region-to-region matching. In particular, recent advances [23, 28, 32] in generative dif- fusion models have demonstrated emergent capabilities in discriminative tasks like segmentation [39] and correspon- dence [35].",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S8",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "Given that our region-to-region matching is an inherent aspect of correspondence, and noting the par- allels between image segmentation and image matting, we explore the applicability of pretrained diffusion models for in-context matting. We therefore introduce IconMatting, a model based on the pre-trained Stable Diffusion model [28], specialized for in-context matting. Given a reference image along with its corresponding foreground of interest as context, the tar- get alpha matte could be inferred by exploiting the feature correspondence from Stable Diffusion such that the target foreground is matched conditioned on the correspondence. However, the matching is often sparse and insufficient to represent the entire target area. To address this, the intra- image similarity, based on the self-attention maps of Stable Diffusion,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S9",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "is additionally used to supplement the missing parts. By leveraging both inter- and intra-image similari- ties, informative guidance of the matting target would be acquired. Finally, any off-the-shelf matting heads can be used to predict the alpha matte. Since the task setting is different from existing mat- ting benchmarks, we introduce a new testing dataset named ICM-57 to offer a broad and thorough validation of in- context matting. This dataset encompasses 57 contextually- aligned image groups; each comprising images in the real world and has either the same category or the same instance of different views, thereby encompassing a rich variety of in-context scenarios, which ensures a comprehensive test of a model to tackle various context. Through extensive experiments on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S10",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "the ICM- 57 and AIM-500 [11] datasets, we showcase the potential of in- context matting and IconMatting. The results indicate that IconMatting, while retaining the automation level akin to automatic matting, rivals the accuracy of trimap-based mat- ting, underscoring the value of in-context matting as a promising direction for image matting. Our contributions include: \u2022 We introduce in-context matting, a novel task setting of image matting that takes advantages of both automatic and auxiliary input-based matting; \u2022 IconMatting: an effective in-context matting model based on Stable Diffusion. \u2022 ICM- 57: a novel dataset and the evaluation framework for in-context matting. 2 2. Related Work We review work related to image matting and in-context learning in vision. Image Matting. Image matting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S11",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "approaches can be coarsely categorized into auxiliary input-based matting and automatic matting. Auxiliary input-based matting requires user input. The user input can be in the form of a trimap [7, 17, 19, 20, 34, 40], scribbles [41], a background image [18, 31], a coarse mask [25, 44], or even a text description [16]. Despite their effectiveness, they require significant manual effort to provide the auxiliary inputs. Automatic matting [4, 11, 13, 26, 45] predicts the alpha matte without any user intervention. They typically assume salient or certain foregrounds that are implicitly defined by the training dataset. The network structures used in auto- matic matting can be divided into two groups: one-stage network with global guidance [26] and parallel multi-task network",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S12",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "[11, 33]. Some recent work has also introduced transformer structures into automatic matting [22]. While both auxiliary input-based and automatic matting have been studied comprehensively, a paradigm that combines the ef- ficiency of automatic matting and the precision of auxiliary input-based matting has not yet been explored. We fill this gap with in-context matting. In-Context Learning in Vision. In-context learning, ini- tially a concept in natural language processing [5], is now made popular in computer vision. It allows models to fast adapt to a variety of tasks with minimal examples. Bar et al. [2] first proposes an in-context learning frame- work using inpainting with discrete tokens on figures and infographics from vision articles, demonstrating applica- tions in foreground segmentation, single",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S13",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "object detection, and colorization. Painter [36] adopts masked image model- ing to perform in-context learning with supervised datasets, achieving highly competitive results on seven diverse tasks. More recently, SegGPT [37], which segments everything in context by unifying different segmentation tasks into an in- context coloring framework. Prompt Diffusion [6] presents a diffusion-based generative framework to enable in-context learning across various tasks. Additionally, Flamingo [1], a family of visual language models, shows rapid adaptation to a variety of image and video tasks with few-shot learn- ing capabilities. These models showcase the potential of in-context learning in addressing diverse vision tasks. The concept of in-context learning, while being transfor- mative in other areas, has not yet impacted the field of im- age",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S14",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "matting. Although SegGPT achieves in-context image segmentation but is limited to coarse levels, lacking in semi- transparency handling. Our IconMatting first introduces in- context learning into image matting, enhancing both the ef- ficiency of auxiliary input-based matting and the precision of automatic matting. Task setting auto auto with ref-input real-world generalization #ref-input Aux \u2717 \u2717 good #images Auto \u2713 - poor zero In-context \u2717 \u2713 good one Table 1. Comparison between in-context matting and two ex- isting image matting paradigms. \u201cAux\u201d and \u201cAuto\u201d are abbre- viations for auxiliary input-based matting and automatic matting, respectively. In-context matting requires only a single reference input to achieve the automation of automatic matting and the gen- eralizability of auxiliary input-based matting. Our work is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S15",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "also related to image co-segmentation [29]. This task aims to segment common objects within a pair of contextual images. However, unlike in-context matting, where users can specify the matting target, co-segmentation operates without user input, predicting rough binary masks delineating common objects across image pairs. 3. In-Context Matting with Diffusion Models We begin with the problem setup, then present our proposed in-context matting model, i.e., IconMatting. 3.1. Problem Setup The objective of in-context matting is to extract the alpha mattes {\u03b1i}N of a specified foreground category from a collection of input images {Ii}N . Through user interac- tion, the matting target is indicated by a reference image Ir and a corresponding binary region of interest (RoI) map MRoI. The RoI",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S16",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "map can take the form of a mask, scribbles, or points as exemplified in Fig. 1. Notably, the reference image can either be a part of the input collection or an en- tirely separate image. When the input image collection has only a single image, users can treat that image as the ref- erence image. In this case, in-context matting degenerates into image matting guided by user interaction. Given {Ii}N and (Ir, MRoI), in-context matting is for- mulated as predicting the alpha matte {\u03b1i}N in {Ii}N of the matting targets informed by (Ir, MRoI). In the context of in-context matting, when provided with a reference input, it becomes an automatic matting system targeted towards a specific foreground. The comparison between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S17",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "in-context matting and existing task settings for image matting is de- tailed in Table 1. 3.2. Overall Architecture Here we first present the overall framework of IconMatting. As shown in Fig. 2, IconMatting is comprised of three com- ponents: a feature extractor, an in-context similarity mod- ule, and a matting head. The feature extractor is responsible for obtaining the RoI features from the reference image, the features and self- attention maps from the target image. They are then fed into 3 v Diffusion UNet Diffusion UNet shared Encoder Self-attention maps Diffusion UNet Inter- feature Inter-features In-context query flatten inter-similarity intra-similarity Conv Fusion Intra- features Detail Decoder In-Context Similarity element-wise multiplication \u2026 Reference Image & Prompt Target Images \u2026 inter-similarity intra-similarity",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S18",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "Matting Head Figure 2. IconMatting integrates a Stable Diffusion-derived feature extractor, an in-context similarity module, and a matting head. It processes a target image It, a reference image Ir, and an RoI map MRoI. Both reference and target image features and target self-attention maps are extracted and used. In-context similarity uses the in-context query from the reference image to create a guidance map, which, combined with self-attention maps, assists in locating the target object. The matting head finally generates the target alpha matte. the in-context similarity module, the core of the framework. The module further consists of inter- and intra-similarity sub-modules: the former leverages the reference RoI fea- tures as an in-context query to derive a guidance map from the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S19",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "target features; the latter integrates the guidance map with multi-scale self-attention maps to obtain guidance for the matting head. Finally, the matting head uses this synthe- sized guiding information and the target features to generate the alpha matte of the target image. 3.3. In-Context Feature Extractor Backbone Selection. As outlined in Section 1, we con- ceptualize the core challenge of in-context matting \u2013 lever- aging the reference context to accurately identify the tar- get foreground \u2013 as a region-to-region matching problem. Therefore, if the features derived from a backbone natu- rally possess correspondence capabilities, referred to as in- context features, they would facilitate the implementation of in-context matting. Tang et al. [35] have found that the text- to-image generative model,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S20",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "Stable Diffusion [28], trained on large-scale text-image paired datasets, exhibit emerge- nent capabilities for both geometric and semantic corre- spondence. It can perform point-to-point correspondence between images across instances, classes, and even domains with simple cosine similarity. Inspired by this observation, we leverage Stable Diffusion as a feature extractor to imple- ment in-context matting. Preliminary on Stable Diffusion. Recent advances in diffusion models, e.g., Stable Diffusion [28], have shown impressive results in both generative and discriminative tasks. Being a feature extractor, Stable Diffusion encodes an image x0 into a latent space, denoted by z0, which, through a noise process defined by {\u03b1t}T , transforms into zt = \u221a\u03b1tz0 + (\u221a1 \u2212 \u03b1t)\u03b5, where \u03b5 \u223c N(0, I) is the randomly-sampled",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S21",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "noise. The latent representation zt un- dergoes forward propagation in a U-Net f\u03b8, generating multi-scale features {Fl}L and self-attention maps {Al}L, which can later be exploited for downstream tasks. Icon- Matting uses the capabilities of Stable Diffusion and both reference and target images to extract multi-scale features and self-attention maps to enhance feature representation. 3.4. In-Context Similarity In-context similarity plays a key role in our model, because the quality of inferred alpha mattes highly depends on the output of this module. In particular, the in-context sim- ilarity module is designed to identify the potential target foreground taking the reference RoI into account, thereby guiding the prediction of the target alpha matte. Accord- ing to our observations, both the reference-target similarity",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S22",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "and target-target similarity matter for locating the potential target foreground. These correspond to the proposed inter- similarity and intra-similarity sub-modules. Observation. The core challenge in in-context matting is a semantic correspondence problem. Given Stable Diffu- sion being the feature extractor, one can associate points of the foreground areas between the reference and target images using the emergent feature correspondence. How- ever, due to the inherent reference-target foreground differ- ence, a rigorous one-to-one mapping of all points between the two areas is unfeasible. There would always be some unmatched outliers, resulting in holes in the matting target 4 (a) Outliers in point to point correspondence 1 2 3 4 5 6 1 2 3 4 5 6 (b) Self-attention of random",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S23",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "points Figure 3. Observations on the inter- and intra similarities. area of the target image, as illustrated in Fig. 3. Since only a subset of points are matched, the goal is changed to how to expand these matched points to cover the entire target foreground area. To address this, we look for other points sharing similar semantic meaning with this subset of points. Intra-image similarity is therefore consid- ered. Intuitively, the self-attention maps from Stable Diffu- sion reflect the similarities between different image patches. As shown in Fig. 3, by randomly sampling a small number of points from the target area and simply summing over the corresponding self-attention maps, the whole target fore- ground can be revealed. Based on this",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S24",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "observation, we use the self-attention maps as intra-image information to sup- plement the inter-image matching results. Inter-Similarity. Formally, given features {Fr l }L and {Ft l }L extracted from the reference image Ir and the tar- get image It, respectively, the layer with the best corre- spondence capability is selected following DIFT [35], de- noted by Fr inter for the reference and Ft inter for the target. Then, features corresponding to the RoI MRoI in Fr inter are extracted and formulated as the in-context query {Qk}K, where K is the length of the query. Qk takes the form Qk = R (k) , (2) R = Fr inter \u2299 MRoI , (3) where \u2299 is the element-wise product and R",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S25",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "(k) denotes the k-th non-zero element of R. Further, {Qk}K is used to compute similarity with Ft inter, identifying regions on It that correspond to the RoI of Ir, formulated as Sk = softmax( Qk \u00b7 Ft inter T \u221a d ) . (4) The similarity map is denoted by {Sk}K, and the mean of all such similarity maps yields S, which measures the degree of similarity between different locations on It and the RoI in Ir, serving as the first intermediate output of in- context similarity. Intra-Similarity. As noted in our observations (Fig. 3),S is typically sparse. Although the target matting region on It is partially covered by S, it is insufficient to guide al- pha prediction. Here we",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S26",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "further design the intra-similarity sub-module to leverage the internal similarity within It to propagate S into a more precise representation of the RoI on It. During feature extraction of It, self-attention maps {Al}L representing its internal similarity are also retained, serving as the input for intra-similarity. This sub-module uses S as a weight to the self-attention maps, thereby gen- erating guiding information that accurately represents the matting target on It, denoted by {S \u2032 l }L. Mathematically, the intra-similarity matching is expressed by S \u2032 l = Al \u2299 S. (5) 3.5. Matting Head The success of ViTMatte [42] implies that the information of original image is important during decoding. Following this practice, in our matting head, the original image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S27",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "is con- catenated and decoded with outputs from previous modules. The guidance map from the in-context similarity module and the intra-features from the backbone are merged and re- fined using a convolutional feature fusion block, including a series of convolution, normalization, and activation layers. The output multi-scale in-context features are progressively merged using a series of fusion layers which comprise up- sampling, concatenation, convolution, normalization, and activation layers. Then, following ViTMatte [42], details from the original image are extracted and combined with the merged feature in a detail decoder, enhancing the de- tails of alpha matte. This matting head effectively melds contextual information with original image details, yielding the generation of a highly precise and refined alpha matte. 3.6. Reference-Prompt",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S28",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "Extension In addition to the mask prompts, points and scribbles can also be transformed into RoI masks. However, these prompts yield less comprehensive in-context queries com- pared with mask prompts. To enhance them, we propose an extension of reference prompt to enrich the in-context queries derived from point and scribble prompts. Since the self-attention maps from the backbone reflect the similarities across regions, we leverage the attention maps of the reference images to expand the RoI mask. This is achieved by including regions in the attention maps that are similar to the prompt locations. Specifically, for each prompt point, the top m points with the highest responses in their corresponding attention maps are integrated into the RoI mask additionally, thus",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S29",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "enriching the in-context query. 5 Figure 4. ICM-57 examples. The dataset encompasses fore- ground subjects including human, animals, plants, and various common objects. It contains both instances from the same cate- gory and the same entity. 4. Results and Discussion 4.1. Datasets To facilitate in-context matting, we establish a hybrid train- ing set, along with a test set, ICM- 57. The existing AIM- 500 dataset was also reorganized to meet the testing re- quirements of in-context matting. In particular, in-context matting requires to organize images into groups where the annotated foregrounds share categories or instances. Such organization allows for random selection of reference and target images within groups during training. In the test set, one or more images in each",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S30",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "group are designated as refer- ence images. Mixing In-Context Training Sets. We selected the RM- 1K dataset [38]. However, such a dataset was insuffi- cient for training. Therefore, a subset of the Open Images dataset [9], focusing on image segmentation, was also em- ployed. The two formed a mixed training set tailored to in-context matting. Both datasets were reorganized. For the RM-1K dataset, we divided it into 222 groups. A subset of 14, 000 images from the Open Images dataset were chosen as well. In the original dataset, each image came with one or more annotations for image segmenta- tion; each corresponds to an object instance. We aggregated these annotations by category, ensuring that the annotations include all instances of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S31",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "the corresponding category. They subsequently formed context groups that met the require- ments of in-context matting. As a result, a mixed training set of 15, 000 images and 450 groups was created. ICM-57 Testing Set. To assess the performance of our model, we constructed the first testing dataset for in- context matting, named ICM-57, which comprises57 image groups that form various real-world contexts. This dataset was created by using the instance-segmented dreambooth dataset [30], to which we supplemented high-precision al- pha matte annotations. Additionally, we reviewed the ex- isting AIM-500 dataset, selected a subset, and categorized these into groups according to their classes to supplement the in-category groups. Examples of the testing set are shown in Fig 4. AIM-500. We",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S32",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "also report performance on the AIM-500 dataset [11]. This enables us to compare our matting model with other public matting results on this dataset. 4.2. Implementation Details Architecture. We employ the U-Net architecture from Stable Diffusion v2-1-v [28]. For feature extraction, the time step of the diffusion process is set to t = 0by default, with an empty string used for conditional input. U-Net has 11 decoder blocks; we extract feature maps from the 5-th, 8-th, and 11-th blocks as the intra-features and ones from the 5-th block as the inter-features. Training Details. We employ distinct loss functions for matting and segmentation, respectively. For matting, we use a combination of \u21131 loss, Laplacian loss, and Gradi- ent loss. For segmentation,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S33",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "we only use the \u21131 loss. To leverage the segmentation dataset while reducing the im- pact of imprecise edge annotations, we adopt the approach from HIM [33] that only backpropagates the loss from the confident areas. During training, the learning rate is set to0.0004 and the batch size is 8. The input images are randomly cropped to a size of 768 \u00d7768 pixels. To prevent deviation from the pre- trained model space in modeling real images, no additional data augmentation is used. We train IconMatting for20, 000 iterations using the AdamW optimizer. Evaluation. We employ the four widely used matting metrics: SAD, MSE, Grad and Conn [27]. Lower values imply higher-quality mattes. In particular, MSE is scaled by a factor",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S34",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "of 1 \u00d7 10\u22123. To reduce randomness, each method is tested for three rounds, where the metrics were averaged. For each group of images, the reference inputs were fixed and consistently used. This minimizes the variations in reference inputs, al- lowing for a more scientific and reliable assessment. 4.3. Main Results Comparison with In-Context Segmentation Models. To explore the effectiveness and superiority of our model, we selected SegGPT [37] and SEEM [47], which also op- erates under in-context learning, as baselines in the image segmentation domain. From Table 2, on the ICM- 57 and AIM-500 datasets, under the same experimental setup (one mask per group of images), our model significantly outper- forms the baselines across all metrics. It is important",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S35",
      "paper_id": "arxiv:2403.15789v1",
      "section": "abstract",
      "text": "to note that due to their distinct task set- ting ( i.e., segmentation rather than matting), the lack of high-quality annotations (e.g., alpha mattes) and the meth- ods (i.e., mask classification rather than alpha matte regres- sion), most of these models have overlooked pixel-level text-semantic alignment and are unable to produce fine- grained masks, as illustrated in Fig. 5. 6 SegGPT OursAIM MatAny ICMP GT Image Ref-input Figure 5. Qualitative results of different image matting methods.Our method can predict the alpha matte of the matting target specified by the reference input, offering notable prediction accuracy while avoiding interference from unrelated foreground elements.",
      "page_hint": null,
      "token_count": 102,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S36",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "MSE SAD GRAD CONN MSE SAD GRAD CONN SegGPT 0.0198 38.81 28.61 18.61 0.0391 42.65 41.95 26.69 SEEM 0.0292 64.28 37.54 23.64 0.0425 114.23 74.51 74.32 Ours 0.0081 19.12 18.65 11.21 0.0062 18.65 15.51 10.98 Table 2. Comparison with in-context segmentation models. Comparison with Automatic and Auxiliary Input-Based Matting Models. We further compare the performance of our IconMatting with both automatic and auxiliary input-based matting models on the ICM- 57 and AIM-500 datasets. Automatic matting methods such as LF [46] and AIM [11] lack specific auxiliary information about the matting target, often produce poor alpha mattes, showing a significant performance gap compared with our model. MGM [44] and MGMiW [25] use a mask for each image as auxiliary input to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S37",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "specify the matting target. Although our",
      "page_hint": null,
      "token_count": 6,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S38",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "of images, it still outperforms MGM in various metrics. Vit- Matte [42], a trimap-based image matting method, neces- sitates manually annotating a trimap for the foreground of each image, making it the performance upper bound for our in-context matting. Nevertheless, the performance of Icon- Matting is on par with VitMatte, underscoring its efficacy and competitiveness in image matting. Comparison with Interactive Matting Models. Re- cently, with the advent of SAM, some researchers have de- signed interactive matting models, such as MatAny [43] and MAM [14], based on this generic image segmenta- tion model. Inspired by this, we also designed an In- Context Matting Pipeline (ICMP) with three stages: cor- respondence, segmentation, and matting, serving as one of the baselines. ICMP",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S39",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "is a combination of existing models, with details available in supplementary materials. For MatAny and MAM, we compare them under three types of interactions: points, scribbles, and masks. On the ICM-57 testing dataset, both baselines receive interac- tion information for each image within a group, whereas IconMatting only receive interaction information from one image in the group to indicate the matting target. De- spite reducing the amount of human interaction, IconMat- ting achieves slightly better performance over the baselines. Limited by the interaction modalities in ICMP, our model is only compared with it under the point interaction setting. Our end-to-end model outperforms the combined ICMP. In ICMP, the cues for the matting target degrade to points during the correspondence phase",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S40",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "of the pipeline, of- ten resulting in sparse information, making our end-to-end approach more effective. 4.4. Ablation Study Different Modules. To validate different modules, we conducted ablation studies in Table 5. Among the four components, the presence or absence of inter- and intra- similarity plays a crucial role in performance. Without intra-similarity, the performance of the model significantly worsens across all four metrics. If both inter- and intra- similarity are absent, the model degenerates to directly pre- dicting the alpha matte from the image, losing the infor- mation source for the specified matting target, and thus the performance markedly deteriorates. 7",
      "page_hint": null,
      "token_count": 101,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S41",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "MSE SAD GRAD CONN MSE SAD GRAD CONN MGM 1 mask per image 0.0341 84.25 61.84 30.21 0.0268 71.91 23.37 21.97 VitMatte 1 trimap per image 0.0030 16.16 14.28 11.14 0.0038 18.79 14.22 12.47 MGMiW 1 mask per image \u2013 \u2013 \u2013 \u2013 0.0030 16.72 14.68 12.02 LF auto 0.0811 205.68 69.53 195.63 0.0667 191.74 64.51 181.26 AIM auto 0.0265 65.07 55.56 25.74 0.0183 48.09 47.58 21.74 Ours 1 mask per group of images 0.0081 19.12 18.65 11.21 0.0062 18.65 15.51 10.98 Table 3. Comparison with automatic and auxiliary input-based matting models.",
      "page_hint": null,
      "token_count": 92,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S42",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "MSE SAD MSE SAD MSE SAD MatAny \u2717 0.0651 129.67 0.0512 115.21 0.0412 95.26 MAM \u2717 0.0149 41.23 0.0141 40.23 0.0109 29.65 ICMP \u2713 0.0112 39.65 \u2013 \u2013 \u2013 \u2013 Ours* \u2717 0.0061 15.28 0.0059 15.97 0.0029 15.28 Ours \u2713 0.0124 23.21 0.0105 24.56 0.0081 19.12 Table 4. Comparison with interactive matting models. In the penultimate row, our method is provided with guidance informa- tion for every image, reducing to an auxiliary input-based method. Our method outperforms automatic methods and some of the aux- iliary input-based methods, and its performance is comparable to that of the trimap-based method, VitMatte. INTER INTRA MF SD MSE SAD GRAD CONN \u2713 \u2713 \u2713 \u2713 0.0081 19.12 18.65 11.21 \u2713 \u2717 \u2713 \u2713 0.0099",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S43",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "24.15 20.36 12.11 \u2717 \u2717 \u2713 \u2713 0.0315 40.32 31.56 19.53 \u2713 \u2713 \u2717 \u2713 0.0054 23.69 21.96 14.52 \u2713 \u2713 \u2713 \u2717 0.0071 18.56 19.54 12.61 Table 5. Ablation study on different modules. INTER, INTRA, MF, and SD respectively stand for inter-similarity, intra-similarity, multi-scale features, and segmentation dataset. Number of Reference Inputs. Intuitively, the more refer- ence inputs there are, the more likely the model is to identify the corresponding matting target. We explored the impact of the number of reference inputs on the performance of our model, as shown in Table. 6. On ICM- 57 test set, the performance improves as the number of reference inputs in- creases; however, the improvement almost ceases when the number of reference",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S44",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "inputs increases from 3 to 4. Therefore, we can conclude that appropriately increasing the number of reference inputs can enhance model performance, which is consistent with intuition. 4.5. Extension to Video Object Matting The technique of in-context matting is easily extendable to video object matting. The key is to use a frame of the video as a reference. For example, an object is marked in the first #Reference MSE SAD GRAD CONN 1 0.0085 19.58 19.14 12.61 2 0.0075 16.57 18.52 11.15 3 0.0070 15.48 17.56 10.56 4 0.0068 15.23 17.02 10.28 Table 6. Ablation study on the number of reference inputs. Figure 6. Extension to video object matting. frame of a video, which serves as a reference input and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S45",
      "paper_id": "arxiv:2403.15789v1",
      "section": "method",
      "text": "all frames of the video are treated as target images. With this setup, the model for in-context matting can predict the alpha matte for each frame of the video, visualized in Fig. 6. 5. Conclusion In this work, we introduce \u2018in-context matting\u2019, which en- ables automatic matting of foreground of interest on target images given a reference image and its prompt. We in- troduce IconMatting as a preliminary solution. Extensive",
      "page_hint": null,
      "token_count": 70,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S46",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "categories and scenes. Being the first work introducing this task, we believe it opens new possibilities for efficient and accurate image matting while reducing user effort, also en- hancing the versatility of image matting techniques. Acknowledgement. This work is supported by the Na- tional Natural Science Foundation of China under Grant No. 62106080. 8 References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716\u201323736, 2022. [2] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Glober- son, and Alexei Efros. Visual prompting via image inpaint- ing. Advances in Neural Information Processing Systems",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S47",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": ", 35:25005\u201325017, 2022. [3] Jagruti Boda and Dhatri Pandya. A survey on image matting techniques. In 2018 International Conference on Commu- nication and Signal Processing (ICCSP), pages 0765\u20130770. IEEE, 2018. [4] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, and Kun Gai. Semantic human matting. In ACM MM, pages 618\u2013626, 2018. [5] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. [6] Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Man- gas, and Luc Van Gool. Prompting diffusion representations for cross-domain semantic segmentation. arXiv preprint arXiv:2307.02138, 2023. [7] Qiqi Hou and Feng Liu. Context-aware image matting for si- multaneous",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S48",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "foreground and alpha estimation. InICCV, pages 4130\u20134139, 2019. [8] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White- head, Alexander C Berg, Wan-Yen Lo, et al. Segment any- thing. arXiv preprint arXiv:2304.02643, 2023. [9] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui- jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. Interna- tional Journal of Computer Vision, 128(7):1956\u20131981, 2020. [10] Jizhizi Li, Sihan Ma, Jing Zhang, and Dacheng Tao. Privacy- preserving portrait matting. In ACM MM, pages 3501\u20133509, 2021. [11] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep Automatic Natural Image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S49",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "Matting. In IJCAI, pages 800\u2013806, 2021. [12] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic natural image matting. arXiv preprint arXiv:2107.07235 , 2021. [13] Jizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. IJCV, 130(2):246\u2013266, 2022. [14] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything. arXiv preprint arXiv:2306.05399, 2023. [15] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep im- age matting: A comprehensive survey. arXiv preprint arXiv:2304.04672, 2023. [16] Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring image matting. In CVPR, pages 22448\u201322457, 2023. [17] Yaoyi Li and Hongtao Lu. Natural image matting via guided contextual attention. In AAAI, pages 11450\u201311457, 2020. [18] Shanchuan Lin, Andrey Ryabtsev, Soumyadip",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S50",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "Sengupta, Brian L Curless, Steven M Seitz, and Ira Kemelmacher- Shlizerman. Real-time high-resolution background matting. In CVPR, pages 8762\u20138771, 2021. [19] Qinglin Liu, Haozhe Xie, Shengping Zhang, Bineng Zhong, and Rongrong Ji. Long-range feature propagating for natural image matting. In ACM MM, pages 526\u2013534, 2021. [20] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. Tripartite information mining and inte- gration for image matting. InICCV, pages 7555\u20137564, 2021. [21] Hao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu. In- dices matter: Learning to index for deep image matting. In ICCV, pages 3265\u20133274, 2019. [22] Sihan Ma, Jizhizi Li, Jing Zhang, He Zhang, and Dacheng Tao. Rethinking portrait matting with privacy preserving. International journal of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S51",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "computer vision, pages 1\u201326, 2023. [23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.arXiv preprint arXiv:2112.10741, 2021. [24] Maxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [25] Kwanyong Park, Sanghyun Woo, Seoung Wug Oh, In So Kweon, and Joon-Young Lee. Mask-guided matting in the wild. In CVPR, pages 1992\u20132001, 2023. [26] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hier- archical structure aggregation for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S52",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "image matting. In CVPR, pages 13676\u201313685, 2020. [27] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptually motivated online benchmark for image matting. In CVPR, pages 1826\u20131833, 2009. [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj \u00a8orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022. [29] Carsten Rother, Tom Minka, Andrew Blake, and Vladimir Kolmogorov. Cosegmentation of image pairs by histogram matching-incorporating a global constraint into mrfs. In 2006 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition (CVPR\u201906) , pages 993\u20131000. IEEE, 2006. [30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S53",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. [31] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Back- ground matting: The world is your green screen. In CVPR, pages 2291\u20132300, 2020. 9 [32] Yang Song and Stefano Ermon. Generative modeling by esti- mating gradients of the data distribution.Advances in Neural Information Processing Systems, 32, 2019. [33] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Human in- stance matting via mutual guidance and multi-instance re- finement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2647\u2013 2656, 2022. [34] Jingwei Tang, Yagiz Aksoy, Cengiz Oztireli, Markus Gross, and Tunc Ozan Aydin. Learning-Based Sampling for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S54",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "Natural Image Matting. In CVPR, pages 3050\u20133058, 2019. [35] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881 , 2023. [36] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6830\u20136839, 2023. [37] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Towards seg- menting everything in context. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1130\u20131140, 2023. [38] Yanfeng Wang, Lv Tang, Yijie Zhong, and Bo Li. From com- posited to real-world: Transformer-based",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S55",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "natural image mat- ting. IEEE Transactions on Circuits and Systems for Video Technology, pages 1\u20131, 2023. [39] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao- long Wang, and Shalini De Mello. Open-vocabulary panop- tic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 2955\u20132966, 2023. [40] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep Image Matting. In CVPR, pages 311\u2013320, 2017. [41] Stephen DH Yang, Bin Wang, Weijia Li, YiQi Lin, and Con- ghui He. Unified interactive image matting. arXiv preprint arXiv:2205.08324, 2022. [42] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. Vitmatte: Boosting image matting with pre- trained plain vision transformers. Information Fusion, page 102091,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S56",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "2023. [43] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu. Matte anything: Interactive natural image matting with seg- ment anything models. arXiv preprint arXiv:2306.04121 , 2023. [44] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided matting via progressive refinement network. InCVPR, pages 1154\u20131163, 2021. [45] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for digital matting. In CVPR, pages 7469\u20137478, 2019. [46] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for digital matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S57",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "pages 7469\u2013 7478, 2019. [47] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything every- where all at once. arXiv preprint arXiv:2304.06718, 2023. 10 In-Context Matting Supplementary Material 6. Overview The supplementary material includes the following sec- tions: \u2022 Implementation details of IconMatting. \u2022 Additional experiments. \u2022 Dataset. \u2022 Implementation of a baseline-In-Context Pipeline. 7. Implementation Details of IconMatting Here, we supplement schematic diagrams for the inter- and intra-similarity modules. The inter-similarity computes the similarity between features extracted from the target image and the in-context query derived from the reference im- age, generating an average similarity map, S. The intra- similarity combines the self-attention maps representing intra-image similarities within the target",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S58",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "image with the similarity map S obtained from the inter-similarity module. This fusion employs elements of S as weights assigned to the self-attention maps, thereby providing guidance infor- mation for the matting target. 8. Additional Experiments 8.1. More Qualitative Results Here, we visualize more qualitative results of IconMatting in real-world scenarios, as shown in Fig. 8. 8.2. Why Composited Dataset is Not Used? Our IconMatting, developed as a model for image mat- ting, underwent exclusive training solely on real-world datasets, omitting composited data\u2014a practice uncommon within the domain of image matting. This methodology was adopted due to the substantial discrepancy observed in model performance between composited and real-world datasets when employing Stable Diffusion as the backbone. To substantiate this assertion,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S59",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "we conducted experiments on Composition-1k and RM-1k datasets. For IconMatting, we modified it to be a trimap-based image matting model. First, we removed the in-context similarity module of IconMat- ting. Second, we concatenated the trimap with the original image and feed them together into the matting head. The training and test sets of Composition-1k were pre-defined, while RM-1k underwent training and test set partitioning in an 8:2 ratio. The experimental findings, as depicted in Tab. 7, re- veal a significant performance gap between RM-1k and Composition-1k datasets. Despite the considerably smaller sample size in RM-1k compared to Composition-1k, the former demonstrated notably superior test results. This dis- crepancy highlights that, Stable Diffusion, pre-trained for generative tasks, does not perform optimally",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S60",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "for image mat- ting tasks on composited datasets within the context of our study. Dataset MSE SAD GRAD CONN Composited dataset 0.0227 110.12 68.84 59.02 Real-world dataset 0.0029 14.11 16.32 13.56 Table 7. Experiment on composited dataset and real-world dataset. 8.3. Ablation on Diffusion Time Steps When using Stable Diffusion as a feature extractor, the choice of the time step t during the diffusion process is cru- cial. Generally, a small t corresponds to features that rep- resent the image more closely, with minimal noise added, which is why methods like ODISE [39] for image segmen- tation use t = 0. Conversely, a large t captures more ab- stract semantic features, as seen in DIFT [35] for semantic correspondence using",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S61",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "t = 261. For IconMatting, we aim to extract features that both ex- press abstract semantics in the inter-similarity module and retain detailed characteristics of the image for predicting the alpha matte with the matting head. Therefore, selecting the appropriate t is a trade-off. As shown in Table 8, our experiments show that performance is suboptimal for rather small t values (e.g., 0) or too large t values (e.g., over 300). The optimal performance is achieved when t is set to 200. Timesteps MSE SAD GRAD CONN 0 0.0091 22.12 20.18 10.98 100 0.0094 21.01 19.54 10.94 200 0.0081 19.12 18.65 11.21 300 0.089 23.84 20.11 12.46 400 0.098 31.39 24.57 14.28 Table 8. Ablation study on the choice of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S62",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "diffusion timesteps. 8.4. Visualization on In-Context Similarity To further illustrate the effectiveness of our core module, the in-context similarity, we visualize both inter- and intra- similarity modules. For inter-similarity, we directly visual- ize S; for intra-similarity, we resize multi-scale {S \u2032 l }L to 1 H*W H*W \u00d7 H \u00d7 W flatten Self-attention maps intra-similarity element-wise multiplication In-context query \u2026 \u2026 Inter-features inter-similarity average Dot product Reference Image & Prompt Target Images Figure 7. Illustration of the inter- and intra-similarity modules. For simplicity, the resize operation is omitted, only the calculation of one element of the in-context query is depicted, and the fusion process of self-attention maps from a single scale is shown. Figure 8. Qualitative results of IconMatting. The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S63",
      "paper_id": "arxiv:2403.15789v1",
      "section": "experiments",
      "text": "first column shows the reference input, while the remaining columns display target images and their predicted alpha mattes. Given a single reference input, our method can automatically process the same instance or category of foreground. a uniform scale, averaged it, and then visualized the result. This is demonstrated in the Fig. 10. In the case of an alarm clock and a monkey, due to some differences between the reference input and the matting tar- get, the lower left part of the matting target is lost in the",
      "page_hint": null,
      "token_count": 87,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S64",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "similarity, the results of intra-similarity complement and complete the matting target, thus predicting a complete al- pha matte. This analysis underscores the significance of considering both inter- and intra-similarity in our approach. 9. Dataset Here, we show more samples of our dataset and add details about the construction of our dataset. For our test set ICM- 57, as described in the main text, it encompasses foregrounds of the same category and same instance, fulfilling the essence of in-context matting. Ex- amples of such instances are depicted in Fig 11. In order to utilize a portion of images from AIM- 500, we modi- fied their annotations to align with the requirements of in- context matting, as illustrated in Fig. 12. 2",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S65",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "Target Image Batch Prompt Propagation Reference Image Prompt Correspondence Semantic Mask Image Matting Alpha MatteTrimap Erode Dilation In-Context Matting Pipeline Figure 9. In-Context Matting Pipeline (ICMP) consists of two parts: prompt propagation module and interactive image matting module. Prompt propagation module generates prompts for target images based on the prompts on reference image through semantic correspon- dence, then interactive image matting module predicts alpha matte with images and prompts. ReferenceImage Inter-Sim Intra-Sim Alpha Figure 10. Visualization on in-context similarity. (a) Same instance (b) Same category Figure 11. ICM-57 encompasses foregrounds of both the same category and same instance. 10. In-context Matting Pipeline In our novel task setting for image matting, there wasn\u2019t an existing baseline available for direct comparison with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S66",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "our IconMatting. Therefore, leveraging some off-the-shelf models, we established a pipeline aimed at achieving in- context matting, serving as a baseline in the experiment in ICM - 57AIM - 500ImageReference Figure 12. Modifications made to the annotations. In the second row of pictures, only the chair is preserved in the alpha matte, which meets the needs of in-context matting. the main text. In-Context Matting Pipeline (ICMP) consists of two modules: a prompt propagation module and an image mat- ting module, as shown in Fig. 9. With ICMP, users provide points as prompts on reference images to indicate matting targets, then the prompt propagation module extends these prompts to all input images and the semantic masks of the corresponding matting targets",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S67",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "are obtained. Subsequently, the image matting module processes the input images and corresponding semantic masks, generating alpha mattes for the specified matting targets across all images. We utilize the semantic correspondence property of fea- tures provided by DINOv2 [24] to achieve prompt corre- spondence, and use SAM [8] to extract coarse semantic masks corresponding to the prompt points, thus realizing our designed prompt propagation, and thus ICMP. 10.1. Prompt Propagation Module While auxiliary input-based image matting can yield the alpha matte for a user-specified matting target, it requires manual prompting for each input image, even when the mat- 3 ting target is the same. Prompt propagation addresses this issue by disseminating the prompts provided on the exam- ple image to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S68",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "all input images, resulting in a set of semantic masks. In essence, prompt propagation can be likened to seman- tic correspondence, wherein prompts from the example im- age are matched to corresponding prompts in other images. By prompt propagation, the user\u2019s prompt for the example image can be propagated to the other images, eliminating the need to manually provide a prompt for each image. Considering that features extracted by DINOv2, a model pretrained on large-scale datasets, exhibit strong generaliza- tion to real-world data for semantic correspondence without further training, we employ DINOv2 to propagate prompt points for the example image to other images. Given an in- put image and prompt points indicating the matting target, SAM can be used to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2403_15789v1:S69",
      "paper_id": "arxiv:2403.15789v1",
      "section": "results",
      "text": "generate a semantic mask correspond- ing to the prompt. 10.2. Image Matting Module This module enables the extraction of the alpha matte for the matting target based on semantic masks from prompt propagation. By applying morphological operations to the mask, we transform a semantic mask from prompt propagation into a pseudo-trimap. Then, any trimap-based image matting model can be used to obtain a alpha matte for the matting target, and our choice is VitMatte [42]. 4",
      "page_hint": null,
      "token_count": 76,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9500075789531395,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 14,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 2350,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5587,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5410,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4285,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4629,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4917,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3467,
        "empty": false
      },
      {
        "page": 8,
        "chars": 3539,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5841,
        "empty": false
      },
      {
        "page": 10,
        "chars": 3121,
        "empty": false
      },
      {
        "page": 11,
        "chars": 4096,
        "empty": false
      },
      {
        "page": 12,
        "chars": 1834,
        "empty": false
      },
      {
        "page": 13,
        "chars": 2314,
        "empty": false
      },
      {
        "page": 14,
        "chars": 1365,
        "empty": false
      }
    ],
    "quality_score": 0.95,
    "quality_band": "good"
  }
}