{
  "paper": {
    "paper_id": "arxiv:2205.06218v1",
    "title": "Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets",
    "authors": [
      "Kenny T. R. Voo",
      "Liming Jiang",
      "Chen Change Loy"
    ],
    "year": 2022,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "This paper performs comprehensive analysis on datasets for occlusion-aware face segmentation, a task that is crucial for many downstream applications. The collection and annotation of such datasets are time-consuming and labor-intensive. Although some efforts have been made in synthetic data generation, the naturalistic aspect of data remains less explored. In our study, we propose two occlusion generation techniques, Naturalistic Occlusion Generation (NatOcc), for producing high-quality naturalistic synthetic occluded faces; and Random Occlusion Generation (RandOcc), a more general synthetic occluded data generation method. We empirically show the effectiveness and robustness of both methods, even for unseen occlusions. To facilitate model evaluation, we present two high-resolution real-world occluded face datasets with fine-grained annotations, RealOcc and RealOcc-Wild, featuring both careful alignment preprocessing and an in-the-wild setting for robustness test. We further conduct a comprehensive analysis on a newly introduced segmentation benchmark, offering insights for future exploration.",
    "pdf_path": "data/automation/papers/arxiv_2205.06218v1.pdf",
    "url": "https://arxiv.org/pdf/2205.06218v1",
    "doi": null,
    "arxiv_id": "2205.06218v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.784729+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2205_06218v1:S1",
      "paper_id": "arxiv:2205.06218v1",
      "section": "body",
      "text": "Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets Kenny T. R. V oo Liming Jiang Chen Change Loy S-Lab, Nanyang Technological University {kvoo001,liming002,ccloy}@ntu.edu.sg",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S2",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "This paper performs comprehensive analysis on datasets for occlusion-aware face segmentation, a task that is cru- cial for many downstream applications. The collection and annotation of such datasets are time-consuming and labor- intensive. Although some efforts have been made in syn- thetic data generation, the naturalistic aspect of data re- mains less explored. In our study, we propose two occlu- sion generation techniques, Naturalistic Occlusion Genera- tion (NatOcc), for producing high-quality naturalistic syn- thetic occluded faces; and Random Occlusion Generation (RandOcc), a more general synthetic occluded data gener- ation method (Figure 1). We empirically show the effec- tiveness and robustness of both methods, even for unseen occlusions. To facilitate model evaluation, we present two high-resolution real-world occluded face datasets with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S3",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "fine- grained annotations, RealOcc and RealOcc-Wild, featuring both careful alignment preprocessing and an in-the-wild setting for robustness test. We further conduct a compre- hensive analysis on a newly introduced segmentation bench- mark, offering insights for future exploration. Our code and dataset are available at https://github.com/kennyvoo/face- occlusion-generation. 1. Introduction This paper explores the problem of occlusion-aware face segmentation, which involves extracting face pixels from an occluded face where occlusions are treated as the back- ground class. Face or occlusion semantic segmentation is extensively used in face-related tasks, such as face recog- nition [31], face-swapping [25, 27], and facial reconstruc- tion [32, 38]. Occlusions on faces remained less explored in the past. In reality, human faces are likely to be partially occluded",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S4",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "by hands or wearable objects such as sunglasses or face masks. These occlusions would often degrade the performance of the face-related tasks. Therefore, occlusion- aware face segmentation is worth studying for many practi- cal applications [18, 25, 38]. Supervised training of occlusion-aware face segmenta- RandOcc NatOcc Figure 1. Examples of data generated by our NatOcc and Ran- dOcc methods from CelebAMask-HQ [22]. The first two rows are naturalistic occluded faces generated by NatOcc using color trans- fer, image harmonization, and super-resolution techniques. The last two rows show occluded faces generated using RandOcc by overlaying random shapes with random textures and transparency. tion requires a large amount of data. However, existing real- world occluded face datasets such as [6,19,22] are not",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S5",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "suit- able for this purpose. They are either low in quantity, low resolution or inaccurately labeled. As data collection and annotation is very time-consuming and labor-intensive, pre- vious studies [25, 28, 29, 38] created their synthetic datasets for training by overlaying commonly wearable objects or hand on existing face datasets with basic augmentation tech- niques. They often replaced the occluders\u2019 texture and color to enrich the occlusions\u2019 diversity. Nevertheless, synthetic data generation often neglects the naturalistic aspects of the data. The color, texture, and edges of the occluders are visually unnatural, as shown in Figure 2. In addition, these synthetic data generation ef- forts [28, 38] usually only work for specific use cases and cannot scale to other applications. Specific",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S6",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "types of oc- 1 Figure 2. Examples of synthetic occluded face images taken from previous studies. The red box shows images from [25], whereas the blue box shows images from [38], and the green box shows images from [28]. Specific types of occluders ( e.g., sunglasses, face masks, and hands) were overlaid at a specific location, and the color and texture of occluders often looked unnatural. cluders (e.g., sunglasses, and mask) were overlaid at a fixed location, as shown in Figure 2. The question then arises whether more general synthetic data generation could per- form as well, if not better than, or at least comparable with these specific methods. Moreover, the commonly used val- idation set, Caltech Occluded Faces in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S7",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "the Wild (COFW) dataset [6] might not be a suitable benchmark since not all the faces are occluded due to different occlusion definitions. Thus, a new standard benchmark is required. In this study, we proposed two data generation tech- niques: Naturalistic Occlusion Generation (NatOcc), an ef- fective method to produce naturalistic synthetic occluded face from CelebAMask-HQ [22] using various techniques such as color transfer, image harmonization, and super- resolution; and Random Occlusion Generation (RandOcc), a more general method by overlaying random shapes with random transparency and textures from the Describable Textures Dataset (DTD) [10]. To evaluate the effective- ness and robustness of our synthetic datasets, they were compared with a real-world occluded face dataset. The real-world occluded face dataset was",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S8",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "prepared by manu- ally correcting approximately 4,000 incorrectly labels from CelebAMask-HQ [22]. The dataset is further split into oc- cluded and non-occluded categories. Our synthetic datasets were generated from the non-occluded images. Some ex- amples of the synthetic data generated by NatOcc and Ran- dOcc are shown in Figure 1. Both data generation meth- ods performed at a level or even better than real-world oc- cluded face dataset. Besides, our methods were proved to be effective and robust to handle unseen occlusions and faces in the wild (see Section 4.1). Furthermore, to fa- cilitate model evaluation, we present two high-resolution real-world occluded face datasets collected from Pexels and Unsplash with fine-grained manually annotated masks: Re- alOcc, which consists of 550",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S9",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "aligned and cropped face im- ages; and RealOcc-Wild, which comprises 270 occluded face images in the wild for robustness test. Our contributions are summarized as follows. 1) We propose two synthetic occluded data generation techniques, NatOcc to produce naturalistic synthetic occluded faces, and RandOcc, a general synthetic occluded data genera- tion method. 2) We provide manually corrected annotation masks and new categories (occluded and non-occluded) for widely used CelebAMask-HQ [22]. 3) To facilitate model evaluation, we contribute two real-world occluded face datasets with manually annotated masks, RealOcc (aligned and cropped) and RealOcc-Wild (in the wild).4) We further benchmark several representative baselines [9, 35, 39] and present a comprehensive analysis, verifying the effective- ness of our method (even for unseen occlusions)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S10",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "and pro- viding insights for future exploration. 2. Related Work Real-World Occluded Face Datasets.Existing real-world occluded face datasets [6, 17, 22, 36] have the following shortcomings: they are either low in quantity, low resolu- tion, incorrectly labeled, or lack of annotated masks. To our knowledge, there are only a limited number of real-world occluded face datasets [6, 22] that cover various occlusions (e.g., sunglasses, hats, foods, and hands). For instance, Real World Occluded Faces (ROF) [13] consists of 1,686 sunglasses-occluded faces and 678 mask-occluded faces. The datasets that are specifically targeted for face mask oc- clusion are Interactive Systems Labs (ISL) Unconstrained Face Mask Dataset (ISL-UFMD) [14] with 10,618 face im- ages, and the Face Mask Label Dataset (FMLD) [4]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S11",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "with 63,072 face images. Additionally, Specs on Faces (SoF) [2] consists of 42,592 glasses-occluded face images and Inter- active Systems Labs (ISL) Unconstrained Face Hand Inter- action Dataset (ISL-UFHD) [14] consists of 10,004 hand- occluded images. Many of these datasets do not have anno- tated masks. The only large-scale, high-quality dataset with masks is CelebAMask-HQ [22]. This dataset contains 30,000 face images (1,024\u00d71,024), each containing masks of different parts of the face and accessories such as sunglasses, hats, and earrings. Out of these 30,000 images, approximately 4,000 images are occluded faces. However, the occluded faces suffer incorrect annotations as some objects (e.g., mi- crophones, food, and hands) that overlap the face area are annotated as part of the face rather",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S12",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "than the background. The COFW dataset [6] used to have 1,007 annotated masks, but the full data is no longer publicly available. A previ- ous study [15] has provided the 500 annotated masks for the training set of the COFW [6] dataset. There are a few shortcomings with this dataset. First, the quantity and res- olution of images in the training set are too low, and the masks of the test partition [6], which were used as a bench- mark in different studies, are not available. Lastly, Part La- bels [19], which is a subset from LFW [17], contains only 2,927 images with annotated masks. However, these masks 2 are coarse, not accurately annotated and lacked variety in occlusion. Hence,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S13",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "it is not suitable to be both training and test set. Synthetic Occluded Face Datasets.Numerous approaches have been proposed to generate synthetic occluded faces. Each method has its own considerations, leading to vary- ing numbers of classes and definitions of occlusions. Many studies such as [25, 28, 29, 38] generated their synthetic datasets by overlaying common occlusions such as sun- glasses, masks, hands onto faces. However, most of them are built on a low-resolution dataset, and the synthetic oc- cluded faces do not look natural due to the occluders\u2019 color, texture, and edges. Besides, these data generation meth- ods usually overlaid specific types of occluders at specific locations and orientations. Since most of their code or datasets are not made",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S14",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "public, it is challenging to reproduce, cross-check, and improve on previous data generation meth- ods. The existing occlusion augmentation techniques such as [12, 40] are not used in previous studies. Instead, they produce unnatural occlusions by covering a region of the image with rectangles. Extended Labeled Faces in-the-Wild (ELFW) [28] has 3,754 labeled faces (250\u00d7250), which is an extension from Part Labels [19]. The original Part Labels [19] has many in- correct annotations and lacks occluded faces. Thus, the con- tribution of [28] went beyond simply improving the masks of the original dataset by adding new categories ( e.g., sun- glasses, head-wearables and face masks), but also increased the occluded face images by overlaying objects and hands over the original",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S15",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "images. They followed the approach de- scribed in [26] to perform occlusion augmentation of hands from both Hand2Face [26] and HandOverFace [33]. Al- though the synthetic dataset is not provided, their code to perform occlusion augmentation is released. The main is- sue with this dataset is that the resolution and quantity are low. The more recent work [38] built their synthetically oc- cluded training dataset based on CelebAMask-HQ [22] where they occluded the dataset with 300 different occlud- ers (e.g., masks, scarfs) and replaced the texture on the orig- inal occluders from 800 textures to create more variation. A similar concurrent work to our study is FaceOcc [37], where they fixed the masks of CelebAMask-HQ [22] and produced a synthetic",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S16",
      "paper_id": "arxiv:2205.06218v1",
      "section": "abstract",
      "text": "occluded face dataset with it. However, they have different definitions of occlusions. At the time of this study, they had yet to release their code or dataset. There- fore, further comparison was not included. 3. Methodology 3.1. Dataset Preparation This section shows our rationale in dataset selection and methods to prepare different datasets. First of all, it is es- sential to set some crucial definitions as each previous study had a different interpretation. Our study has two classes,",
      "page_hint": null,
      "token_count": 78,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S17",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "is treated as that category (see Table 1) at the time of this study. Table 1. Definition of classes in our dataset. Class Definition Face The skin of the head includes eyes, nose, mouth but excluding ears. Face (Grey Area) Tattoo, shadow, moustache, and beard overlapped with face, as well as skin of the bald person are considered as part of the face.",
      "page_hint": null,
      "token_count": 63,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S18",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "(Occlusions) Non-face area and any objects such as sunglasses, shirt, hair, microphone, hands that are physically covering (overlap) the face. Words from mag- azines or copyright labels fall into this category as well.",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S19",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "(Grey Area) Transparent/Translucent glasses Figure 3. Examples of the wrongly annotated masks in CelebAMask-HQ [22] as occlusions are treated as part of the face. Face Dataset. CelebAMask-HQ [22] is a suitable and rel- evant face dataset since it contains 30,000 high-quality im- ages with refined masks. The initial face masks were ob- tained by subtracting the skin masks with the masks of sun- glasses, neck, and head in the dataset. After that, we man- ually corrected the wrong annotated masks (see Figure 3) such as hands and microphone using an online annotation platform, Segments.ai1. Next, the dataset was split into oc- cluded and non-occluded categories. The occluded category refers to faces that are overlapped or intersected with any objects. Most",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S20",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "of the corrupted and cartoon images were ex- cluded from the dataset. A subset of images (716 images) made of 80 identities was extracted from the non-occluded category to act as the test set. The partition is summarized in Table 2. Table 2. Summary of our partition of CelebAMask-HQ [22]. Category Quantity CelebAMask-HQ-WO (Train) 24603 CelebAMask-HQ-WO (Test) 716 CelebAMask-HQ-O 4597 Excluded Images 86 WO - Without occlusion O - Occluded 1https://segments.ai/ 3 EgoHands11k Hands Figure 4. Comparison of the images from CelebAMask-HQ [22] overlaid with different hands datasets. The hands from 11k Hands [1] have finer details and higher resolution than Ego- Hands [3]. Occluders Dataset. Unlike other studies [25, 28, 29, 37, 38] that use very specific occluders such",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S21",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "as sunglasses and face mask, we extracted 128 common objects across 20 categories ( e.g., food, bottles, cellphones, and cups) from the Microsoft Common Objects in Context (COCO) dataset [23] with COCOAPI [23]. The original resolutions of the COCO objects were low, so we performed super- resolution ( \u00d74) of the original images with GLEAN [8]. In contrast to other occlusions, hands have similar color to faces, making them harder to detect as occlusion. There are several existing hand datasets [3, 33]. Initially, we used the 200 hands of EgoHands [3] that were sampled by [25]. However, the image resolution of the hands is low and blurry. The edges and details of the hands are not preserved after resizing and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S22",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "overlaid onto CelebAMask-HQ [22] im- ages, as shown in Figure 4. Therefore, we sampled 200 images from another hand dataset, 11k Hands [1], which has a higher resolution of 1,600\u00d71,200. Due to the lack of fine masks, we manually annotated the 200 hands images. The only drawback of this hands dataset is the lack of dif- ferent postures, but it is good enough for our study. The comparison of the hands overlaid by both datasets is shown in Figure 4, the hands from 11k hands [1] have finer details and higher resolution than EgoHands [3]. Validation Set. To facilitate model evaluation, we intro- duce a new validation set, RealOcc consisting of 550 high resolution (1,024\u00d71,024) occluded face images from web-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S23",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "sites such as Pexels and Unsplash, covering different occlu- sions (e.g., hands, masks, food and sunglasses). The face was aligned and cropped as shown in Figure 5 using the same method used in FFHQ [20]. However, due to occlu- sions, approximately 265 occluded face images failed to be detected by dlib [21]. With reference to the aligned and cropped images, we manually aligned and cropped the 265 face images. To speed up the labeling process, coarse masks were produced with the models that are trained with our synthetic dataset, followed by correcting the mask using La- belMe [34] and Segments.ai. We introduced another valida- Cropped and aligned (FFHQ \u2019s method)Manually cropped and aligned Figure 5. Examples of aligned and cropped",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S24",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "occluded face images in RealOcc with different methods. tion dataset for model robustness test, RealOcc-Wild, con- sisting of 270 occluded faces in the wild (without cropping and alignment) that are mainly made of the images failed to be detected by dlib [21]. Due to our definition of occlusion (e.g., transparent glasses are occlusion, and mustache over- lapping faces is not occlusion), the masks in the COFW [6] dataset were manually modified. 3.2. Naturalistic Occlusion Generation (NatOcc) Due to the lack of real-world, large-scale occluded face datasets, previous studies [25, 28, 29, 37, 38] have pro- posed different synthetic occluded face generation tech- niques. In this work, we propose a NatOcc method to pro- duce high-quality naturalistic occluded face images from",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S25",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "CelebAMask-HQ-WO (Train) (see Table 2). Algorithm 1: Color transfer via Sliced Optimal Transport (SOT) [5] with custom preprocess 1 sQuantity \u2190 source < lowerThresh; 2 tQuantity \u2190 target == 0; 3 blackRatio \u2190 sQuantity/tQuantity; 4 if blackRatio >1 then 5 mean(rgb) \u2190 Mean of non-black source pixels in each channel ; 6 Replace blackRatio \u2212 1 ratio of black pixels in source with mean(rgb); 7 end 8 Clipped pixels value of source to upperThresh ; 9 ColorTransferViaSOT(source,target); 4 Source Target Result W/O preprocessW/ preprocess Figure 6. Comparison of the color transfer via Sliced Optimal Transport (SOT) [5] with or without custom preprocess. The pre- process mitigates the issue of black pixels imbalance between the source and the target images, thus",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S26",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "improving the quality of the color transfer. Color Transfer. In reality, most hand occlusions have a similar color to the face. To simulate this scenario, col- ors from the face were transferred to the hands using color transfer via Sliced Optimal Transport (SOT) [5]. The size of the source (face) and target (hand) images must be the same. If the quantity of the black pixels of the source (face) is larger than those of the target (hand), some area of the hands will appear black due to black pixels imbalance, as shown in Figure 6. On the other hand, if a part of the pix- els of the source (face) are too bright, the hands will look unnatural as well.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S27",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "Therefore, preprocess is necessary to address this issue. To resolve this issue, a ratio of black pixels at the source (face) were replaced with the average of each RGB channel of the non-black pixels at the source (face) correspondingly, and the pixels values of the source (face) image were clipped at a certain threshold. The steps of the preprocess before the color transfer is shown in Al- gorithm 1. The comparison of the color transfer with and without preprocess is shown in Figure 6. This simple pre- process can solve most of the cases. However, future work can look into color transfer via Sliced Partial Optimal Trans- port (SPOT) [5] that might handle the above issues better. To speed up",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S28",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "the long process of this method, we have uti- lized GPU and multiprocessing. Samples of the synthetic images with or without color transfer can be found in Fig- ure 7. Augmentation. Affine augmentation, image compression, random brightness and contrast were applied to both faces and occluders using Albumentations [7]. Besides, occluders were randomly resized to 0.5-1 of the face size. Moreover, the edges of occluders were carefully considered to produce a more naturalistic composite image. This was done by ap- plying Gaussian blur to the mask of the occluders before alpha blending, a method of overlaying a foreground im- age over a background image. After alpha blending, the W/O color trasnferW/ color transfer Figure 7. The effects of color transfer",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S29",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "via SOT [5] on 11k Hands [1] with CelebAMask-HQ-WO (Train) images. W/O harmonizationW/ harmonization Figure 8. The effects of image harmonization on the occluders using RainNet [24]. The images after image harmonization look more natural. intersection of the occluders and the faces in the compos- ite image was blurred again. The occluders were randomly positioned around the faces. Additional Augmentation (Hand Only). Besides color transfer, hands were positioned so that the fingers always point to the face, followed by a small random rotation to simulate hands in real-life scenarios. Image Harmonization. To further improve the naturalis- tic aspect of synthesized images, we applied RainNet [24] to harmonize the foreground (occluder) to match the back- ground (face image). COCO [23] occluders",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S30",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "look more nat- ural after image harmonization (see Figure 8). However, it was not the same case for the hand occluder as the color of the hand was changed after image harmonization, defeating the purpose of color transfer. Thus, image harmonization was not applied to the hand occluder. 3.3. Random Occlusion Generation (RandOcc) To overlay hands onto the faces, studies such as [26, 28] find the matching target face images with the same pos- ture as the source face image, followed by overlaying the source image\u2019s hands onto the target face image. Occluders such as sunglasses and face masks were overlaid onto eyes and mouth, respectively. Such a synthetic dataset genera- tion method cannot be applied to most applications. Thus,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S31",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "we propose a more general occlusion method, Random Oc- 5 Figure 9. Examples of CelebAMask-HQ-WO (Train) occluded with random shape and texture from DTD [10]. 30 percent of the occluders are slightly transparent. clusion Generation (RandOcc), which creates synthetic oc- cluded data with minimal effort. The performance and ro- bustness of this RandOcc dataset will be compared with the real occluded dataset and our NatOcc dataset. Occlusion Augmentation. RandOcc overlaid random shape with random transparency and texture from Describ- able Textures Dataset (DTD) [10], which consists of 5,640 images from 47 categories. The same augmentation process (see Section 3.2) was applied in RandOcc. Transparent/Translucent Object Simulation. To simu- late transparent or translucent objects, the alpha mask of alpha blending was",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S32",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "randomly set between 0.5 to 0.8. This is applied to approximately 30 percent of the dataset. Ex- amples of the RandOcc dataset can be found in Figure 9. 4. Experiments 4.1. Settings We provide quantitative and qualitative results on differ- ent variants of our dataset. The training was carried out with two CNNs, PSPNet [39] and DeepLabv3+ [9] with pre-trained ResNet-101 backbone [16], and a Vision Trans- former, SegFormer [35] with pre-trained MIT-B5 backbone. The trained models were tested on two datasets, COFW (train set) [6], and RealOcc-Wild to compare the robustness of each model. The test results of CelebAMask-HQ-WO (Test) are provided in the Appendix. Implementation Details.All experiments were carried out using MMSegmentation [11]. Every model was trained on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S33",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "4 Tesla V100 GPUs, with an input image size of 512 \u00d7512 and a batch size of 8 for 30k iterations. The optimizer for PSPNet [39] and DeepLabv3+ [9] is SGD with learning rate of 0.01, momentum of 0.9 and decay rate of 0.0005. The op- timizer for SegFormer [35] is AdamW with a learning rate of 6e-05 and weight decay rate of 0.01. Evaluations take place every 400 iterations on the RealOcc dataset. In all the experiments, images were resized to 512 \u00d7512. Basic data augmentations, such as random horizontal flips, 30 de- grees of random rotation, and photometric distortion were applied. The loss function for all the experiments is the bi- nary cross-entropy loss with online hard example",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S34",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "mining (OHEM) [30]. Datasets. The definition of training datasets is shown in Table 3. Different combinations of training datasets can be found in Table 4. All the synthetic datasets were generated using C-WO, i.e., CelebAMask-HQ-WO (Train) (see Table 2). The validation sets are the RealOcc, while the test sets are COFW (training set) [6] and RealOcc-Wild. Evaluation Metrics. We evaluate the model performance by comparing mIoU, i.e., the mean Intersection over Union, across all the classes on the validation dataset. 4.2. Results and Analysis We evaluate our two data generation methods with differ- ent approaches. NatOcc focuses on producing naturalistic occluded faces, while RandOcc produces occlusions with a general approach. NatOcc. Table 4 shows that the models trained with our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S35",
      "paper_id": "arxiv:2205.06218v1",
      "section": "background",
      "text": "NatOcc dataset (C-WO-NatOcc-SOT, C-WO-NatOcc) per- formed at a level or better than the real-world occluded face dataset (C-CM), demonstrating the effectiveness of our",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S36",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "DeepLabv3+ [9]). As shown in Figure 11, the CNN models trained with NatOcc datasets can segment the faces more accurately than the ones trained with the C-CM. Adding C-CM to the NatOcc dataset further enhanced the model\u2019s performance. In particular, SegFormer [35] trained on C- WO-NatOcc-SOT and C-CM achieved the highest score of 98.02. Although our synthetic datasets do not cover most of the occlusions (e.g., glasses, face masks, camera, and food), the models trained with our NatOcc dataset can generalize well to these unseen occluded faces. We include more re- sults in Appendix. RandOcc. The RandOcc dataset (C-WO-RandOcc) brought huge improvement over the one without occlusions (C-WO), and it was not far behind the real-world occluded dataset (C-CM). Figure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S37",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "11 shows that SegFormer [35] trained with RandOcc datasets was able to generalize to un- seen occlusions despite the unnatural occluders of RandOcc datasets (see Figure 9). The advancement in deep learning models has made this type of general data generation prac- tical and possible. C-WO-Mix, the mixture of RandOcc and NatOcc further improved the performance closer to C- CM. Further exploration can be done in future work to cre- ate a more robust and effective general synthetic occluded data generation method that can save human labor and time preparing synthetic data for every application. Hard Occluders (Transparent/Translucent). Seg- Former [35] trained on the dataset without any real trans- parent/translucent occluders (C-WO-NatOcc) failed to ac- curately detect transparent glasses, as",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S38",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "shown in Fig- ure 10. Although C-WO-RandOcc contains synthetic trans- parent/translucent occluders generated by RandOcc in Sec- 6 Table 3. Definition of datasets in our experiments. Class Definition C-Original CelebAMask-HQ-WO (Train) and CelebAMask-HQ-O with original masks. C-CM CelebAMask-HQ-WO (Train) and CelebAMask-HQ-O with corrected masks. C-WO CelebAMask-HQ-WO (Train). C-WO-NatOcc One set of hand-occluded (without color transfer) face dataset and one set of COCO-occluded face dataset generated by NatOcc with C-WO. C-WO-NatOcc-SOT One set of hand-occluded (with color transfer) face dataset and one set of COCO-occluded face dataset generated by NatOcc with C-WO. C-WO-RandOcc Two sets of occluded face dataset generated by RandOcc with C-WO. C-WO-Mix Half set of C-WO-RandOcc and one set of C-WO-NatOcc. Table 4. Overall Performance:Results of PSPNet [39],",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S39",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "DeepLabv3+ [9] and SegFormer [35] with different combination of datasets. The best results for each validation set are marked in bold. The metrics are mIoU (higher is better). Quantity RealOcc(mIoU) COFW (Train)(mIoU) RealOcc-Wild(mIoU) PSPNet DeepLabv3+ SegFormer PSPNet DeepLabv3+ SegFormer PSPNet DeepLabv3+ SegFormer C-Original 29,200 89.52 88.13 88.33 89.64 88.62 91.36 85.21 82.05 85.24 C-CM 29,200 96.15 96.13 97.42 91.82 92.77 94.87 91.33 91.01 95.16 C-WO 24,602 89.38 89.01 91.36 89.53 88.97 92.24 83.86 84.14 86.72 C-WO + C-WO-NatOcc 24,602 + 49,204 96.65 96.51 97.30 90.71 91.21 94.30 91.34 91.70 94.17 C-WO + C-WO-NatOcc-SOT 24,602 + 49,204 96.35 96.59 97.18 92.32 91.74 93.55 93.26 92.69 94.27 C-WO + C-WO-RandOcc 24,602 + 49,204 95.09 95.21 96.53 90.82 91.35 93.14 89.54 89.68 92.84",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S40",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "C-WO + C-WO-Mix 24,602 + 73,806 96.55 96.66 97.37 90.99 91.20 93.74 92.14 91.84 94.40 C-CM + C-WO-NatOcc 29,200 + 49,20497.28 97.33 97.95 91.61 92.66 94.86 92.13 93.81 95.43 C-CM + C-WO-NatOcc-SOT 29,200 + 49,204 97.17 97.2998.02 92.07 92.91 94.60 92.84 93.73 94.53 tion 3.3, the model trained with C-WO-RandOcc alone was not able to detect transparent glasses as occlusion. How- ever, the model trained on C-WO-Mix, i.e., a mixture of the datasets generated by NatOcc and RandOcc, was able to detect transparent glasses as occlusion, as shown in Fig- ure 10, demonstrating the possibility to simulate transpar- ent/translucent objects. The examples in Figure 10 show that RandOcc is complementary to NatOcc to potentially detect transparent/translucent objects. Impact of an",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S41",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Unclean Dataset. Table 4 and Figure 11 show that the C-Original that has incorrect masks (ignored some occlusions) performed poorly and surprisingly worse than the C-WO, which does not have any occluded faces. This indicates that unclean datasets have a significant neg- ative impact on the model\u2019s performance even with Seg- Former [35], the better deep learning model. Thus, it shows the importance of clean data. Class Imbalance. Occlusions would increase the already large amount of background pixels in an image. There- fore, the IoU of background significantly affects the score of the mIoU, resulting in the high mIoU of the dataset with incorrect annotations (C-Original) and the dataset without occlusions (C-WO). For instance, the background and face IoU of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S42",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "PSPNet [39] trained on C-original are 95.04 and 83.99, respectively, and this result in a mIoU of 89.52. De- spite having high mIOU, PSPNet trained on C-original per- form badly on occlusion-aware face segmentation as shown in Figure 11. Therefore, better metrics such as frequency C-WO-Mix C-WO-NatOccC-WO-RandOcc Figure 10. The visual results on transparent/translucent occlud- ers of SegFormer [35] trained on C-WO-NatOcc, C-WO-RandOcc and C-WO-Mix. Models trained on C-WO-NatOcc and C-WO- RandOcc failed to detect transparent glasses as occlusion accu- rately. In contrast, the model on C-WO-Mix can detect transparent glasses as occlusions, proving that RandOcc is complementary to NatOcc to detect transparent/translucent objects. weighted IoU can be explored in future work to better eval- uate the model performance. Impact",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S43",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "of Color Transfer. Despite the minor differ- ences between the model performance trained with C-WO- NatOcc and C-WO-NatOcc-SOT (i.e., with or without color 7 GT C-Original C-CM C-WO C-WO + C-WO-NatOcc C-WO + C-WO-NatOcc-SOT C-WO + C-WO-RandOcc C-WO + C-WO-Mix C-CM + C-WO-NatOcc C-CM + C-WO-NatOcc-SOT PSPNetDeepLabv3+SegForrmerPSPNetDeepLabv3+SegForrmer Figure 11. Occlusion-aware face segmentation results of PSPNet [39], DeepLabv3+ [9] and SegFormer [35] trained on different variations of datasets. Overall, the models trained with our NatOcc datasets obtain comparable or even better results than the models trained with real-occluded dataset (C-CM). SegFormer [35] trained with our RandOcc shows a huge improvement over C-Original and C-WO. transfer), no conclusion can be made on which one is bet- ter. Without a hand-occluded face dataset",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S44",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "with annotation masks, the effectiveness of color transfer cannot be evalu- ated fairly. In reality, faces might be occluded by the hands of another person, which are very different in color from that of the faces. A mixture of hands with and without color transfer might be able to complement each other and achieve higher performance. Robustness Analysis. The robustness of the trained mod- els was evaluated on real-world occluded faces in the wild, specifically COFW (Train) [6] and RealOcc-Wild. Over- all, the models trained with our synthetic datasets per- formed better and can boost the performance of the real- world occluded face dataset (C-CM). In addition, the results on CelebAMask-HQ-WO (Test) shows that despite the im- provement on segmenting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S45",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "occluded faces, NatOcc and Ran- dOcc datasets did not bring any negative side effect on seg- menting non-occluded faces. Please refer to Appendix for additional test results. 5. Conclusion and Future Work In this paper, we proposed NatOcc and RandOcc, two occlusion generation methods that are proven to be effec- tive for occlusion-aware face segmentation, even for unseen occlusions. Besides, we contributed the corrected masks and new categories of CelebAMask-HQ [22]. To facil- itate model evaluation, we introduce two high-resolution real-world occluded face datasets, RealOcc and RealOcc- Wild. Further, we benchmark several representative base- lines and provide insights for future exploration. As for fu- ture work, devising more advanced techniques to produce higher-quality synthetic data to better simulate real-world data",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S46",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "could be interesting. Moreover, improving generaliza- tion of synthetic data can be further studied. In addition, our benchmark can be enriched with higher-quality synthetic data and more baselines for a more comprehensive analysis. Last but not least, practical applications of occlusion-aware face segmentation in face-related tasks ( e.g., face recogni- tion and face-swapping) could be further explored. Acknowledgments. This study is supported under the RIE2020 Industry Alignment Fund \u2013 Industry Collabora- tion Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). 8 References [1] Mahmoud Afifi. 11K Hands: Gender recognition and bio- metric identification using a large dataset of hand images. Multimedia Tools and Applications, 2019. 4, 5 [2] Mahmoud Afifi and Abdelrahman Abdelhamed.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S47",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "AFIF4: Deep gender classification based on adaboost-based fusion of isolated facial features and foggy faces. Journal of Visual Communication and Image Representation, 2019. 2 [3] Sven Bambach, Stefan Lee, David J Crandall, and Chen Yu. Lending a hand: Detecting hands and recognizing activities in complex egocentric interactions. In Proceedings of the IEEE International Conference on Computer Vision, 2015. 4 [4] Borut Batagelj, Peter Peer, Vitomir \u02c7Struc, and Simon Do- bri\u02c7sek. How to correctly detect face-masks for COVID-19 from visual information? Applied Sciences, 2021. 2 [5] Nicolas Bonneel and David Coeurjolly. Sliced partial opti- mal transport. Association of Computing Machinery Trans- actions on Graphics (Proceedings of Special Interest Group on Computer Graphics and Interactive Techniques), 2019. 4, 5 [6] Xavier P",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S48",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Burgos-Artizzu, Pietro Perona, and Piotr Doll \u00b4ar. Robust face landmark estimation under occlusion. In Pro- ceedings of the IEEE International Conference on Computer Vision, 2013. 1, 2, 4, 6, 8, 11 [7] Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khved- chenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmen- tations. Information, 2020. 5 [8] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. GLEAN: Generative latent bank for large-factor image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 2021. 4 [9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In European Conference on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S49",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Computer Vision, 2018. 2, 6, 7, 8, 12 [10] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014. 2, 6, 17 [11] MMSegmentation Contributors. MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark. https : / / github . com / open - mmlab/mmsegmentation, 2020. 6 [12] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 3 [13] Mustafa Ekrem Erak \u03b9n, U \u02d8gur Demir, and Haz \u03b9m Kemal Ekenel. On recognizing occluded faces in the wild. In 2021 International Conference of the Biometrics Special Interest Group, 2021. 2 [14] Fevziye",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S50",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Irem Eyiokur, Haz\u0131m Kemal Ekenel, and Alexander Waibel. A computer vision system to help prevent the trans- mission of COVID-19. arXiv preprint arXiv:2103.08773 , 2021. 2 [15] Golnaz Ghiasi and Charless C. Fowlkes. Using segmentation to predict the absence of occluded parts. In British Machine Vision Conference, 2015. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 6 [17] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In Workshop on faces in\u2019Real-Life\u2019Images: detection, align- ment, and recognition, 2008. 2 [18] Liming Jiang,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S51",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. DeeperForensics-1.0: A large-scale dataset for real-world face forgery detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 1 [19] Andrew Kae, Kihyuk Sohn, Honglak Lee, and Erik Learned- Miller. Augmenting CRFs with Boltzmann machine shape priors for image labeling. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2013. 1, 2, 3 [20] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, 2019. 4, 11 [21] Davis E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research, 2009. 4 [22]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S52",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGAN: Towards diverse and interactive facial image ma- nipulation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, 2020. 1, 2, 3, 4, 8 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European conference on computer vision, 2014. 4, 5, 11, 16 [24] Jun Ling, Han Xue, Li Song, Rong Xie, and Xiao Gu. Region-aware adaptive instance normalization for image har- monization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 5 [25] Yuval Nirkin, Iacopo Masi, Anh Tran Tuan, Tal Hassner, and Gerard Medioni.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S53",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "On face segmentation, face swapping, and face perception. In2018 13th IEEE International Conference on Automatic Face & Gesture Recognition . IEEE, 2018. 1, 2, 3, 4 [26] Behnaz Nojavanasghari, Charles E Hughes, Tadas Bal- tru\u02c7saitis, and Louis-Philippe Morency. Hand2Face: Auto- matic synthesis and recognition of hand over face occlusions. In 2017 Seventh International Conference on Affective Com- puting and Intelligent Interaction, 2017. 3, 5 [27] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um \u00b4e, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. DeepFacelab: In- tegrated, flexible and extensible face-swapping framework. arXiv preprint arXiv:2005.05535, 2020. 1 [28] Rafael Redondo and Jaume Gibert. Extended labeled faces in-the-wild (ELFW): Augmenting classes for face segmen- 9 tation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S54",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "arXiv preprint arXiv:2006.13980 , 2020. 1, 2, 3, 4, 5 [29] Shunsuke Saito, Tianye Li, and Hao Li. Real-time facial segmentation and performance capture from RGB input. In European Conference on Computer Vision, 2016. 1, 3, 4 [30] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard ex- ample mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 6 [31] Lingxue Song, Dihong Gong, Zhifeng Li, Changsong Liu, and Wei Liu. Occlusion robust face recognition based on mask learning with pairwisedifferential siamese network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. 1 [32] Hitika Tiwari, Min-Hung Chen, Yi-Min Tsai, Hsien-Kai Kuo, Hung-Jen Chen, Kevin Jou, KS Venkatesh,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S55",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "and Yong-Sheng Chen. Self-supervised robustifying guidance for monocular 3D face reconstruction. arXiv preprint arXiv:2112.14382, 2021. 1 [33] Aisha Urooj and Ali Borji. Analysis of hand segmentation in the wild. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, 2018. 3, 4 [34] Kentaro Wada. Labelme: Image Polygonal Annotation with Python. 4 [35] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. SegFormer: Simple and ef- ficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems , 2021. 2, 6, 7, 8, 12 [36] Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. WIDER FACE: A face detection benchmark. In Proceed- ings of the IEEE Conference on Computer",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S56",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Vision and Pattern Recognition, 2016. 2 [37] Xiangnan Yin and Liming Chen. FaceOcc: A diverse, high- quality face occlusion dataset for human face extraction. arXiv preprint arXiv:2201.08425, 2022. 3, 4 [38] Xiangnan Yin, Di Huang, Zehua Fu, Yunhong Wang, and Liming Chen. Segmentation-reconstruction-guided facial image de-occlusion. arXiv preprint arXiv:2112.08022, 2021. 1, 2, 3, 4 [39] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. InPro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2, 6, 7, 8, 12 [40] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceed- ings of the Association for the Advancement of Artificial In- telligence Conference on Artificial",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S57",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "Intelligence, 2020. 3 10 Appendix A. Additional Quantitative Results More examples of the generated datasets and quantitative results are presented in this section. Non-occluded Face Dataset.Table 5 shows the test results on the CelebAMask-HQ-WO (Test), which is a face dataset without any occlusion. This test aims to verify that our gen- erated dataset will not affect the models\u2019 performance in segmenting non-occluded faces. The models trained with the synthetic dataset generated by NatOcc and RandOcc are improved compared to the models trained on C-original and C-WO. Cropped and Aligned COFW Dataset. Additional test has been carried out on cropped and aligned COFW dataset [6]. 400 face images was successfully obtained us- ing the same method as FFHQ [20]. Table 5",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S58",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "shows that the overall performance is better compared to the COFW dataset [6] without cropped and aligned. B. Additional Qualitative Results The additional qualitative results in Figure 12 and Fig- ure 13 show that models trained with both NatOcc and RandOcc datasets perform as well as or better than models trained on the real-world dataset (C-WO). In some exam- ples, they are even better than real-world datasets, showing the effectiveness of our data generation methods. C. More Examples of NatOcc and RandOcc Datasets Additional examples of the NatOcc dataset are shown in Figure 14, Figure 15, and Figure 16. Figure 14 shows the hand-occluded faces without color transfer while Fig- ure 15 shows the hand-occluded faces with color transfer. Figure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S59",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "16 shows the COCO [23] objects-occluded faces af- ter image harmonization. Moreover, some examples of the RandOcc dataset are shown in Figure 17. D. More Examples of Validation Sets Figure 18 shows additional examples of the RealOcc while Figure 19 shows some examples of occluded faces in the wild. 11 Table 5. Additional quantitative test results:PSPNet [39], DeepLabv3+ [9], and SegFormer [35] with different combination of datasets. The best results for each validation set are marked in bold. The metrics are mIoU (higher is better). Quantity CelebAMask-HQ-WO (Test)(mIoU) COFW (Train) (cropped and aligend)(mIoU) PSPNet DeepLabv3+ SegFormer PSPNet DeepLabv3+ SegFormer C-Original 29,200 97.71 97.23 97.18 93.21 92.37 92.87 C-CM 29,200 97.78 97.79 97.88 95.34 95.32 95.62 C-WO 24,602 97.66 97.70 97.84",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S60",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "92.88 92.74 93.54 C-WO + C-WO-NatOcc 24,602 + 49,204 97.77 97.76 97.86 94.45 94.46 94.87 C-WO + C-WO-NatOcc-SOT 24,602 + 49,204 97.71 97.77 97.87 94.61 94.47 94.63 C-WO + C-WO-RandOcc 24,602 + 49,204 97.68 97.76 97.83 93.97 93.83 94.19 C-WO + C-WO-Mix 24,602 + 49,204 97.70 97.76 97.76 93.92 94.55 94.67 C-CM + C-WO-NatOcc 29,200 + 49,204 97.74 97.79 97.87 95.38 95.32 95.53 C-CM + C-WO-NatOcc-SOT 29,200 + 49,20497.78 97.76 97.85 95.26 95.23 95.46 PSPNetPSPNet DeepLabv3+DeepLabv3+ SegForrmerSegForrmer GT C-Original C-CM C-WO C-WO + C-WO-NatOcc C-WO + C-WO-NatOcc-SOT C-WO + C-WO-RandOcc C-WO + C-WO-Mix C-CM + C-WO-NatOcc C-CM + C-WO-NatOcc-SOT PSPNetDeepLabv3+SegForrmer Figure 12. Examples of the inference results on hand-occluded faces. NatOcc and RandOcc are effective in simulating real-world occluded datasets.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S61",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "12 GT C-Original C-CM C-WO C-WO + C-WO-NatOcc C-WO + C-WO-NatOcc-SOT C-WO + C-WO-RandOcc C-WO + C-WO-Mix C-CM + C-WO-NatOcc C-CM + C-WO-NatOcc-SOT PSPNetPSPNet DeepLabv3+DeepLabv3+ SegForrmerSegForrmerPSPNetDeepLabv3+SegForrmer Figure 13. Examples of the inference results on objects-occluded faces. NatOcc and RandOcc are effective in simulating real-world occluded datasets. 13 Figure 14. Examples of the hand-occluded faces generated by NatOcc without color transfer. 14 Figure 15. Examples of the hand-occluded faces generated by NatOcc with color transfer. 15 Figure 16. Examples of the COCO [23] objects occluded faces generated by NatOcc without color transfer. 16 Figure 17. Examples of the images generated by RandOcc by overlaying random shape with random transparency and texture from DTD [10]. 17 Figure 18. Examples of the RealOcc,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2205_06218v1:S62",
      "paper_id": "arxiv:2205.06218v1",
      "section": "method",
      "text": "aligned and cropped real-world occluded faces. 18 Figure 19. Examples of the RealOcc-Wild, real-world occluded faces in the wild. 19",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9514660707812083,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 19,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3575,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5415,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4753,
        "empty": false
      },
      {
        "page": 4,
        "chars": 3744,
        "empty": false
      },
      {
        "page": 5,
        "chars": 3989,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4950,
        "empty": false
      },
      {
        "page": 7,
        "chars": 4070,
        "empty": false
      },
      {
        "page": 8,
        "chars": 2913,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5850,
        "empty": false
      },
      {
        "page": 10,
        "chars": 2429,
        "empty": false
      },
      {
        "page": 11,
        "chars": 1807,
        "empty": false
      },
      {
        "page": 12,
        "chars": 1327,
        "empty": false
      },
      {
        "page": 13,
        "chars": 376,
        "empty": false
      },
      {
        "page": 14,
        "chars": 93,
        "empty": false
      },
      {
        "page": 15,
        "chars": 90,
        "empty": false
      },
      {
        "page": 16,
        "chars": 106,
        "empty": false
      },
      {
        "page": 17,
        "chars": 136,
        "empty": false
      },
      {
        "page": 18,
        "chars": 85,
        "empty": false
      },
      {
        "page": 19,
        "chars": 82,
        "empty": false
      }
    ],
    "quality_score": 0.9515,
    "quality_band": "good"
  }
}