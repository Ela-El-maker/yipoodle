{
  "paper": {
    "paper_id": "arxiv:2411.01896v1",
    "title": "MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network",
    "authors": [
      "Longfeng Shen",
      "Yanqi Hou",
      "Jiacong Chen",
      "Liangjin Diao",
      "Yaxi Duan"
    ],
    "year": 2024,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Accurate segmentation of brain tumors plays a key role in the diagnosis and treatment of brain tumor diseases. It serves as a critical technology for quantifying tumors and extracting their features. With the increasing application of deep learning methods, the computational burden has become progressively heavier. To achieve a lightweight model with good segmentation performance, this study proposes the MBDRes-U-Net model using the three-dimensional (3D) U-Net codec framework, which integrates multibranch residual blocks and fused attention into the model. The computational burden of the model is reduced by the branch strategy, which effectively uses the rich local features in multimodal images and enhances the segmentation performance of subtumor regions. Additionally, during encoding, an adaptive weighted expansion convolution layer is introduced into the multi-branch residual block, which enriches the feature expression and improves the segmentation accuracy of the model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018 and 2019 datasets show that the architecture could maintain a high precision of brain tumor segmentation while considerably reducing the calculation overhead.Our code is released at https://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet",
    "pdf_path": "data/automation/papers/arxiv_2411.01896v1.pdf",
    "url": "https://arxiv.org/pdf/2411.01896v1",
    "doi": null,
    "arxiv_id": "2411.01896v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:50:38.452553+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2411_01896v1:S1",
      "paper_id": "arxiv:2411.01896v1",
      "section": "body",
      "text": "MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network LongfengShen1,2,3* ,YanqiHou1\uff0c2,\u2020 ,JiacongChen1,2,LiangjinDiao1,2,andYaxiDuan1,2 1 AnhuiEngineeringResearchCenterforIntelligentComputingandApplicationonCognitive Behavior(ICACB),CollegeofComputerScienceandTechnology,HuaibeiNormalUniversity, Huaibei,Anhui,China 2 InstituteofArtificialIntelligence,HefeiComprehensiveNationalScienceCenter,Hefei,China 3 AnhuiBig-DataResearchCenteronUniversityManagement,Huaibei,Anhui,China *Correspondingauthor(s).E-mail(s):longfengshen521@126.com \u2020Theseauthorscontributedequallytothiswork.",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S2",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "Accurate segmentation of brain tumors plays a key role in the diagnosis and treatment of brain tumor diseases.Itservesasacriticaltechnologyforquantifying tumorsand extractingtheir features.Withthe increasing application of deep learning methods, the computational burden has become progressively heavier. To achieve a lightweight model with good segmentation performance, this study proposes the MBDRes-U-Net model using the three-dimensional (3D) U-Net codec framework, which integrates multibranchresidualblocksandfusedattentioninto themodel.Thecomputationalburdenofthemodel is reduced by the branch strategy, which effectively uses the rich local features in multimodal images and enhances the segmentation performance of subtumor regions. Additionally, during encoding, an adaptive weighted expansion convolution layer is introduced into the multi-branch residual block, which enriches the feature expression and improves the segmentation accuracy of the model. Experiments on the Brain Tumor Segmentation (BraTS)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S3",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "Challenge 2018 and 2019 datasets show that the architecture could maintain a high precision of brain tumor segmentation while considerably reducing the calculation overhead.Our code is released at https://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet Keywords: Brain tumor segmentation, lightweight model, Brain Tumor Segmentation (BraTS) Challenge,groupconvolution 1. Introduction The glioma, which can be categorized into low-grade glioma (LGG) and high-grade glioma (HGG) subtypes, is the most common primary malignant brain tumor. It has a high incidence, recurrence rate, andmortality,butalowcurerate,whichmakestreatmentchallenging.Treatmentalsorequiresaccurate medical imaging of the tumors and the processing and analysis of images, which rely heavily on physicians. Consequently, the accurate segmentation of brain tumors is key for medical diagnosis and pathologicalanalysis,asshown inFig 1(e).Thistaskincludesthedivisionofseveralsubareas\u2014thatis, the enhanced core (EnC), peritumoral edema (PTE), and non-enhanced core (NEC) areas [1].Among the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S4",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "many medical imaging methods, magnetic resonance imaging (MRI) offers considerable advantages in the treatment of gliomas. It can provide extensive tumor information and is non-invasive; moreover, it does not expose the patient's body to radiation during imaging. There are four common modalities (Fig. 1(a)\u20131(d))\u2014that is, fluid-attenuated inversion recovery (FLAIR), T1-weighted (T1), contrast-enhanced T1-weighted (T1c), and T2-weighted (T2) modalities. Notably, different imaging effects emphasize different tissue characteristics and tumor diffusion regions [2]. Doctors usually manually mark a large number of MRI scans, layer-by-layer and piece-by-piece; the division of tumor regions is dependent on their experience and expert cognition. This work is tedious, time-consuming, and prone to differences of opinion; therefore, accurate automated segmentation has importantresearchimplications. Fig. 1 VisualizationofarandomsampleintheBrainTumorSegmentation(BraTS)2018dataset.From lefttoright:(a)FLAIR,(b)T1,(c)T1c,(d)T2,and(e)groundtruth.Reddenotesthenecroticand non-enhancingcore,yellowdenotestheenhancedcore,andgreendenotestheperitumoraledema Many",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S5",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "attempts have been made to solve the multimodal MRI brain tumor segmentation problem, including edge segmentation, region-based [3], atlas-based [4], and machine learning-based methods. Although they have low computational complexity, these methods rely heavily on artificial annotation and a large number of datasets for pre-training, which can be time-consuming and result in poor segmentationperformance. Since 2012, the multimodal brain tumor segmentation (BraTS) challenge (jointly sponsored by the International Association for Medical Image Computing and Computer-Assisted Interventions (MICCAI) and other organization), has highlighted the progress of deep-learning algorithms in brain tumor segmentation tasks. The performance of U-Net [5] and its variants in multimodal brain tumor segmentationtasks has madeconsiderable progress. From the three-dimensional (3D) U-Net model[6] to the TransBTS [7] and SwinUNETR",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S6",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "models [8], studies have continuously improved the performanceof brain tumor segmentation methods. Moreover, from the initial integration ofthe U-Net architecturetovariousattemptstointroducetransformers,thesemodelshaveintroducedinnovationand progress to the field of brain tumor segmentation. NN-U-Net [9] confirmed the prominent role of the U-Net architecture in image segmentation tasks, with the basic U-Net architecture being effective and competitiveacrossmultipleBraTStasks. Moreover, because the attention mechanism enables neural models to focus accurately on all relevant elements of the input information, it has become an important component for improving the performance of deep neural models. For example, the squeeze-and-excitation (SE) module in SENet [10]\u2014the winner of the ImageNet Competition in 2017\u2014assigned different weights to different positions of the image from a channel domain perspective using a weight matrix to obtain more",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S7",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "important channel-feature information. However, compared to the attention mechanism that focuses only on the channel, a module that combines the channel and spatial attention mechanism can achieve better results. For example, the convolutional block attention module (CBAM) [11] successively infers anattentiondiagramalongthechannelandspatialdimensions. Although 3D convolutional neural networks (CNNs) have achieved significant results in brain tumor segmentation, these models are usually too complex to calculate and generate a large number of parameters, incurring a high computational overhead. Attempts have been made to alleviate this problem by using a lightweight model architecture; however, the segmentation performance of lightweight models falls short of that achieved by advanced models. Improving segmentation accuracy whilemaintainingalowercomputationalcostremainsadifficultproblemtosolve. Accordingly, we proposed a multimodal-based 3D brain tumor lightweight model (MBDRes-U-net), which",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S8",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "not only solves the problem of single features based on single-modal image representation but also relieves the heavy computational burden caused by complex models. A residual block based on multibranch parallel convolution was used in the proposed model to replace the common 3D convolution block; the computational complexity was reduced using group convolution. An adaptive 3-D dilation convolution operation was introduced in the encoder to obtain multiscale feature representation, and a 3D multi-attention module (SCA3D) was added to the encoding stage to enable CNNstofocusattentiononthetumorarea.Theproposedmodelexhibitsspecificadvantagesoverother submodels of the same type in terms of parameter quantity and model complexity. We evaluated the modelontheBraTS2018andBraTS2019datasets,obtaininggoodsegmentationperformanceonboth. Thecontributionsofthisstudycanbesummarizedasfollows: 1) Inthisstudy,a3Dlightweightcodecmodel,MBDRes-U-net,wasproposedforthemulti-modal MRimagesegmentationofbraintumors.MBDRes-U-netwasdevelopedonasymmetric encoder\u2013decoderarchitecturethatintegratedthenewresidualblockandattentionmechanism. Withitssimplemodelstructure,itcanbeusedasabaselinefor3Dbraintumorsegmentation,thus promotingresearchonbraintumorMRIimagesegmentation. 2) Anewresidualblockbasedonamultibranchatrousconvolutionwasproposed,which simultaneouslyenlargedtheacceptancedomainandreducedthenumberofparameters,thereby solvingthecomputationaloverheadproblemscausedby3Dconvolution.Additionally,we introducedadaptiveatrousconvolutioninsteadofanordinaryatrousconvolution,enrichingthe featurerepresentation. 3) Aplug-and-play3Dattentionmodule(3DSACA),bettersuitedtobraintumorsegmentation,was proposed.Animprovedcombinationmechanismbetweenthespatialandchannelattentionwas explored. 2. Related",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S9",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "Work 2.1 Convolutional Neural Network Model In recent years, deep learning methods, particularly CNNs, have been widely used in medical image processing applications. CNN-based models can accurately capture the local features of two-dimensional (2D) and 3D medical images through learning. Kamnitsas et al. proposed a fully connectedmultiscalemodelarchitecture(DeepMedic)[12]to segment3Dbraintumorimages.Chenet al. proposed improving the segmentation accuracy of brain tumors using the dense connection of a CNN [13]. With the emergence of U-Net [5], the advantages of the encoder\u2013decoder symmetric architecture (based on skip links) in medical image segmentation have become increasingly evident, andithasbeenwidelyusedinbraintumorsegmentationinrecentyears.However,thedatalosscaused by using 2D slices of medical images can be difficult to ignore. To better learn the imaging characteristicsof3Ddataandmeettheclinicalaccuracyrequirementsforprovidingmedicalassistance, \u00c7i\u00e7eket al. extendedtheU-Netmodelfrom2Dto3Dimages[14].Subsequently,Wanget al. proposed a brain tumor segmentation model based",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S10",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "on the 3DU-Net model [15]. Recently, Jiang et al. proposed the two-stage cascaded U-Net model that won first place in the BraTS 2019 Challenge Segment Task [16]. TransBTS [7] replaces the 3DU-Net [14] bottleneck layer with a transformer integrated block to extractmorefull-textinformationandcompensatefortheinabilityoftheearliermodeltoestablishlong connections; the participating team from the German Cancer Research Center proposed a brain tumor segmentation method based on the nnU-Net model, winning the BraTS 2020 Challenge segmentation task [9]. Hatamizadeh et al. proposed a full transformer encoder model based on the U-net architecture\u2014namely the SwinUNETR [8] model\u2014which has had a considerable impact on brain tumor segmentation. The winning model for 2021 was an improvement based on the nnU-Net model [17]. With the rapid development of CNN technology, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S11",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "improving computational cost of models is evident. Considering the efficiency of CNN-based models, many lightweight models for brain tumor segmentationhavebeen proposed.Chen et al. used aseparable3Dconvolution (S3D-U-Net) toreduce the computational cost and memory requirements [18]; however, the segmentation accuracy, particularly for enhancing tumor regions, was low. Zhou et al. [19] proposed a 3D residual neural model (ERV-NET) using a lightweight ShuffleNetV2 model [20] as the encoder and introduced a residual block decoder to avoid degradation. Chen et al. proposed a new 3D extended multi-fiber model (DMF-Net) in which the multi-scale image representation for segmentation was obtained by introducing a weighted 3D extended convolution operation, which reduced the number of model parameters and achieved accurate segmentation [21]. However, the lack of channel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S12",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "information exchange has not yet been resolved. The HMNet model [22] used high-resolution multiresolution branches in parallel to extract multiresolution features, further reducing the complexity and computationaloverheadofthemodel. 2.2 Attention Mechanism To further improve the accuracy of tumor segmentation models, attention mechanisms can be introducedtofocusattentionontumor-relatedregions.Themostcommonlyusedattentionmechanisms in medical image processing include channel and spatial attention, both of which enhance the original features by aggregating the same features in all locations using different aggregation strategies, transformations, and enhancement functions. For example, Mobarakol et al. adopted a 3D U-Net architecture that combined channel and spatial attention with a decoder model for segmentation [23]. The MBANet model [24] included 3D multibranch attention using 3D spatial attention (SA) as the attention layer in the encoder to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S13",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "offer channel and spatial attention. Inspired by SA, the 3D SACA attention module used a channel shuffle [20] to promote information flow between channels without generating parameters. However, the initial grouping operation was discarded and 3D SE was used as the 3D channel excitation module and the residual block with 3D convolution as the spatial excitation module. As a plug-and-play attention block, the 3D SACA module is more suitable for processing 3D convolutionmodelsof3Dimages. 3. Methods 3.1 MBDRes-U-Net The architecture of the MBDRes-U-Net model is shown in Fig. 2. The algorithm uses the codec framework of the 3D U-Net architecture. Considering that the segmentation target tumor region is located in the MRI output, we added a multibranch 3D SACA mixed attention",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S14",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "module in front of the encodersothatthemodelcouldfocusmoreattentiontotheregionofinterestbeforeextractingfeatures. During the encoding stage, a convolution with a step size of 2 was first used for downsampling to reduce the display memory, after which multi-scale features were extracted through six multi-branch extended convolution residual (MBDRes) blocks with adaptive extended convolution layers. In the decoder, a trilinear interpolation method was used to up-sample the feature maps. These up-sampled feature maps were then concatenated with high-resolution features obtained from skip connections.Subsequently,theconcatenatedfeatureswaspassedthroughadecodingconvolutionblock followedbyanMBResblocktoprogressivelyrecovertheoriginalresolutionstep-by-step.Next,allthe channel information was fused using a convolution with a step size of 1. Finally, a segmentationmapwasobtainedusingtheSoftMaxfunctiontorealizeend-to-endsegmentation. Fig. 2 Proposedmulti-scalelightweightmodel(MBDRes-U-Net)forMRIsegmentationofbrain tumors,wherethesizecorrespondsto ,gdenotesthenumberof branches,andg=8inthiswork 3.2 Multibranch Residual Block Theeraof 3DCNNs enabled the full utilization ofthe characteristics ofMRI 3Ddata.However,when the 3D convolution kernel runs on the entire",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S15",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "feature mapping channel, the computational complexity\u2014that is, the floating-point operations per second (FLOPS)\u2014grows exponentially. Consequently,a3DCNNincursahighcomputationalcostduringtraining. After the pre-activated residual block (Fig, 2(a)) used in ResNET v2 [25] is extended to a 3D convolution model (Fig. 2(b)), group convolution\u2014an effective model acceleration method (including ResNeXt [26] and ShuffleNet [27] models)\u2014can be introduced to alleviate the computational burden. SupposethattheResNETv2unit[25]isdividedintog parallelbranches,andthekernelsizeisconstant at , then the original parameter quantity of (b) is .Theparameterquantityofthe residual block after multi-branch grouping is whichisreducedbyg times. Fig. 3 Structureofthemulti-branchresidualblock.Theparameterg denotesthenumberofbranches,g =8inthiswork.Theparameters , ,and denotetheweightsofeachbranchoftheadaptive dilationconvolutionallayer,andd denotesthedilationrate This grouping strategy can effectively reduce the number of model parameters and accelerate the calculations. However, the multiple branches generated by channel grouping work independently and in parallel, affecting the normal information interaction",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S16",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "between channels and reducing the learning abilityofthemodel.Tosolvetheproblemofalackofchannelinformationexchange(Fig 2(c)),wecan adda convolutionallayer atthebeginningand endofthe residualblockto serveasthe information route between each branch. Additionally, the residual connection can be arranged outside the unit such that information at a lower level can be transmitted directly to a higher level without generating additional parameters, increasing the learning ability of the model. Thus, the MBR block canbedenotedanMBResblock. Consideringthatthesizeoftheconvolutionkernelinthetraditionalconvolutionislimited(whichleads to its limited acceptance domain), to expand the receptive field of the model, learn the multiscale features of brain tumor MRI, and capture 3D spatial correlations, we introduced an adaptive weighted expandedconvolutionlayerintheencoderparttoreplacetheconventionalconvolutionoperation;thus, we obtained a multibranch expanded convolution residual block (denoted the MBDRes block). Capturing multiscale information is an effective strategy that has been used successfully before. Tokunaga et",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S17",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "al. proposed a semantic segmentation task in pathology using three parallel CNNs and weighted concatenation to extract multiscale information [28]. In the dense fused maxout network (DFMN) [21] proposed by Chen et al., three parallel convolution layers with different expansion rates wereusedastheweightedsum. The structure of the adaptive weighted expansion convolution layer comprises three parallel 3D expansion volume integration branches, the expansion rates of each branch being 1, 2, and 3, respectively (Fig. 3(d)). Three weights ( , , and ) are assigned to each branch after initialization, and the results of each branch are then added. The initialization of this weight ensuresthateachbranchhasthesameimpactonthemodelinitially. 3.2 3D SACA There is a considerable imbalance in the BraTS dataset, in which tumor regions account for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S18",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "only 1.5% ofthe MRI images and enhanced tumors (ETs)account for only 11%ofthewhole tumor (WT)images [29].Toeliminatetheimpactoflarge-areabackgroundsonthesegmentation,the learningabilityofthe model between the spatial details and advanced morphological features was balanced,the model being morefocusedonthetumorregion. A3D attention mechanism can beintroduced, and the 3D spaceand channel attention can beextracted usingthefeaturerelationshipbetweenthe3Dspaceandchannel,asfollows: Its structure is shown in Fig. 4. A channel split [20] operation can be introduced to divide the input feature into two parts by channel, and , where C, H, W and D denote the number, height, width, and depth of channels inthefeaturemap,respectively. firstcarriesoutaveragepoolingto obtainthe globalchannel information, before passing the information into the channel excitation module to obtain channelcorrelation. isintroduced into thespatialexcitationmodule,and the spatial feature correlation is aggregated to dimension through a point multiplicationoperationofthe3Dconvolutionlayer,sothatspatialattentionweightingcanberealized. Consequently, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S19",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "modelis ableto adaptively adjust the featureresponses atdifferent spatial positions. Next, and are aggregated. We can then fuse the residuals to reduce the sparsity caused by the parallel excitation. Finally, channel shuffling is used to solve the Information exchange problemcausedbythebranchstrategy. Fig. 4 3DSACAmodulestructure 4. Results and Discussion 4.1 Experimental Details 4.1.1 Dataset and data pretreatment WeevaluatedtheMBDRES-U-NETmodelonthefollowingdatasets: 1) BraTS2018dataset:Thetrainingdatasetcomprised285samples;thevalidationdatasetcomprised 66samples. 2) BraTS2019dataset:Thetrainingdatasetcomprised335samples;thevalidationdatasetcomprised 125samples. The volume of each was . The tags for tumor segmentation included the background (tag 0), necrotic and non-enhancing tumors (tag 1), peritumoral edema (tag 2), and GD-enhancingtumors(tag4).Tofacilitatemodeltraining,thedatasetswerepretreatedasfollows: (i) Thedatasetswereacquiredbymultiplemechanismsresultinginunevenintensity;hence,we standardizedtheMRIimagesusingtheZ-scoremethod. (ii)Thebackgroundinformationofthebraintumorimageismeaninglessforsegmentation;therefore, werandomlycroppedthedatato voxelinputs. (iii) Topreventover-fitting,weusedthefollowingdataenhancementstrategiestoaddtrainingdata: 0.5foraxial,coronal,andsagittalrandomreversals;therandomrotationangleintervalwas . 4.1.2 Assessment indicators Theeffectivenessofthemodelwasevaluatedbasedonthecomputationalcomplexityandsegmentation accuracy. Validation was conducted using the validation dataset through an online portal provided by theorganizersoftheBraTSChallenge.Specifically,thesegmentationaccuracywasmeasuredusingthe Dice coefficient",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S20",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "and Hausdorff distance (95%), where ET, WT, and TC refer to the regions of the enhancedtumor(label1),entiretumor(labels1,2,and4),andtumorcore(labels1and4),respectively. Complexity can be determined using the Params and FLOPS metrics. Params represents the spatial complexityofthemodel,andFLOPS representsitstimecomplexity,expressedasfollows: where , and denote the height, width, and depth of the convolutional kernel, respectively, and denotethe number of input and output channels, respectively, and parameters h, w, and d denotetheheight,width,anddepthoftheimage,respectively. 4.1.3 Experimental setup We ran the experimental code in Python 3.6 using 16 lots, trained the model for 500 cycles on three parallel NVIDIA A30 GPUs, and built all experimental models using the PyTorch framework. The Adamoptimizerwasused,andthelearningratewassetto0.001. 4.2 Comparison Experiments with State-of-the-art Methods To verify the performance of the proposed model, we compared the MBDRes-U-Net segmentation performance with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S21",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "that of other advanced models (including the 3D U-Net, U-Net-based, CNN with Transformer, and other lightweight models) on the BraTS 2018 and 2019 datasets. The comparison results are presented in Tables 1 and 2. Compared to the non-lightweight models, the proposed model exhibits the advantage of low model complexity, enabling more efficient segmentation of the WT and TC. The parameters of MBDRes-U-Net parameters are one-fourth of those of the traditional 3D U-Net model (Table 1). Moreover, the computational complexity is reduced by 1643.75 G, and the segmentationaccuracy is considerably improved (being 3.2%,1.8%,and 13.6% higher than that of the 3D U-Net model in the ET, WT, and TC segmentation, respectively). Compared with those of the 3D-ESP-Net and S3D-U-Net models, although the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S22",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "number of parameters for MBDRes-U-Net increase marginally, the computational complexity is just one-third of both, and the segmentation accuracy is considerably improved. In comparison to the DMF-Net model with similar parameters, while the Dice coefficient of the MBDRES-U-net model is 0.1% lower, the Dice coefficients of the WT and TC increases by 0.7% and 1.8%, respectively. Although the proposed model is not as lightweight as the HMNet model, the Dice coefficients of the ET, WT, and TC increase by 0.5%, 0.2%, and 1.0%, respectively, and the Hausdorff distance reduces by 0.002, 0.389, and 2.037 mm, respectively. The overallmean scoreis0.6%higher than thatof the HDC-Net model,which has approximately thesame computational complexity. Compared to that of the latest ADHDC-Net brain tumor segmentation model,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S23",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "although the score of the MBRes-U-Net model in ET is 0.3% lower, those of the WT and TC are 0.5% and 0.3% higher, respectively, the average score of the MBRes-U-Net model being 0.17% higher than that of the non-lightweight ADHDC-Net model. Consequently, the proposed method is a moreefficientalgorithmthatcanachievecomparablesegmentationaccuracy. Table 1 ComparisonofsplitperformanceusingtheBraTS2018dataset.(-)indicatesnoresults reported.Bestresultsinbold Model FLOPS (G) Params (M) Dice_score (%) Hausdor ff95 ET WT TC ET WT TC 3DU-Net[14] 1669.50 16.21 75.9 88.5 71.7 6.040 17.100 11.620 3D-ESP-Net[30] 76.51 3.36 73.7 88.3 81.4 5.302 5.463 7.853 S3D-U-Net[18] 75.20 3.32 74.9 89.3 83.1 (-) (-) (-) DMF-Net[21] 27.04 3.88 79.2 89.6 83.5 3.385 4.861 7.743 HMNet[22] 129.40 0.80 78.6 90.1 84.3 2.699 4.727 7.731 HDC-Net[32] 25.53 2.95 80.0 89.7 83.5 2.403 5.615",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S24",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "6.227 ADHDC-Net[31] (-) (-) 80.2 90.0 84.3 2.411 4.779 6.125 MBDRes-U-Net 25.75 3.85 79.9 90.5 84.6 2.697 4.338 5.704 The results using the BraTS 2019 dataset are listed in Table 2. By retraining the model, the MBRes-U-Net model is lighter and more efficient than the 3D U-Net model. It has fewer parameters and higher segmentation accuracy than both the 3D ESP-Net and DMF-Net models. Compared to the latest brain tumor segmentation model, the MBRes-U-Net model achieves considerable improvements for ET and CT\u2014specifically, 0.09% and 0.5% higher than the MBANet model, and 0.8% and 1.3% higherthan theADHDC-Netmodel,respectively.Although theMBRes-U-NetETand WTDicescores are 0.5% lower, the TC Dice_score is 1.6% higher than that of the TransBTS model. Compared with the HDC-Net model, which has",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S25",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "a similar computational complexity, the TC advantages are evident (increasing by 2.6%),and the overallmean value increasesby 1.03%.Compared to the HMNetmodel, although the proposed model parameters are 3M more than the HMNet model and theWT Dice_score is 0.4% lower, they are 1.1% and 0.5% higher for the ET and CT, respectively, with 103 G less computational complexity. Consequently, it is evident that the MBRes-U-Net model is more competitivethantheotherlightweightandnon-lightweightmodels. Table 2 ComparisonofsplitperformanceusingtheBraTS2019dataset.(-)indicatesnoresults reported.Bestresultsinbold Model FLOPS (G) Params (M) Dice_score (%) Hausdor ff95 ET WT TC ET WT TC 3DU-Net[14] 1669.50 16.21 73.7 89.4 80.7 6.41 12.32 10.44 TransBTS[7] 263.73 30.63 78.8 90.0 81.9 3.73 5.64 6.04 3DESP-Net[30] 76.51 3.36 66.3 87.1 78.6 6.84 7.42 9.74 DMF-Net[21] 27.04 3.88 77.6 90.0 81.5 2.99",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S26",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "4.64 7.98 MBANet[24] (-) (-) 78.2 89.8 83.0 3.08 5.88 5.09 HMNet[22] 129.40 0.80 77.2 89.9 83.0 4.01 5.21 6.57 HDC-Net[32] 25.53 2.95 77.7 89.6 80.9 2.95 8.38 7.68 ADHDC-Net[31] (-) (-) 77.5 90.0 82.2 4.36 4.78 6.15 MBDRes-U-Net 25.75 3.85 78.3 89.5 83.5 3.15 4.72 5.78 Additionally, several visualization results for the MBDRes-U-Net model are presented in Fig 5. The different colors represent different types of tumors\u2014that is, the red area denotes necrotic and non-enhancingtumors,the yellowareadenotesanenhancing tumor,and the greenareadenotesedema. Moreover, from left to right, the segmentation results of the FLAIR, DMF-Net, HDC-Net, ADHDC-Net, and MBDRes-U-Net models are overlaid on the FLAIR image. As shown, the SGEResU-Net model can effectively segment the enhanced tumor, overall tumor, and core tumor regions.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S27",
      "paper_id": "arxiv:2411.01896v1",
      "section": "abstract",
      "text": "Fig. 5 SegmentationresultsontheBRATS2018trainingdataset.Theresultsaresuperimposedonthe FLAIRimage.Eachcolordenotesatumorcategory:namelyreddenotesnecroticandnon-enhanced tumors,yellowdenotesanenhancetumor,andgreendenotesedema 4.3 Ablation Experiment 4.3.1 Adaptive weighted expansion convolution layer Comparative ablation experiments, as listed in Table 3,were conducted to verify whether the adaptive weighteddilationconvolutionlayerandadaptiveweightingalgorithmwererequired. Table 3 ComparisonofdifferentsettingsfortheweightadaptationlayerusingtheBraTS2018dataset. Bestresultsinbold",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S28",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "ET WT TC MBDRes-U-Net Learnable 79.86 90.45 84.56 MBDRes-U-Net 78.18 88.96 83.97 MBDRes-U-Net NOWeighting 79.31 89.95 83.98 Comparing the proposed model with the scheme without the adaptive weighted extended convolution layer, evidently, the extended convolution improves the Dice score. The effectiveness of the weighted strategyisprovenbycomparingitwithanequal-weightscheme( ).Owing to its ability to learn and adaptively select multiscale context information, this weighting strategy results inmorefavorablescores,particularlyfortheWTmetrics. The weights , and used in the training process are shown in Fig. 6. Evidently, the weight parameters in each unit are in a convergent state, reflecting the role of the multibranch extended convolution residual block. Notably, (blue line, corresponding to a small receptive field) plays an important role in the first three blocks and is weakened in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S29",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "the higher blocks. However, evidently, the extended branch of (red line, corresponding to the large receptivefield)hasadominantinfluenceonMBDResBlock-2\u20136,whichmaybeduetothefactthatthe kernelwithasmallerreceptivefieldcannotcaptureusefulsemanticinformationatahigherlevelwitha smallerdimension. Fig. 6 Changesofadaptiveweightsandsumsinthetrainingprocess.MBDResblock-1isthefirst MBDReskblock,andsoonforMBDResblock-2\u20136.Thebluelineinthepicturedenotes , orange denotes andgreendenotes 4.3.2 Multibranch fused attention An ablation experiment was conducted on the BraTS 2018 dataset to assess the need for attention modules and the effectiveness of the parallel branching strategy. The experimental results are summarizedinTable 4. Table 4 IndicatorsfortheevaluationoftheuseofdifferentattentionstrategiesusingtheBraTS2018 dataset.Bestresultsinbold",
      "page_hint": null,
      "token_count": 65,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S30",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "(G) Params (M) Dice_score (%) Hausdor ff95 ET WT TC ET WT TC Noattention 79.24 90.02 84.55 3.068 8.837 6.038 ch_se 25.745 3.849 78.24 52.59 82.55 2.723 20.013 6.460 ch_se+sp_se 25.753 3.849 78.78 89.87 83.31 2.762 5.376 6.378 3DSA 25.741 3.849 79.12 89.87 84.32 2.921 6.147 5.598 3DSACA 25.745 3.849 79.86 90.45 84.56 2.697 4.338 5.704 Using only the channel excitation module (ch_se) and the channel model in series with a spatial excitation module ( ) verified the need for branch parallelism. Additionally, a comparativeexperimentwasconducted using the3DSAmodelinsteadofthe 3DSACA model(which also exhibits dual-attention parallelism). The experimental results show that the proposed attention mechanismismorecompetitive. 5. Conclusions In this study, we proposed a novel 3D U-Net model\u2014namely, the MBDRes-U-Net model\u2014which comprised a multibranch",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S31",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "residual block and integrated multibranch fused attention. In particular, we placed an attention block in the encoder to ensure the model paid more attention to the region of interest before extracting the features. In the encoder, we introduced an adaptive weighted expansion convolutional layer to replace the common convolutional layer to form a multibranch expansion convolutional residual block and extract the multiscale features, enriching the feature representation of the image. Additionally, the MBDRes-U-Net model was compared with advanced methods using the BraTSdataset.Through quantitative analysisof the MBDRes-U-Net segmentation resultsand labels,it was evident that the MBDRes-U-Net model exhibited better computational efficiency, fewer parameters,andoptimalsegmentationperformance.Inthefuture,weplantoevaluatetheapplicationof theMBDRes-U-Netmodeltoothertypicalmedicalimagesegmentationtasks. References [1] MenzeBH,JakabA,BauerS,et al. (2014)Themultimodalbraintumorimagesegmentation benchmark(BRATS)[J].IEEETransMedImaging,34(10):1993\u20132024. [2] Baid,U,Ghodasara,S,Bilello,M,Mohan,S,Calabrese,E,Colak,E,Farahani,K, Kalpathy\u2013Cramer,J,Kitamura,FC,Pati,S,et al. (2021)TheRSNA-ASNR-MICCAIBraTS2021 benchmarkonbraintumorsegmentationandradiogenomicclassification.arXivpreprint arXiv:2107.02314 [3] LiuJ,LiM,WangJ,et al. (2014)AsurveyofMRI-basedbraintumorsegmentationmethods[J]. TsinghuaScienceandTechnology,19(6):578\u2013595.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S32",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "[4] WangH,SuhJW,DasSR,et al. (2012)Multi-atlassegmentationwithjointlabelfusion[J].IEEE TransPatternAnalMachIntell,35(3):611\u2013623. [5] RonnebergerO,FischerP,BroxT(2015)U-net:Convolutionalnetworksforbiomedicalimage segmentation[C]//MedicalImageComputingandComputer-AssistedIntervention\u2013MICCAI 2015:18th InternationalConference,Munich,Germany,October5\u20139,2015,Proceedings,PartIII 18.SpringerInternationalPublishing,234\u2013241. [6] \u00c7i\u00e7ek\u00d6,AbdulkadirA,LienkampSS,et al. (2016)3DU-Net:Learningdensevolumetric segmentationfromsparseannotation[C]//MedicalImageComputingandComputer-Assisted Intervention\u2013MICCAI2016:19th InternationalConference,Athens,Greece,October17\u201321, Proceedings,PartII19.SpringerInternationalPublishing,424\u2013432. [7] WangW,ChenC,DingM,et al. (2021)TransBTS:Multimodalbraintumorsegmentationusing transformer[C]//MedicalImageComputingandComputerAssistedIntervention\u2013MICCAI2021: 24th InternationalConference,Strasbourg,France,September27\u2013October1,Proceedings,PartI 24.SpringerInternationalPublishing,109\u2013119. [8] HatamizadehA,NathV,TangY,et al. (2020)SwinUNETR:Swintransformersforsemantic segmentationofbraintumorsinMRIimages[C]//Brainlesion:Glioma,MultipleSclerosis, StrokeandTraumaticBrainInjuries:6th InternationalWorkshop,BrainLes2020,Heldin ConjunctionwithMICCAI2020,Lima,Peru,October4,RevisedSelectedPapers,PartII6. SpringerInternationalPublishing,272\u2013284. [9] IsenseeF,J\u00e4gerPF,FullPM,et al. (2020)nnU-Netforbraintumorsegmentation[C]// Brainlesion:Glioma,MultipleSclerosis,StrokeandTraumaticBrainInjuries:6th International Workshop,BrainLes2020,HeldinConjunctionwithMICCAI2020,Lima,Peru,October4, RevisedSelectedPapers,PartII6.SpringerInternationalPublishing,118\u2013132. [10] HuJ,ShenL,SunG(2018)Squeeze-and-excitationnetworks[C]//ProceedingsoftheIEEE ConferenceonComputerVisionandPatternRecognition.7132\u20137141. [11] WooS,ParkJ,LeeJY,et al. (2018)CBAM:Convolutionalblockattentionmodule[C]// ProceedingsoftheEuropeanConferenceonComputerVision(ECCV),3\u201319. [12] KamnitsasK,LedigC,NewcombeVFJ,et al. (2017)Efficientmulti-scale3DCNNwithfully connectedCRFforaccuratebrainlesionsegmentation[J].MedImageAnal,36:61\u201378. [13] ChenL,WuY,D\u2019SouzaAM,et al. (2018)MRItumorsegmentationwithdenselyconnected3D CNN[C]//MedicalImaging2018:ImageProcessing.SPIE,10574:357\u2013364. [14] \u00c7i\u00e7ek\u00d6,AbdulkadirA,LienkampSS,et al. (2016)3DU-Net:learningdensevolumetric segmentationfromsparseannotation[C]//MedicalImageComputingandComputer-Assisted Intervention\u2013MICCAI2016:19th InternationalConference,Athens,Greece,October17\u201321, Proceedings,PartII19.SpringerInternationalPublishing,424\u2013432. [15] WangF,JiangR,ZhengL,et al. (2019)3DU-Netbasedbraintumorsegmentationandsurvival daysprediction[C]//Brainlesion\uff1a5th InternationalWorkshop,BrainLes2019,Heldin ConjunctionwithMICCAI2019,Shenzhen,China,October17,RevisedSelectedPapers,PartI5. SpringerInternationalPublishing,131\u2013141. [16] JiangZ,DingC,LiuM,et al. (2019)Two-stagecascadedU-Net:1st placesolutiontoBraTS challenge2019segmentationtask[C]//Brainlesion:Glioma,MultipleSclerosis,Strokeand TraumaticBrainInjuries:5th InternationalWorkshop,BrainLes2019,HeldinConjunctionwith MICCAI2019,Shenzhen,China,October17,RevisedSelectedPapers,PartI5.Springer InternationalPublishing,231\u2013241. [17] LuuHM,ParkSH.(2021)Extendingnn-U-Netforbraintumorsegmentation[C]//Brainlesion: 7th InternationalWorkshop,BrainLes2021,HeldinConjunctionwithMICCAI2021,Virtual Event,September27,RevisedSelectedPapers,PartII7.SpringerInternationalPublishing, 173\u2013186. [18] ChenW,LiuB,PengS,et al. (2018)S3D-UNet:Separable3DU-Netforbraintumor segmentation[C]//Brainlesion:Glioma,MultipleSclerosis,StrokeandTraumaticBrainInjuries: 4th InternationalWorkshop,BrainLes2018,HeldinConjunctionwithMICCAI2018,Granada, Spain,September16,RevisedSelectedPapers,PartII4.SpringerInternationalPublishing, 358\u2013368. [19] ZhouX,LiX,HuK,et al. (2021)ERV-Net:Anefficient3Dresidualneuralnetworkforbrain tumorsegmentation[J].ExpertSystAppl,170:114566. [20] MaN,ZhangX,ZhengHT,et al. (2018)ShuffleNetv2:PracticalguidelinesforefficientCNN architecturedesign[C]//ProceedingsoftheEuropeanConferenceonComputerVision(ECCV), 116\u2013131. [21] ChenC,LiuX,DingM,et al. (2019)3Ddilatedmulti-fibernetworkforreal-timebraintumor segmentationinMRI[C]//MedicalImageComputingandComputerAssisted Intervention\u2013MICCAI2019:22nd",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2411_01896v1:S33",
      "paper_id": "arxiv:2411.01896v1",
      "section": "method",
      "text": "InternationalConference,Shenzhen,China,October13\u201317, Proceedings,PartIII22.SpringerInternationalPublishing,184192. [22] ZhangR,JiaS,AdamuMJ,et al. (2023)HMNet:HierarchicalMulti-ScaleBrainTumor SegmentationNetwork[J].JClinMed,12(2):538. [23] IslamM,VibashanVS,JoseVJM,et al. (2019)Braintumorsegmentationandsurvivalprediction using3DattentionU-Net[C]//Brainlesion:Glioma,MultipleSclerosis,StrokeandTraumatic BrainInjuries:5th InternationalWorkshop,BrainLes2019,HeldinConjunctionwithMICCAI 2019,Shenzhen,China,October17,RevisedSelectedPapers,PartI5.SpringerInternational Publishing,262\u2013272. [24] CaoY,ZhouW,ZangM,et al. (2023)MBANet:A3Dconvolutionalneuralnetworkwith multi-branchattentionforbraintumorsegmentationfromMRIimages[J].BiomedSignalProcess Control,80:104296. [25] HeK,ZhangX,RenS,et al. (2016)Deepresiduallearningforimagerecognition[C]// ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.770\u2013778. [26] XieS,GirshickR,Doll\u00e1rP,et al. (2017)Aggregatedresidualtransformationsfordeepneural networks[C]//ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition, 1492\u20131500. [27] ZhangX,ZhouX,LinM,et al. (2018)ShuffleNet:Anextremelyefficientconvolutionalneural networkformobiledevices[C]//ProceedingsoftheIEEEConferenceonComputerVisionand PatternRecognition,6848\u20136856. [28] TokunagaH,TeramotoY,YoshizawaA,et al. (2019)Adaptiveweightingmulti-field-of-view CNNforsemanticsegmentationinpathology[C]//ProceedingsoftheIEEE/CVFConferenceon ComputerVisionandPatternRecognition,12597\u201312606. [29] WangP,ChungACS(2022)Relaxandfocusonbraintumorsegmentation[J].MedImageAnal, 75:102259. [30] NuechterleinN,MehtaS.(2018)3D-ESPNetwithpyramidalrefinementforvolumetricbrain tumorimagesegmentation[C]//Brainlesion:Glioma,MultipleSclerosis,StrokeandTraumatic BrainInjuries:4th InternationalWorkshop,BrainLes2018,HeldinConjunctionwithMICCAI 2018,Granada,Spain,September16,RevisedSelectedPapers,PartII4.SpringerInternational Publishing,245\u2013253. [31] LiuH,HuoG,LiQ,et al. (2023)Multiscalelightweight3Dsegmentationalgorithmwith attentionmechanism:Braintumorimagesegmentation[J].ExpertSystAppl,214:119166. [32] LuoZ,JiaZ,YuanZ,et al. (2020)HDC-Net:Hierarchicaldecoupledconvolutionnetworkfor braintumorsegmentation[J].IEEEJBiomedHealthInform,25(3):737\u2013745.",
      "page_hint": null,
      "token_count": 65,
      "paper_year": 2024,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9740911745096843,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 16,
    "empty_pages": 1,
    "empty_page_pct": 0.0625,
    "page_stats": [
      {
        "page": 1,
        "chars": 2498,
        "empty": false
      },
      {
        "page": 2,
        "chars": 3034,
        "empty": false
      },
      {
        "page": 3,
        "chars": 3297,
        "empty": false
      },
      {
        "page": 4,
        "chars": 3836,
        "empty": false
      },
      {
        "page": 5,
        "chars": 1548,
        "empty": false
      },
      {
        "page": 6,
        "chars": 1873,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2538,
        "empty": false
      },
      {
        "page": 8,
        "chars": 1677,
        "empty": false
      },
      {
        "page": 9,
        "chars": 2991,
        "empty": false
      },
      {
        "page": 10,
        "chars": 2757,
        "empty": false
      },
      {
        "page": 11,
        "chars": 1208,
        "empty": false
      },
      {
        "page": 12,
        "chars": 1412,
        "empty": false
      },
      {
        "page": 13,
        "chars": 2715,
        "empty": false
      },
      {
        "page": 14,
        "chars": 3195,
        "empty": false
      },
      {
        "page": 15,
        "chars": 2913,
        "empty": false
      },
      {
        "page": 16,
        "chars": 0,
        "empty": true
      }
    ],
    "quality_score": 0.9741,
    "quality_band": "good"
  }
}