{
  "paper": {
    "paper_id": "arxiv:1905.06747v1",
    "title": "Inductive Guided Filter: Real-time Deep Image Matting with Weakly Annotated Masks on Mobile Devices",
    "authors": [
      "Yaoyi Li",
      "Jianfu Zhang",
      "Weijie Zhao",
      "Hongtao Lu"
    ],
    "year": 2019,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Recently, significant progress has been achieved in deep image matting. Most of the classical image matting methods are time-consuming and require an ideal trimap which is difficult to attain in practice. A high efficient image matting method based on a weakly annotated mask is in demand for mobile applications. In this paper, we propose a novel method based on Deep Learning and Guided Filter, called Inductive Guided Filter, which can tackle the real-time general image matting task on mobile devices. We design a lightweight hourglass network to parameterize the original Guided Filter method that takes an image and a weakly annotated mask as input. Further, the use of Gabor loss is proposed for training networks for complicated textures in image matting. Moreover, we create an image matting dataset MAT-2793 with a variety of foreground objects. Experimental results demonstrate that our proposed method massively reduces running time with robust accuracy.",
    "pdf_path": "data/automation/papers/arxiv_1905.06747v1.pdf",
    "url": "https://arxiv.org/pdf/1905.06747v1",
    "doi": null,
    "arxiv_id": "1905.06747v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.075254+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_1905_06747v1:S1",
      "paper_id": "arxiv:1905.06747v1",
      "section": "body",
      "text": "Inductive Guided Filter: Real-time Deep Image Matting with Weakly Annotated Masks on Mobile Devices Yaoyi Li1\u2217 , Jianfu Zhang1 , Weijie Zhao2 and Hongtao Lu1 1Shanghai Jiao Tong University 2Versa {dsamuel, c.sis}@sjtu.edu.cn, weijie.zhao@versa-ai.com, htlu@sjtu.edu.cn",
      "page_hint": null,
      "token_count": 34,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S2",
      "paper_id": "arxiv:1905.06747v1",
      "section": "abstract",
      "text": "Recently, signi\ufb01cant progress has been achieved in deep image matting. Most of the classical image matting methods are time-consuming and require an ideal trimap which is dif\ufb01cult to attain in prac- tice. A high ef\ufb01cient image matting method based on a weakly annotated mask is in demand for mo- bile applications. In this paper, we propose a novel method based on Deep Learning and Guided Filter, called Inductive Guided Filter, which can tackle the real-time general image matting task on mobile de- vices. We design a lightweight hourglass network to parameterize the original Guided Filter method that takes an image and a weakly annotated mask as input. Further, the use of Gabor loss is proposed for training networks for complicated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S3",
      "paper_id": "arxiv:1905.06747v1",
      "section": "abstract",
      "text": "textures in image matting. Moreover, we create an image mat- ting dataset MAT-2793 with a variety of foreground objects. Experimental results demonstrate that our proposed method massively reduces running time with robust accuracy.",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S4",
      "paper_id": "arxiv:1905.06747v1",
      "section": "introduction",
      "text": "Image matting is a fundamental problem in computer vision, which models an image as a linear combination of a fore- ground and a background image: Ii = \u03b1iFi + (1 \u2212\u03b1i)Bi,\u03b1i \u2208[0,1], (1) where \u03b1i is the linear coef\ufb01cient at a pixel position i, Fi for the foreground pixel at iand Bi for the corresponding back- ground pixel. Image matting task is more than a high-accurate segmentation and proposed for the natural image decomposi- tion, which takes transparency into consideration. The gener- ated alpha matte can highly reduce the workload and special requirement of image or video editing for advertisement, de- sign, Vlog, \ufb01lm and so on. With rapid growth of the users who are using mobile devices to edit",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S5",
      "paper_id": "arxiv:1905.06747v1",
      "section": "introduction",
      "text": "images or videos, a fast and accurate model which can enhance user experience is in high demand. However, most of the classical methods[Levin et al., 2008; Chen et al., 2013; Cho et al., 2016; Xu et al., 2017] are time- \u2217Work done as an intern at Versa Figure 1: An illustration of our proposed method. consuming which are not capable of running on mobile de- vices for real-time. Another obstacle for image matting on mobile devices is that the classical methods are sensitive with the input mask. Most of the time we can only obtain weakly annotated masks on mobile devices due to the limitation of time latency and computing ability. We coin the term \u201cweakly annotated mask\u201d to describe",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S6",
      "paper_id": "arxiv:1905.06747v1",
      "section": "introduction",
      "text": "a noisy or inexact mask which gives some inaccurate annotations for foreground and background. A weakly annotated mask can be an output binary image of a segmentation method, a thresholded depth map or an inexact annotated mask from user interaction. It contrasts conven- tional trimap which has an accurate annotation but consumes much time to compute. Reducing the quality of input masks or trimaps can massively degrade performance for the classi- cal methods. In this paper, we introduce a new deep learning framework that is speci\ufb01cally tailored for mobile devices by signi\ufb01cantly reducing network parameters while retaining compatible ac- curacy. Compared with the classical methods that are highly dependent on the quality of trimap, our proposed model is robust with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S7",
      "paper_id": "arxiv:1905.06747v1",
      "section": "introduction",
      "text": "weakly annotated masks. We build our neural network for image matting as an image-to-image translation manner. With the help of GAN framework and three other different losses, we can generate highly detailed image mat- tes with a tiny network. Our main contributions in this paper are three-fold and can be summarized as followings: \u2022We design a novel real-time framework for the weakly annotated image matting task, dubbed Inductive Guided Filter. We are the \ufb01rst to introduce the combination of the deep convolutional neural networks and Guided Fil- ter into the image matting task. arXiv:1905.06747v1 [cs.CV] 16 May 2019 Figure 2: The overview of our generator. Linear coef\ufb01cientsA and B are generated from two branches sharing a same light weight Hourglass",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S8",
      "paper_id": "arxiv:1905.06747v1",
      "section": "introduction",
      "text": "backbone. The right-most two upsampling layers both have a factor 4 which mimic Fast Guided Filter [He and Sun, 2015] for acceleration. \u2022We propose a Gabor loss based on a bundle of Gabor \ufb01l- ters to extract more comprehensive high-frequency fea- tures. To the best of our knowledge, no prior works have introduced such a loss function. \u2022We further create an image matting dataset with 2793 foregrounds for the training of deep image matting called MAT-2793, which is the current biggest dataset for image matting to our knowledge. We evalu- ate our proposed method on MAT-2793 and Adobe Composition-1k testing dataset. Compared with the classical methods, our proposed method can achieve ro- bust image matting effectively and ef\ufb01ciently.",
      "page_hint": null,
      "token_count": 118,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S9",
      "paper_id": "arxiv:1905.06747v1",
      "section": "related_work",
      "text": "In this section, we review previous works on deep image mat- ting and Guided Filter [He et al., 2010] which are highly re- lated to our method. Deep Image Matting Many recent deep image matting ap- proaches can be broadly categorized as general deep im- age matting approaches and ad hoc deep image matting ap- proaches that are tailored for speci\ufb01c tasks. General deep image matting approaches attempt to predict the alpha matte given any natural image and the ideal trimap. Cho et al. were the \ufb01rst to introduced deep learning into im- age matting task [2016]. Their DCNN matting aimed to learn convolutional neural networks to combine the output of dif- ferent classical image matting approaches. The Adobe Deep",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S10",
      "paper_id": "arxiv:1905.06747v1",
      "section": "related_work",
      "text": "Image Matting (Adobe DIM) proposed in [Xu et al., 2017] was the \ufb01rst end-to-end deep image matting method, which signi\ufb01cantly outperforms the conventional methods. Lutz et al. further employed the generative adversarial networks (GAN) in their proposed AlphaGAN [Lutz et al., 2018]. Some deep image matting methods are specialized in prac- tical application scenarios like portrait matting. Shen et al. proposed a portrait matting method [2016] with a deep network for trimap generation followed by a closed-form matting [Levin et al., 2008 ], which can propagate gradi- ents from closed-form matting to the neural network. Chen et al. proposed a Semantic Human Matting [Chen et al., 2018b] which incorporated person segmentation and matting in an end-to-end manner. More lightweight deep",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S11",
      "paper_id": "arxiv:1905.06747v1",
      "section": "related_work",
      "text": "image mat- ting methods were proposed for portrait and hair matting on mobile devices [Zhu et al., 2017; Levinshtein et al., 2018; Chen et al., 2019]. Guided Filter Guided Filter was proposed in [He et al., 2010] as an edge-preserving smoothing operator that had a theoretical connection with the matting Laplacian matrix. Deep Guided Filter [Wu et al., 2018] applied the Guided Fil- ter to the output image of an image-to-image translation net- work as a super-resolution block and propagates the gradient through Guided Filter to the low-resolution output. Zhu et al. proposed a fast portrait matting method with a feathering block inspired by Guided Filter [Zhu et al., 2017], which will be elaborated in Section 3.1.",
      "page_hint": null,
      "token_count": 117,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S12",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "We attempt to build an extremely high ef\ufb01cient image mat- ting neural network which takes a weakly annotated mask as input. To this end, we employ the idea of linear model as- sumption in Guided Filter [He et al., 2010], which is robust and ef\ufb01cient in feathering tasks. Following [Lutz et al., 2018], we adopt Generative Adver- sarial Network (GAN) [Goodfellow et al., 2014] architecture to our model. The coarse architecture of our method is illus- trated in Figure 1 and details of generator in Figure 2. 3.1 Inductive Guided Filter Formulation In Guided Filter [He et al., 2010], the basic assumption is that the output alpha is a linear transform of guidance image I in a small window \u03c9k",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S13",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "centered at pixel k: \u03b1i = AkIi + Bk,\u2200i\u2208\u03c9k, (2) in which Ak and Bk are linear coef\ufb01cients to be optimized. The optimization objective is to minimize the difference be- tween output \u03b1i and the corresponding pixel Mi on input weakly annotated mask M with the regularization on Ak. Figure 3: Some foreground object samples from the MAT-2793 dataset. In the image matting setting, Guided Filter solves an opti- mization problem for each image and mask to generate a lin- ear transformation from input image I to matte estimation \u03b1 which is as close to input mask M as possible. Although Guided Filter is a fast and effective method for weakly annotated image matting task, it is limited by the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S14",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "con- straint that the difference between optimal alpha matte and weakly annotated mask should be small enough. Empirically, the mask from a semantic segmentation method or user inter- action will have a relatively large difference from the ground- truth alpha (see our testing set samples in Figure 9). Different from Guided Filter, our method attempts to deal with a supervised learning task instead of an optimization problem. We abandon the objective function and remove the constraint on the difference between matte estimation and mask. An inductive model based on the linear transform as- sumption is built to leverage the ground truth information in an image matting dataset. We formula the Inductive Guided Filter as \u03b1= \u03c6A(I,M ) \u25e6I+ \u03c6B(I,M ),",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S15",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "(3) where \u25e6denotes Hadamard product and we parameterize A and B in Guided Filter by neural networks \u03c6A(I,M ) and \u03c6B(I,M ). Networks \u03c6A and \u03c6B take image I and weakly annotated mask M as input and share backbone parameters (see last two layers in Figure 2 for details). The optimization objective of Inductive Guided Filter is to minimize the differ- ence between the alpha matte prediction and ground truth. For any image and mask, \u03c6A and \u03c6B can generate the spe- ci\ufb01c coef\ufb01cients Aand B, to build a linear transform model for alpha matte. A similar idea was also mentioned in [Zhu et al., 2017 ]. The main difference between our method and theirs is that the authors of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S16",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "[Zhu et al., 2017] formulated their feathering block based on the closed-form solution of Guided Filter: \u03b1i = AkMF i + BkMB i + Ck,\u2200i\u2208\u03c9k, (4) where MF and MB are masks for foreground and back- ground. Ak,Bk and Ck are coef\ufb01cients that parameterized by a neural network like \u03c6(\u00b7) in our method. From Equation (4) we can derive that the output of their feathering block will only preserve the edge and gradient of the mask instead of the input image. It can be seen as an attention map on the mask. Consequently, a weakly annotated mask may lead to perfor- mance degradation. On the contrary, Inductive Guided Fil- ter can be regarded as an attention map on the original",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S17",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "input image, which is the same as the linear transform in Guided Filter. It is more robust to the noise in a mask. 3.2 Generator The generator consists of a lightweight Hourglass backbone, spatial attention mechanism, and a linear transformation. We build a lightweight Hourglass backbone following the structure of U-Net [Ronneberger et al., 2015] and the Hour- glass module in Stacked Hourglass Networks [Newell et al., 2016] which prove to be effective to preserve low-level in- formation from high-resolution features. Only two residual blocks are involved in the bottleneck. Moreover, depthwise convolution, which is widely used in lightweight deep neu- ral networks [Chollet, 2017; Sandler et al., 2018], is adopted in the \ufb01rst two convolution layers and the shortcut",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S18",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "connec- tions. We only introduce depthwise blocks to the layers that have high-resolution feature maps as [Nekrasov et al., 2018] did. There is no 1 \u00d71 convolution layer between the \ufb01rst two depthwise convolutions. We regard them as adaptive down- samplings. All efforts aim to reduce inference latency. Spatial attention [Xu et al., 2015; Chen et al., 2017a] has shown effectiveness in various computer vision tasks. Some previous deep image matting methods adopted spatial atten- tion in their structures, which conduces to a decent matting performance [Chen et al., 2018b; Zhu et al., 2017]. As for our attention mechanism, we fuse the feature from input and bottleneck to compute an attention map which is applied to the high-resolution features in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S19",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "the decoder. Besides the adversarial loss, we impose three loss functions on the generator: global loss, local loss and Gabor loss. The real alpha matte and predicted alpha matte in triplet input are denoted by \u03b1and \u02dc\u03b1. Global Loss To supervise the alpha matte prediction, we leverage the global loss which is a L1 loss between the ground truth alpha and the estimated alpha, Lg = \u2225\u03b1\u2212\u02dc\u03b1\u22251, (5) Local Loss When we are training a network for image mat- ting, in contrast to global loss, we would like objective func- tion to focus more on the boundary of foreground object. Lo- cal loss is a weighted reconstruction based on a difference function \u2206(\u03b1,M) = \u03b4(|\u03b1\u2212M|> \u03f5). The difference func- tion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S20",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "yields a binary boundary map, in which 1 for the same values in ground truth and mask and 0 for the other pixels. \u03b4 Image Ground Truth Mask Trimap Guided Filter Adobe DIM Ours Composition Figure 4: The visual comparison results on MAT-2793 testing set. The trimap is only for Adobe DIM. The composition is composed with our results and random backgrounds. function enforces the small differences between below \u03f5are neglected. We use \u03f5 = 0.01 in the loss function. The local loss function is written as Ll = \u2225\u2206(\u03b1,M) \u25e6(\u03b1\u2212\u02dc\u03b1)\u22251, (6) and in practice we apply an addition morphological dilation with a 7 \u00d77 kernel to the difference function for a larger boundary area. Gabor Loss Perceptual loss proposed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S21",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "in [Johnson et al., 2016] dramatically improves the visual quality of predictions in supervised image transformation tasks. It provides a se- ries of supervisions from high-frequency features to semantic level features. Perceptual loss utilizes a VGG network pre- trained on RGB color images with speci\ufb01c classes. However, the alpha matte is a gray-scale image. Therefore, we design a Gabor loss to resemble the perceptual loss in our case. Gabor loss replaces the pretrained multi-layer kernels in perceptual loss with a set of single-layer Gabor \ufb01lters to extract high- frequency features. Gabor \ufb01lter was introduced into the neural network as a kernel or initialization in some previous works [Ouyang and Wang, 2013; Luanet al., 2018] due to its comparability to the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S22",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "kernels from shallow layers. Thus, we de\ufb01ne the Gabor loss by Lgb = \u2211 \u03c6gb\u2208\u03a6 \u2225\u03c6gb(\u03b1) \u2212\u03c6gb(\u02dc\u03b1)\u22252 2, (7) where function \u03c6gb(\u00b7) denotes the convolution with Gabor \ufb01l- ter, \u03a6 is the set of different Gabor \ufb01lters. In our training, we design 16 different 7 \u00d77 Gabor \ufb01lters with 16 orientations in \u03a6. All of the \ufb01lters have wavelength \u03bb = 5 , spatial as- pect ratio \u03b3 = 0.5 and standard deviation \u03c3 = 0.5 . We have also tried a larger set with different wavelengths and standard deviations, and no additional remarkable bene\ufb01ts were mani- fested in the training. Some deep image matting method introduces gradient loss into their objective functions with a similar motivation [Levinshtein et al.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S23",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "2018; Chen et al., 2019]. Gradient loss minimizes the difference between the image gradient of the input image and predicted alpha, globally or locally. Com- paring with image gradient, Gabor loss is an extended version with the capability to extract more comprehensive features, especially for alpha matte which is rich in the high-frequency component. 3.3 Discriminator Lutz et al. \ufb01rst introduced GAN into their proposed Alpha- GAN [2018]. In AlphaGAN, the discriminator takes a trimap and a newly composited image from predicted alpha matte as its input. Since image matting does not focus on semantic information, it is ambiguous to judge whether a composited image is real or not. A composited image may have a high \ufb01- delity when it",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S24",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "is generated from an incorrect or a partial alpha matte.",
      "page_hint": null,
      "token_count": 10,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S25",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "Guided Filter 0.101 9.57 15.53 5.53 Adobe DIM 0.148 8.54 16.53 4.42 Our method 0.028 2.51 6.45 1.51 Table 1: The quantitative results on the MAT-2793 testing set To overcome this, we feed the discriminator with a con- ditional triplet input which consists of an original image, a weakly annotated mask and an alpha matte, analogous to some methods with pair input [Chen et al., 2017b; Hu et al., 2018]. Given a triplet input, the discriminator can predict the self-consistency of an input. Concretely, the critic is designed to predict whether an estimated alpha matte is correct condi- tioned on the input image and mask. Adversarial Loss We employ LSGAN [Mao et al., 2017] with gradient penalty [Gulrajani et al.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S26",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "2017] as the adversar- ial loss. The loss function is de\ufb01ned as Ladv = LD + LG + \u03bbgpE\u02c6\u03b1[(\u2225\u2207\u02c6\u03b1D(\u02c6\u03b1|I,M )\u22252 \u22121)2] LD = E\u03b1[(D(\u03b1|I,M ) \u22121)2] + E\u02dc\u03b1[D(\u02dc\u03b1|I,M )2] LG = \u2212E\u02dc\u03b1[(D(\u02dc\u03b1|I,M ) \u22121)2], (8) where \u02c6\u03b1is a convex combination of \u03b1and \u02dc\u03b1with a random coef\ufb01cient sampled from uniform distribution. We use \u03bbgp = 10 in our training. 3.4 Full Loss and Implement Details The full loss function in Inductive Guided Filter is L= \u03bbgLg + \u03bblLl + \u03bbgbLgb + \u03bbadvLadv, (9) in which we use \u03bbg = 10,\u03bbl = 1,\u03bbgb = 200 and \u03bbadv = 1. We leverage PatchGAN[Isola et al., 2017], which is capa- ble of discriminating the \ufb01delity of local patches, to drive the attention of critic",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S27",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "to detailed textures. Spectral Normalization [Miyato et al., 2018] has shown an appealing superiority in the training of GAN. We apply Spectral Normalization layers in our discriminator as well as the Batch Normalization[Ioffe and Szegedy, 2015]. We incorporate training tricks: learning rate warm-up and cosine learning rate decay, following[Xie et al., 2018]. Adam optimizer [Kingma and Ba, 2014] is adopted with \u03b21 = 0.5, \u03b22 = 0.999 and initial learning rate0.0001 for both generator and discriminator. The size of input images, masks, and output alpha matte are 512 \u00d7512. Out-channels of the \ufb01rst 5 convolution layers in the generator are 4,4,8,16,32, and all 5 convolution layers have a stride 2. The whole network has 460,456 trainable parameters with 400,990 in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S28",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "discriminator and 59,466 in the generator.",
      "page_hint": null,
      "token_count": 6,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S29",
      "paper_id": "arxiv:1905.06747v1",
      "section": "experiments",
      "text": "In this section, we evaluate our method on two datasets, our MAT-2793 and Adobe Composition-1k[Xu et al., 2017]. We compare the proposed method with the state-of-the-art Adobe Deep Image Matting (Adobe DIM) [Xu et al., 2017] as well",
      "page_hint": null,
      "token_count": 38,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S30",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "Guided Filter 0.130 176.2 150.5 111.2 Adobe DIM 0.243 105.2 55.0 56.1 Our method 0.064 46.2 42.1 35.7 Table 2: The quantitative results on the Adobe Composition-1k with weakly annotations. as Guided Filter [He et al., 2010] quantitatively and qualita- tively. Extensive experiments are conducted to make an ef\ufb01- ciency comparison with some ad hoc real-time deep matting methods. 4.1 Dataset MAT-2793 We create an image matting dataset MAT-2793, which con- tains 2793 foreground objects, to tackle our weakly annotated image matting task. Most of the foregrounds and correspond- ing alpha mattes are gathered from the Internet as transpar- ent PNG images especially from some free clipart website. Therefore, small parts of the foreground objects are not real- world images.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S31",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "We split our dataset into a training set with 2504 samples and a testing set with 289 samples. In our experiments, we select 5360 high-resolution back- ground images from the SUN dataset [Xiao et al., 2010] and ADE20k [Zhou et al., 2017]. We composite each object onto random backgrounds with a \ufb02ipping and 11 different rotation angles to synthesis 22 different images. To generate weakly annotated masks, we \ufb01rst treat the alpha larger than 0.5 as foreground. Then we apply dilation and erosion together to the foreground mask with random kernel size both from5\u00d75 to 30 \u00d730. If the area of the foreground in a generated mask is less than half of the foreground in alpha matte, this train- ing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S32",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "sample will be abandoned. We resize the shorter side of composited images, corresponding alpha mattes, and masks to 600 pixels. For data augmentation, we randomly crop the image and resize the crop to 512 \u00d7512. Moreover, we ran- domly change the hue of images in the training phase. In testing, we make a 600 \u00d7600 center crop and resize it to 512 \u00d7512.",
      "page_hint": null,
      "token_count": 63,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S33",
      "paper_id": "arxiv:1905.06747v1",
      "section": "results",
      "text": "For a fair comparison between our method, Guided Filter and Adobe DIM\u2217, we generate a trimap from each mask as the input of Adobe DIM. We apply a dilation along with an ero- sion on each mask both with a 20 \u00d720 kernel to create an unknown area. The errors are only computed in the unknown area. We implement the fast version of Guided Filter[He and Sun, 2015] with downsampling factor 4 in all of our experi- ments. We follow the image matting evaluation metrics suggested in [Rhemann et al., 2009]. We report the quantitative results under SAD, MSE, Gradient errors and Connectivity errors in Table 5. Furthermore, we display qualitative visual results in Figure 5. Results show that our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S34",
      "paper_id": "arxiv:1905.06747v1",
      "section": "results",
      "text": "proposed method is robust to noises in input masks and capable of capturing detailed texture in composited images. \u2217We use the implementation from https://github.com/foamliu/ Deep-Image-Matting Image Ground Truth Mask Trimap Guided Filter Adobe DIM Ours Composition Figure 5: The visual comparison results on Adobe Composition-1k. The trimap is only for Adobe DIM. The composition is composed with our results and random backgrounds.",
      "page_hint": null,
      "token_count": 62,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S35",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "BANet-64\u2020(512x512) 1080 Ti 1 23.3 LDN+FB\u2020(128x128) TITAN X 1 13 Adobe DIM (512x512) V100 1 51.1 Ours w/ I/O (512x512) V100 1 3.48 256 1.46 Ours w/o I/O (512x512) V100 1 2.19 256 0.18 Table 3: The results of speed evaluation on Nvidia GPU devices. We also display the speed of our method with or without GPU I/O.",
      "page_hint": null,
      "token_count": 58,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S36",
      "paper_id": "arxiv:1905.06747v1",
      "section": "results",
      "text": "Adobe Composition-1k is an image matting testing set with 1000 images and 50 unique foreground objects proposed in [Xu et al., 2017]. In experiments on Adobe Composition-1k, we generate the weakly annotated mask in the same way as we did in Section 4.1. To generate weakly trimaps for DIM, considering the large image size in Adobe Composition-1k, we apply dilation and erosion om masks with a 50 \u00d750 kernel. We then resize all the images to 512 \u00d7512 in our experiments. We illustrate the visual comparison results in Figure 5 and the quantitative results in Table 2. All the errors are calcu- lated in the unknown area of our generated weakly trimaps. The experiment results also show that our proposed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S37",
      "paper_id": "arxiv:1905.06747v1",
      "section": "results",
      "text": "method is capable of handling the weakly input. 4.4 Speed Evaluation We evaluate the ef\ufb01ciency of the proposed method on differ- ent platforms including Nvidia GPU, computer CPU, and mo- bile devices. Some real-time deep image matting methods for portrait (LDN+FB [Zhu et al., 2017], BANet-64 [Chen et al., 2019] or hair (HairMatteNet [Levinshtein et al., 2018 ]) are also included in this evaluation. We report the performance of these methods from the data in their original papers. \u2020Data from their original papers",
      "page_hint": null,
      "token_count": 83,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S38",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "HairMatteNet\u2020(224x224) iPad Pro GPU 30 LDN+FB\u2020(128x128) Core E5-2660 38 Adreno 530 62 Guided Filter (512x512) Core i7-7700 62.0 Adobe DIM (512x512) Core i7-7700 4003.2 Ours (512x512) Core i7-7700 13.0 iPhone Xs 15.7 iPhone X 22.3 iPhone SE 25.9 Table 4: The results of speed evaluation on CPU and mobile devices. We demonstrate the inference speed on Nvidia GPU de- vices in Table 3 and speed on CPU or difference mobile de- vices in Table 4. We deploy our model on different iPhone devices via the Apple CoreML framework. We can notice that taking a 512 \u00d7512 image as input, our method can run at 5000+ FPS on a single Nvidia V100 GPU with batch size 256 and achieve real-time performance",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S39",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "on an iPhone SE in production in 2016.",
      "page_hint": null,
      "token_count": 8,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S40",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "In this paper, we propose a extremely ef\ufb01cient method for weakly annotated image matting on mobile devices, dubbed Inductive Guided Filter. A lightweight hourglass backbone and a novel Gabor loss are leveraged in the model. We also create a large image matting dataset MAT-2793. Evaluation on two testing datasets demonstrates that our proposed model is robust to the weakly annotated input mask and is competent to extract texture details in an image matting task. References [Chen et al., 2013] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. Knn matting. IEEE TPAMI, 2013. [Chen et al., 2017a] Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and Tat-Seng Chua. Sca-cnn: Spatial and channel-wise attention in convolutional networks for image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S41",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "captioning. In CVPR, 2017. [Chen et al., 2017b] Micka\u00a8el Chen, Ludovic Denoyer, and Thierry Arti`eres. Multi-view data generation without view supervision. arXiv preprint arXiv:1711.00305, 2017. [Chen et al., 2018a] Liang-Chieh Chen, Yukun Zhu, George Papan- dreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmen- tation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 801\u2013818, 2018. [Chen et al., 2018b] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, and Kun Gai. Semantic human matting. In ACM MM, 2018. [Chen et al., 2019] Xi Chen, Donglian Qi, and Jianxin Shen. Boundary-aware network for fast and high-accuracy portrait seg- mentation. arXiv:1901.03814, 2019. [Cho et al., 2016] Donghyeon Cho, Yu-Wing Tai, and Inso",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S42",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "Kweon. Natural image matting using deep convolutional neural networks. In ECCV, 2016. [Chollet, 2017] Franc \u00b8ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017. [Goodfellow et al., 2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. [Gulrajani et al., 2017] Ishaan Gulrajani, Faruk Ahmed, Martin Ar- jovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017. [He and Sun, 2015] Kaiming He and Jian Sun. Fast guided \ufb01lter. arXiv preprint arXiv:1505.00996, 2015. [He et al., 2010] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image \ufb01ltering. In ECCV, 2010. [Hu et al., 2018] Yibo Hu, Xiang Wu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S43",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "Bing Yu, Ran He, and Zhenan Sun. Pose-guided photorealistic face rotation. In CVPR, 2018. [Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by re- ducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. [Isola et al., 2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional ad- versarial networks. In CVPR, 2017. [Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and Li Fei- Fei. Perceptual losses for real-time style transfer and super- resolution. In ECCV, 2016. [Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [Levin et al., 2008] Anat Levin, Dani Lischinski, and Yair Weiss. A closed-form",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S44",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "solution to natural image matting. IEEE TPAMI, 2008. [Levinshtein et al., 2018] Alex Levinshtein, Cheng Chang, Ed- mund Phung, Irina Kezele, Wenzhangzhi Guo, and Parham Aarabi. Real-time deep hair matting on mobile devices. In CRV. IEEE, 2018. [Luan et al., 2018] Shangzhen Luan, Chen Chen, Baochang Zhang, Jungong Han, and Jianzhuang Liu. Gabor convolutional net- works. IEEE TIP, 27(9):4357\u20134366, 2018. [Lutz et al., 2018] Sebastian Lutz, Konstantinos Amplianitis, and Aljosa Smolic. Alphagan: Generative adversarial networks for natural image matting. In BMVC, 2018. [Mao et al., 2017] Xudong Mao, Qing Li, Haoran Xie, Ray- mond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2017. [Miyato et al., 2018] Takeru Miyato, Toshiki Kataoka, Masanori Koyama,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S45",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "and Yuichi Yoshida. Spectral normalization for gen- erative adversarial networks. arXiv preprint arXiv:1802.05957, 2018. [Nekrasov et al., 2018] Vladimir Nekrasov, Thanuja Dharmasiri, Andrew Spek, Tom Drummond, Chunhua Shen, and Ian Reid. Real-time joint semantic segmentation and depth estimation us- ing asymmetric annotations. arXiv preprint arXiv:1809.04766, 2018. [Newell et al., 2016] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. In ECCV, 2016. [Ouyang and Wang, 2013] Wanli Ouyang and Xiaogang Wang. Joint deep learning for pedestrian detection. In ICCV, 2013. [Rhemann et al., 2009] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptually motivated online benchmark for image matting. In CVPR, 2009. [Ronneberger et al., 2015] Olaf Ronneberger, Philipp",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S46",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [Sandler et al., 2018] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. [Shen et al., 2016] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and Jiaya Jia. Deep automatic portrait matting. In ECCV, 2016. [Sup, ] Supervise.ly. https://supervise.ly/. Accessed: 2018. [Wu et al., 2018] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Fast end-to-end trainable guided \ufb01lter. In CVPR, 2018. [Xiao et al., 2010] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. [Xie et al., 2018] Junyuan",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S47",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "Xie, Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, and Mu Li. Bag of tricks for image clas- si\ufb01cation with convolutional neural networks. arXiv preprint arXiv:1812.01187, 2018. [Xu et al., 2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015. [Xu et al., 2017] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In CVPR, 2017. [Zhou et al., 2017] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. [Zhu et al., 2017] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S48",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "and Ming Tang. Fast deep matting for por- trait animation on mobile phone. In ACM MM, 2017. Appendices A Results of Real Images We further evaluate the proposed method on real images. We employ DeepLab v3+ [Chen et al., 2018a] as the segmenta- tion method to demonstrate the performance of our method in practice. We only test on some images which have semantic objects that can be segmented by DeepLab v3+, and we adopt the segmentation as the input mask of our method. The results of potted plant images which are gathered from Google are shown in Figure 6. The results of images from Supervise.ly Person dataset [Sup, ] can be viewed in Figure 7. In addition, Figure 8 also",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S49",
      "paper_id": "arxiv:1905.06747v1",
      "section": "conclusion",
      "text": "demonstrate the results of samples from Adobe Composition-1k [Xu et al., 2017]. B Larger Unknown Area for Adobe DIM We also evaluate the performance of different methods with a larger unknown area in trimap for Adobe DIM [Xu et al., 2017]. We apply the dilation and erosion twice with a 25\u00d725 kernel to generate a larger unknown area. Since errors are only computed in the unknown area, the quantitative results of our method and Guided Filter [He et al., 2010 ] are also changed. The experiment results are shown in Table 5 and Figure 9.",
      "page_hint": null,
      "token_count": 95,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1905_06747v1:S50",
      "paper_id": "arxiv:1905.06747v1",
      "section": "method",
      "text": "Guided Filter 0.112 24.49 21.33 14.25 Adobe DIM 0.146 21.36 20.54 14.20 Our method 0.017 3.29 8.32 1.89 Table 5: The quantitative results with larger unknown area in trimap on the MAT-2793 testing set Image DeepLab v3+ Ours Figure 6: Results of some potted plant images. Masks are generated by DeepLab v3+. Image DeepLab v3+ Ours Figure 7: Results of samples from Supervisely Person dataset. Masks are generated by DeepLab v3+. Image DeepLab v3+ Ours Figure 8: Results of samples from Adobe Composition-1k. Masks are generated by DeepLab v3+. Image Ground Truth Mask Trimap Adobe DIM Ours Figure 9: The visual comparison results with larger unknown area in trimap on MAT-2793 testing set. The trimap is only for Adobe DIM.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2019,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.950539536800846,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 12,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 4060,
        "empty": false
      },
      {
        "page": 2,
        "chars": 4097,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4837,
        "empty": false
      },
      {
        "page": 4,
        "chars": 2839,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5252,
        "empty": false
      },
      {
        "page": 6,
        "chars": 3186,
        "empty": false
      },
      {
        "page": 7,
        "chars": 5978,
        "empty": false
      },
      {
        "page": 8,
        "chars": 1368,
        "empty": false
      },
      {
        "page": 9,
        "chars": 105,
        "empty": false
      },
      {
        "page": 10,
        "chars": 120,
        "empty": false
      },
      {
        "page": 11,
        "chars": 114,
        "empty": false
      },
      {
        "page": 12,
        "chars": 179,
        "empty": false
      }
    ],
    "quality_score": 0.9505,
    "quality_band": "good"
  }
}