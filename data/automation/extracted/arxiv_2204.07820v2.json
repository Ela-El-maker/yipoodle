{
  "paper": {
    "paper_id": "arxiv:2204.07820v2",
    "title": "FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring",
    "authors": [
      "Suiyi Zhao",
      "Zhao Zhang",
      "Richang Hong",
      "Mingliang Xu",
      "Yi Yang",
      "Meng Wang"
    ],
    "year": 2022,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Blind image deblurring (BID) remains a challenging and significant task. Benefiting from the strong fitting ability of deep learning, paired data-driven supervised BID method has obtained great progress. However, paired data are usually synthesized by hand, and the realistic blurs are more complex than synthetic ones, which makes the supervised methods inept at modeling realistic blurs and hinders their real-world applications. As such, unsupervised deep BID method without paired data offers certain advantages, but current methods still suffer from some drawbacks, e.g., bulky model size, long inference time, and strict image resolution and domain requirements. In this paper, we propose a lightweight and real-time unsupervised BID baseline, termed Frequency-domain Contrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with attractive properties, i.e., no image domain limitation, no image resolution limitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the lightweight property and performance superiority, two new collaboration units called lightweight domain conversion unit(LDCU) and parameter-free frequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements inter-domain conversion in lightweight manner. PFCU further explores the similarity measure, external difference and internal connection between the blurred domain and sharp domain images in frequency domain, without involving extra parameters. Extensive experiments on several image datasets demonstrate the effectiveness of our FCL-GAN in terms of performance, model size and reference time.",
    "pdf_path": "data/automation/papers/arxiv_2204.07820v2.pdf",
    "url": "https://arxiv.org/pdf/2204.07820v2",
    "doi": null,
    "arxiv_id": "2204.07820v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:50:38.452937+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2204_07820v2:S1",
      "paper_id": "arxiv:2204.07820v2",
      "section": "body",
      "text": "FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring Suiyi Zhao, Zhao Zhang, Richang Hong Hefei University of Technology, China Mingliang Xu Zhengzhou University, China Yi Yang University of Technology Sydney, Australia Meng Wang Hefei University of Technology, China",
      "page_hint": null,
      "token_count": 41,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S2",
      "paper_id": "arxiv:2204.07820v2",
      "section": "abstract",
      "text": "Blind image deblurring (BID) remains a challenging and signifi- cant task. Benefiting from the strong fitting ability of deep learn- ing, paired data-driven supervised BID method has obtained great progress. However, paired data are usually synthesized by hand, and the realistic blurs are more complex than synthetic ones, which makes the supervised methods inept at modeling realistic blurs and hinders their real-world applications. As such, unsupervised deep BID method without paired data offers certain advantages, but current methods still suffer from some drawbacks, e.g., bulky model size, long inference time, and strict image resolution and domain requirements. In this paper, we propose a lightweight and real-time unsupervised BID baseline, termed Frequency-domain Contrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with attractive",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S3",
      "paper_id": "arxiv:2204.07820v2",
      "section": "abstract",
      "text": "properties, i.e., no image domain limitation, no image res- olution limitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the lightweight property and performance superiority, two new collaboration units called lightweight domain conversion unit (LDCU) and parameter-free frequency-domain contrastive unit (PFCU) are designed. LDCU mainly implements inter-domain con- version in lightweight manner. PFCU further explores the similarity measure, external difference and internal connection between the blurred domain and sharp domain images in frequency domain, without involving extra parameters. Extensive experiments on sev- eral image datasets demonstrate the effectiveness of our FCL-GAN in terms of performance, model size and inference time. CCS CONCEPTS \u2022 Computing methodologies \u2192Computer vision tasks ; \u2022 Im- age representation; blind image deblurring; neural",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S4",
      "paper_id": "arxiv:2204.07820v2",
      "section": "abstract",
      "text": "networks ; KEYWORDS Unsupervised image deblurring baseline; frequency-domain con- trastive loss; lightweight network; real-time inference Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ACM Conference\u201922, January, 2022, City, Country \u00a9 2022 Association for Computing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S5",
      "paper_id": "arxiv:2204.07820v2",
      "section": "abstract",
      "text": "Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn Performance Ours SOTA PSNR\u2191 24.84 23.56 PSNR\u2191 24.84 23.56 SSIM\u2191 0.771 0.738 SSIM\u2191 0.771 0.738 NIQE\u2193 3.924 5.289 NIQE\u2193 3.924 5.289 Model Size (MB) 24.6 606.8 \u2248 25x lighter Model Size (MB) 24.6 606.8 \u2248 25x lighter Runtime (ms) 10.86 62.06 \u2248 5x faster Runtime (ms) 10.86 62.06 \u2248 5x faster Figure 1: Performance comparison between the SOTA unsu- pervised method UID-GAN [20] and our FCL-GAN. Where PSNR, SSIM [38] and NIQE [24] are compared on the Go- Pro test set [25], and \"Runtime\" indicates the time to infer a 1280*720 resolution image using Nvidia RTX 2080Ti. ACM Reference Format: Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S6",
      "paper_id": "arxiv:2204.07820v2",
      "section": "abstract",
      "text": "Wang. 2022. FCL-GAN: A Lightweight and Real-Time Baseline for Unsuper- vised Blind Image Deblurring. In Proceedings of ACM International Confer- ence\u201922, January, 2022, City, Country.ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn",
      "page_hint": null,
      "token_count": 32,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S7",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "Blind image deblurring (BID), as a classical multimedia processing task, aims at recovering a latent image from a blurred input. Blurred pictures in real-world are common, which will greatly affect the image quality and degrade the related low-level vision perception and high-level tasks. Conventional optimization-based methods assume the latent sharp image satisfies various priors [1, 4, 14, 28, 29, 43], and transform the deblur problem into maximum a posteriori probability optimization. However, these methods require complex iterative optimizations with a long inference time. Moreover, the deblurring results contain unpleasant heavy artifacts. In recent years, data-driven deep BID methods (usually super- vised) [5, 7, 17, 25, 34, 35, 45, 46, 48, 50] have achieved superior per- formance, benefiting from the rapid",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S8",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "development of deep learning. Data-driven supervised BID methods usually use a large amount of arXiv:2204.07820v2 [cs.CV] 24 Jul 2022 ACM Conference\u201922, January, 2022, City, Country Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng Wang BSS S* B SB B* Lightweight Domain Conversion Unit (LDCU) GB2S GS2B GS2B GB2S DSDS DBDB l l l l l l negative buffer l l l l l l negative buffer Parameter-Free Frequency-Domain Contrast Unit (PFCU) Modulus-PerformingModulus-Performing FFTFFT BinarizationBinarization De-MarginalizationDe-Marginalization ( ( , ))Min sim q pos ( ( , )) iiMax sim q neg\uf0e5 B S B S B S BS Sharp-guide positive stream Sharp-guide negative stream Sharp-guide anchor stream Sharp-guide positive stream Sharp-guide negative stream Sharp-guide anchor stream Blurred-guide",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S9",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "positive stream Blurred-guide negative stream Blurred-guide anchor stream Blurred-guide positive stream Blurred-guide negative stream Blurred-guide anchor stream Sharp-guide Blurred-guide Figure 2: The architecture of FCL-GAN, which consists of two collaborative units: LDCU and PFCU. LDCU performs conversion of different domains (i.e., \ud835\udc35\u2192\ud835\udc46\ud835\udc35\u2192\ud835\udc35\u2217\u2248\ud835\udc35, \ud835\udc46\u2192\ud835\udc35\ud835\udc46\u2192\ud835\udc46\u2217\u2248\ud835\udc46), and PFCU pulls similar latent representation (i.e., \ud835\udc4f\ud835\udc59\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc51\u2192\u2190\ud835\udc4f\ud835\udc59\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc51, \ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d\u2192\u2190\ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d) and pushes dissimilar latent representation (i.e., \ud835\udc4f\ud835\udc59\ud835\udc62\ud835\udc5f\ud835\udc5f\ud835\udc52\ud835\udc51\u2190\u2192\ud835\udc60\u210e\ud835\udc4e\ud835\udc5f\ud835\udc5d) in the frequency domain. synthetic paired data to train a deep neural network (DNN) with various cost functions, and then learn an end-to-end mapping from blurred to sharp images. Note that a large amount of synthetic paired data is the key to the success of supervised BID methods. However, collecting paired data by hand is expensive, and strictly paired data is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S10",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "usually impossible in reality. Besides, realistic blur is more complex and diverse than the synthetic ones. Moreover, the synthetic data cannot reflect all blurs, resulting in the limitated performance of current supervised deblur methods in practical ap- plications. These challenges have given rise to the unsupervised deep BID methods [3, 20, 26, 51], which aim at learning the mapping of blurred images to sharp images without any paired data. Deep unsupervised BID methods are rarely studied due to higher difficulty and greater challenge, compared with the supervised mod- els. Specifically, due to the lack of strong constraints in unpaired case, researchers tend to design bulky deep models to seek weak connections between input and output, while this usually results in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S11",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "long inference time. As a result, deploying such bulky deep mod- els into mobile devices for online and real-time computation will be challenging. Besides, current unsupervised deep models per- form BID under numerous restrictions, e.g., specific image domain [20, 26], needing multiple frames for training [3] and small input resolutions [51]. Clearly, these will directly limit the development and real-world application of unsupervised BID models. In this paper, we therefore propose a simple and effective deep unsupervised BID baseline to address the aforementioned short- comings of current studies. From the performance point of view, the proposed baseline should satisfy the following conditions: 1) high model performance, i.e., achieving SOTA performance of unsu- pervised methods; 2) lightweight model size , i.e.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S12",
      "paper_id": "arxiv:2204.07820v2",
      "section": "introduction",
      "text": "meeting the configuration requirements of most devices at present; 3) fast in- ference speed, i.e., the baseline can work in a real-time situation, below 33.33 ms/frame. Besides, from the perspective of overcoming",
      "page_hint": null,
      "token_count": 32,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S13",
      "paper_id": "arxiv:2204.07820v2",
      "section": "limitations",
      "text": "1) no image domain limitation , i.e., can work on arbitrary im- age domain; 2) no image resolution limitation , i.e., can process high-resolution images; 3) no multi-frame input limitation , i.e., can perform single-input-single-output mapping. To illustrate our baseline meets the above conditions, we accept a high-resolution natural image as input and compare the SOTA\u2019s output in Figure 1. Clearly, our method outperforms SOTA in different aspects. The major contributions of this paper are summarized as follows: \u2022We propose a lightweight and real-time baseline (FCL-GAN) for unsupervised BID. FCL-GAN drives the task by a frequency- domain contrastive loss constrained lightweight CycleGAN, as shown in Figure 2. Two new collaborative units, called lightweight domain conversion unit (LDCU) and parameter- free",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S14",
      "paper_id": "arxiv:2204.07820v2",
      "section": "limitations",
      "text": "frequency-domain contrast unit (PFCU), are designed, which jointly makes sure \u201clightweight\u201d and \u201creal-time\u201d, in ad- dition to overcoming the limitations of current unsupervised methods. To the best of our knowledge, this is the first light- weight and real-time deep unsupervised BID method, which can be regarded as a guiding method for future research. \u2022Although CycleGAN [52] is a popular architecture trained without paired data by a cycle consistency loss, it still suffers from large chromatic aberration, severe artifacts and model redundancy when handling BID task (See Figure 3). As such, we disassemble and analyze the model structure and basic constituent units, and present a novel lightweight domain conversion unit (LDCU), which can deblur the degraded images better in a lightweight",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S15",
      "paper_id": "arxiv:2204.07820v2",
      "section": "limitations",
      "text": "and real-time manner. FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring ACM Conference\u201922, January, 2022, City, Country \u2022To further improve the deblur performance without extra pa- rameters, we introduce a new concept of frequency-domain contrastive learning (FCL), and present a parameter-free frequency-domain contrastive unit (PFCU) to measure the similarity between latent representations in frequency do- main as a contrastive constraint. FCL can also address the shortcomings of contrastive learning on deblur task. \u2022Extensive simulations on several datasets demonstrated the SOTA unsupervised deblur performance of our FCL-GAN, with stonger generalization ability to handle real-world blurs. To be specific, FCL-GAN allows for a smaller model size (24.6MB vs. 606.8MB) and less inference time (0.011s vs. 0.062s), compared with current",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S16",
      "paper_id": "arxiv:2204.07820v2",
      "section": "limitations",
      "text": "SOTA method.",
      "page_hint": null,
      "token_count": 2,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S17",
      "paper_id": "arxiv:2204.07820v2",
      "section": "related_work",
      "text": "2.1 Data-Driven Deep BID Deep fully-supervised BID. Benefiting from large-scale paired training data, supervised BID methods can learn accurate map- ping more easily. For example, a \u201cmulti-scale\u201d end-to-end training manner [25] was proposed for directly deblurring without blur kernels. Kupyn et al. [17, 18] leverages CGAN [23] and wgan-gp [12] to obtain visually realistic deblurring results in a generative- adversarial manner. Zhang et al. [50] provide a Real-World blurred images dataset and design an effective GAN-based network (DB- GAN) to model real-world blur. The \u201cmulti-patch\u201d strategy is also used to partition the image in spatial dimensions and then perform a coarse-to-fine progressive deblurring [48][46]. Instead of using convolution, Zamir et al. [45] introduce and refine the transformer [37] to deblur",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S18",
      "paper_id": "arxiv:2204.07820v2",
      "section": "related_work",
      "text": "high-resolution images, achieving state-of-the-art (SOTA) fully-supervised BID performance. Deep semi-supervised BID. Semi-supervised methods learn an approximate blurred-sharp mapping based on small-scale paired data and large-scale unpaired data. Compared with fully-supervised BID, it will be more difficult for the semi-supervised BID to learn a blurred-sharp mapping. Therefore, Nimisha et al. [ 21] first try to estimate the global camera motion from small-scale paired data in a semi-supervised manner and use the obtained global camera motion to perform single image deblurring and change detection. Deep unsupervised BID. The training of unsupervised meth- ods does not involve paired data, and instead, it uses large-scale un- paired data for deblurring. Compared with the fully-/semi-supervised BID, unsupervised BID methods are more difficult to learn an",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S19",
      "paper_id": "arxiv:2204.07820v2",
      "section": "related_work",
      "text": "ac- curate blurred-sharp mapping due to the weak constraint between blurred and sharp images. For example, a self-supervised optimiza- tion scheme [3] was proposed based on the existing deblur models, which utilizes the continuous frames of video and introduces a physically-based blur information model during training to improve the deblur performance. Strictly speaking, this is not an unsuper- vised BID technique since it is built on a fully supervised model and continuous frames. Based on the simple generative adversarial network (GAN) [11], Nimisha et al. [26] introduce scale-space gra- dient loss and reblurring loss to self-supervise the model to perform domain-specific deblurring. Lu et al. [20] entangle the content and blur of the blurred image over the domain-specific datasets, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S20",
      "paper_id": "arxiv:2204.07820v2",
      "section": "related_work",
      "text": "then the blur can be easily removed from the blurred image. Zhao et al. [51] focus on the chromatic aberration problem of unsupervised",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S21",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "rection strategy to maintain color information while deblurring, which achieves better unsupervised BID performance. 2.2 Contrastive Learning With the iteration of self-supervised and unsupervised techniques, contrastive learning has been attracting more and more attention. In general, contrastive learning aims to map the original data into a latent representation space in which anchors are pulled close to positive samples and pushed farther away from negative samples. In this way, the model can not only learn from positive signals but also benefits from correcting undesirable behaviors. In recent years, Contrastive learning has been widely used in various high-level vision tasks, e.g., object detection [41], medical image segmentation [2] and image caption [ 8], which has achieved superior perfor- mance in high-level tasks",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S22",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "and is receiving increasing attention. More recently, Contrastive learning has been successfully applied to various low-level vision tasks and achieved SOTA performance, e.g., image denoising [ 9], image dehazing [ 40] and image super- resolution [49]. Similarly, Chen et al. [6] first introduce contrastive learning to unsupervised single image deraining task and perform contrastive constraints at the feature level, which achieves SOTA performance in unsupervised single image deraining. 3 PROPOSED BASELINE METHOD 3.1 Architecture We show the architecture and learning process of our FCL-GAN in Figure 2. Clearly, it has two main cooperating units, i.e., LDCU and PFCU. LDCU is a lightweight unit that implements interconver- sion between different domain images. PFCU applies contrastive learning for deblurring at a technical",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S23",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "level, making the output an- chor closer to the positive sample. In what follows, we detail the interactive process between the LDCU and the PFCU. For ease of description, we will begin with some basic definitions: \u2022\u201cStream\u201d: The process of data transfer, e.g., \u201cpositive stream\u201d is used to transfer positive samples and positive latent repre- sentations. When the positive samples are sharp, the \u201cstream\u201d will be denoted as \u201csharp-guide stream\u201d. \u2022\u201cBuffer\u201d: The place where the samples used by the previous epochs are stored. Samples inside the \u201cbuffer\u201d are treated as negative samples. For example, for the \u201csharp-guide\u201d streams, samples inside the \u201cbuffer\u201d are all blurred. LDCU is the basis of the whole architecture of FCL-GAN, which implements domain conversion. As",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S24",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "shown in Figure 2, the LDCU contains two branches, i.e., the deblur branch: \ud835\udc35\u2192\ud835\udc46\ud835\udc35\u2192\ud835\udc35\u2217\u2248\ud835\udc35and the reblur branch: \ud835\udc46\u2192\ud835\udc35\ud835\udc46\u2192\ud835\udc46\u2217\u2248\ud835\udc46. Similarly, PFCU will impose dif- ferent contrastive constraints for the two branches, i.e., sharp-guide contrast constraint for the deblur branch, while using blurred-guide contrast constraint for the reblur branch. Let\u2019s take the deblur branch as an example, the latent images\ud835\udc46\ud835\udc35, the sharp images \ud835\udc46and the images in the sharp-guide negative buffer will be transferred to the PFCU as the anchor, positive samples and negative samples, respectively, to obtain the latent representations. Then, the similar- ity are calculated and contrastive constraints are performed. Note that the framework of FCL-GAN is lightweight and real-time, since ACM Conference\u201922, January, 2022, City, Country Suiyi Zhao, Zhao",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S25",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng Wang LDCU is lightweight and real-time, and PFCU does not involve extra parameters and calculations during the inference process. 3.2 Lightweight Domain Conversion Unit Let D\ud835\udc35 be the blurred image domain, and let D\ud835\udc46 be the sharp im- age domain, our ultimate goal is to map the images in D\ud835\udc35 without ground truth to D\ud835\udc46. To this end, we introduce LDCU as the back- bone of our FCL-GAN. LDCU contains two generators (\ud835\udc3a\ud835\udc352\ud835\udc46 and \ud835\udc3a\ud835\udc462\ud835\udc35) and two discriminators (\ud835\udc37\ud835\udc35 and \ud835\udc37\ud835\udc46): \ud835\udc3a\ud835\udc352\ud835\udc46 (\ud835\udc3a\ud835\udc462\ud835\udc35) for map- ping images in D\ud835\udc35 (D\ud835\udc46) to D\ud835\udc46 (D\ud835\udc35); \ud835\udc37\ud835\udc35 (\ud835\udc37\ud835\udc46) for discriminating the authenticity of images in D\ud835\udc35 (D\ud835\udc46). Motivated by [6, 39], two functional circuits",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S26",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "are set to perform deblurring and blur genera- tion. Taking the deblurring circuit as an example, given images \ud835\udc35 in D\ud835\udc35, \ud835\udc3a\ud835\udc352\ud835\udc46 maps \ud835\udc35to images \ud835\udc46\ud835\udc35 in D\ud835\udc46, and then \ud835\udc3a\ud835\udc462\ud835\udc35 remaps \ud835\udc46\ud835\udc35 to images \ud835\udc35\u2217in D\ud835\udc35, i.e., \ud835\udc35\u2192\ud835\udc46\ud835\udc35\u2192\ud835\udc35\u2217\u2248\ud835\udc35. Throughout the deblurring circuit, \ud835\udc37\ud835\udc46 is used to discriminate whether \ud835\udc46\ud835\udc35 is truly an image in D\ud835\udc46. Similar to the deblurring circuit, the blur generation circuit accepts images in D\ud835\udc46 for the opposite mapping, i.e., \ud835\udc46\u2192\ud835\udc35\ud835\udc46\u2192\ud835\udc46\u2217\u2248\ud835\udc46. In general, converting images between D\ud835\udc35 and D\ud835\udc46 can be re- garded as an image-to-image translation task. As a classical image- to-image translation framework in this domain, CycleGAN [52] has a very powerful ability to learn inter-domain differences. However, CycleGAN cannot achieve the expected deblurring",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S27",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "result, and there may be several potential reasons for this: 1) Incompatibility of network architectures . Unlike the image-to-image translation task, the network architecture of the deblurring method is elabo- rated and sometimes a slight change can severely disrupt the results, e.g., changing BN to IN, which can be seen in ablation studies; 2) Different degrees of task complexity . Image to image transla- tion tasks (e.g., zebra\u2194horse, orange\u2194apple and summer\u2194winter) tend to have a straightforward inter-domain difference, so the deep network can easily learn precise inter-domain difference. However, the inter-domain difference for image deblurring is usually complex, which can be reflected by the blurring degree. As exemplified by the blurring degree in extreme cases, when the blurring degree is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S28",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "very large, one cannot even read any information in the blurred image; when the blurring degree is very small, the blurred image can even be regarded as a sharp image. Therefore, directly migrat- ing CycleGAN to the deblurring task is not feasible and will lead to a series of negative effects, such as chromatic aberration, severe artifacts and parameter redundancy, as shown in Figure 3. As such, we strive to make the network lightweight and more effective. Specifically, we perform a fully recursive decomposition of entire LDCU, carefully design each minimal component, and fi- nally reconstitute LDCU with these elaborate components. We will mainly focus on refining the generator in LDCU, since it takes up the direct representation of domain",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S29",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "conversion, as the key component of LDCU. Generally speaking, the number of generator parame- ters is much higher than that of the discriminator in the whole generation-discrimination structure because a sufficient number of parameters will better exploit the powerful nonlinear mapping capability of DNN. However, the generator will tend to saturate when the number of parameters reaches a certain level. Thus, a larger number of parameters will make the model redundant, which will seriously hinder the inference and deployment of the model. Blurred CycleGAN [52] Ours GoPro [25]CelebA [19] Model size (MB) = (107.90 vs. 24.56), CSE (ratio) \u22481.5 Figure 3: Visualizing the negative effects when CycleGAN [52] is applied directly to the deblurring task. We can see that CycleGAN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S30",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "often results in severe chromatic aberrations (both two groups are about 1.5 times larger than ours), model redundancy (107.90 MB vs. Our 24.56 MB) and artifacts. To minimize model redundancy, we introduce and perform the following operations from bottom to top: 1) meta design; 2) light- weight structure design. Next, we detail the operations. Meta design. Convolutional neural networks (CNN) consist of a stack of non-divisible units, e.g., convolution (Conv), batch normalization (BN), instance normalization (IN) and Rectified linear unit (ReLU). However, efficiently organizing these non-divisible units to build a deblurring network is a problem worth exploring. Conv and ReLU are necessary, because they respectively support the CNN\u2019s parametric learning capability and nonlinear fitting capability. Note that the normalization unit",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S31",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "is a mandatory factor. For supervised deblurring, researchers usually did not use the normalization units for model\u2019s structure design, since adding ad- ditional normalization units under the condition of adequate con- straints may severely hinder the model to learn an accurate 1-to-1: blurred\u2192sharp mapping [7, 25, 30, 35, 36, 46, 48]. However, in some other fields (e.g., image-to-image translation and style transfer), re- searchers also used IN for model\u2019s structure design, because these tasks expect to learn a 1-to-1 style mapping [44, 47, 52]. However, in unsupervised deblurring, the lack of strong constraints and the diversity of blurring degrees lead to n-to-n style mapping. Besides, BN normalizes to multiple samples (mini-batch) instead of IN, and is more likely to learn",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S32",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "global differences among domains. To this end, we introduce the concepts of basic and residual metas to learn this weakly-constrained style mapping based on BN, as shown in Figure 4 (a) and (e). We have analyzed the importance of our proposed basic and residual metas in detail in ablation studies. Lightweight structure design. In the field of image restoration, many efforts have been devoted to designing complex models to ob- tain desired results. However, no matter how complex the model is, they are all based on two underlying structures, i.e., encode-decoder structure [25] (see Figure 5 (b)) or single-scale pipeline [31] (see Figure 5 (a)). The encoder-decoder structure can effectively abstract the content information but cannot maintain the spatial details",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S33",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "of the images. The single-scale pipeline can ensure accurate spatial information, but cannot easily abstract the image contents. With FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring ACM Conference\u201922, January, 2022, City, Country Conv IN ReLU (a) (b) (c) Basic Meta Residual Meta (d) (e) Conv ReLU Conv (f) Conv ReLU Conv BN ReLU BasicMeta Conv Norm BasicMeta Conv Norm ReLU Performance epoch0 lr\u2193 lr\u2193 BN Norm-free Performance epoch0 lr\u2193 lr\u2193 BN Norm-free (g) Figure 4: Different forms of basic and residual metas. \"Norm\" means the normalization in basic beta, which can be BN, IN or norm-free. LDCU is based on (a) and (e). (g) is a schematic diagram for norm-free that leads to instability during training.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S34",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S35",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "output",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S36",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "(a)",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S37",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S38",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "output",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S39",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "(a)",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S40",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input output (b)",
      "page_hint": null,
      "token_count": 3,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S41",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input output (b)",
      "page_hint": null,
      "token_count": 3,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S42",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input output (c)",
      "page_hint": null,
      "token_count": 3,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S43",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "input output (c) BasicMeta BasicMeta ResMetas ResMetas Skip ConnectionSkip Connection Figure 5: Different structures of DNN models. ResMetas con- tains several residual metas. Only two basic metas are shown when coding for simplifying the schematic. the same settings, the number of parameters is the same as that of encoder-decoder structure. Nevertheless, from the viewpoint of inference speed, without the encoding-decoding process, the model based on single-scale pipeline forward at the original resolution, which can reduce the inference speed and affect the real-time capa- bility. To simplify the descriptions, we next use encode-decoder and single-scale to represent these two structures. Considering the strengths and weaknesses of the two existing structures, Zamir et al. [46] add an additional original resolution module after",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S44",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "encoder-decoder, which balances the advantages of both, but also involves extra parameters and inference time. Inspired by Cho et al. [7], we introduce a new structure termed lightweight encoder-decoder (LED)to incorporate the strengths of both encoder- decoder and single-scale, as shown in Figure 5 (c). Compared with encoder-decoder, LED stacks residual metas at a larger resolution, which will be more favorable for preserving the spatial details. Com- pared with single-scale, LED contains both encoding and decoding, which facilitates the abstraction of the image content. Figure 6 (a) illustrates the difference between LED and encoder- decoder, i.e., the ResMetas of encoder-decoder are all gathered in the deep layer, while our LED\u2019s can be distributed in the shallow layer. Figures 6 (b)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S45",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "and (c) compare the number of parameters and the number of calculations for both structures under the same setting. Note that it is known that for the number of parameters: encoder-decoder\u2019s = single-scale\u2019s; for the number of calculations: encoder-decoder\u2019s < single-scale\u2019s. However, as we can see from ResMetas in Encoder -decoderResMetas in LED ResMetas in Encoder -decoderResMetas in LED input shadow deep (b, c, h, w) (b, 2c, h//2, w//2) (b, 4c, h//4, w//4) output shadow l l l l l l (b, 4c, h//4, w//4) (b, 2c, h//2, w//2) (b, 2c, h//2, w//2) 2 1 1 2 (a) (b) (c) The number of paremeters (kernel k * k) 1 2 4c * 4c * k * k = 16c",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S46",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "2 k 2 2c * 2c * k * k = 4c 2 k 2 c * c * k * k = c 2 k 2 The number of paremeters (kernel k * k) 1 2 4c * 4c * k * k = 16c 2 k 2 2c * 2c * k * k = 4c 2 k 2 c * c * k * k = c 2 k 2 T he amount of calculations (kernel k * k) 1 2 4c * 4c * k * k * 1/4h * 1/4w = c 2 k 2 hw 2c * 2c * k * k * 1/2h * 1/2w = c 2 k 2 hw c *",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S47",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "c * k * k * h * w = c 2 k 2 hw T he amount of calculations (kernel k * k) 1 2 4c * 4c * k * k * 1/4h * 1/4w = c 2 k 2 hw 2c * 2c * k * k * 1/2h * 1/2w = c 2 k 2 hw c * c * k * k * h * w = c 2 k 2 hw Figure 6: (a) Difference between LDE and encoder-decoder. (b) and (c) compare the number of parameters and the number of calculations brought by the red and blue arrows. Figure 6, LED\u2019s parameters are much lower than that of the encoder- decoder and the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S48",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "number of calculations is the same. At this point, we can conclude that LED considers the advantages of encoder- decoder and single-pipeline while making the model lightweight without introducing extra calculations. Finally, we will illustrate the effectiveness of our LED in ablation studies. 3.3 Parameter-Free Frequency-Domain Contrastive Unit (PFCU) The difference between blurred and sharp images is hard to explain in the spatial domain. However, the difference in the frequency domain can be explained by the fact that the blurred image loses the high-frequency signal in a sharp image. A few efforts have been made to investigate the frequency domain in the BID task, but they are limited to only two ways: either using constraints in the frequency domain between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S49",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "the latent image and ground truth [7], or integrating Fast Fourier transform (FFT) and inverse Fast Fourier transform (IFFT) in the model [22]. However, these approaches are ground truth-dependent and difficult to understand. We have studied a large number of blurred and sharp images in the frequency domain and found that: without normalizing the images, the blurred images tended to be all black, however the sharp images tended to be all white. Besides, the higher the degree of blurring, the darker the image, as shown in Figure 7. However, it is ACM Conference\u201922, January, 2022, City, Country Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng Wang High degree Low degree blurredsharp Figure 7: Different degrees of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S50",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "blurred and sharp images in the frequency domain without normalization. very challenging to directly measure the difference between blurred and sharp frequency domain images in an unsupervised manner. Therefore, we put our hopes on powerful contrastive learning. Two key questions need to be addressed to fully exploit the effect of contrastive learning: 1) How to get the latent representation of samples? 2) How to calculate the similarity between the latent representations? Note that DCD-GAN [6] used additional branch to obtain potential representation and used the cosine similarity function to define distances. However, this approach will make the model tend to update the extra branches and ignore the task itself. To address the above problems well, based on the principle of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S51",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "lightweight, we introduce a novel parameter-free frequency-domain contrast unit (i.e., PFCU) for contrastive learning in frequency do- main. Figure 2 shows the structure of PFCU in detail, which consists of several layers, i.e., the FFT layer, modulus-performing layer, bi- narization layer and de-marginalization layer. Specifically, given an input sample \ud835\udc3c, the FFT layer firstly performs a fast Fourier trans- form on \ud835\udc3cto obtain the frequency domain output\ud835\udc3c\ud835\udc53 of complex type. The modulus-performing layer then modulos \ud835\udc3c\ud835\udc53 to obtain the real space representation \ud835\udc3c\ud835\udc5f. The binarization layer aims to binarize \ud835\udc3c\ud835\udc5f according to a threshold value (zero) to obtain binarized \ud835\udc3c\ud835\udc4f. Finally, the de-marginalization layer aims to preserve \ud835\udc3c\ud835\udc4f\u2019s central region to further highlight the frequency domain variation and obtain the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S52",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "latent representation \ud835\udc43\ud835\udc3c of input \ud835\udc3c. We express the whole process of obtaining the latent representation by the following formula: \ud835\udc43\ud835\udc3c = \ud835\udc43\ud835\udc39\ud835\udc36\ud835\udc48 (\ud835\udc3c). (1) After obtaining the potential representation of samples, calcu- lating the similarity will be the most critical issue to be addressed. Note that it is unfeasible to use the traditional cosine similarity di- rectly because there are no learnable parameters in the calculation of latent representations, and two frequency domain images with small differences may have a low similarity score, which is certainly unreasonable (See Figure 8). For this purpose, considering the differ- ence in black coverage between various latent representations, we design a new similarity measurement method. Specifically, given two latent representations \ud835\udc431, \ud835\udc432 \u2208R\u210e\u00d7\ud835\udc64",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S53",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "and a hyperparameter \ud835\udf14, we first chunk \ud835\udc431, \ud835\udc432 to obtain \ud835\udc43\ud835\udc4f1, \ud835\udc43\ud835\udc4f2 \u2208R \u210e\u00d7\ud835\udc64 \ud835\udf142 \u00d7\ud835\udf14\u00d7\ud835\udf14, and then calculate the black coverage for each chunk to obtain \ud835\udc45\ud835\udc4f1, \ud835\udc45\ud835\udc4f2 \u2208 R \u210e\u00d7\ud835\udc64 \ud835\udf142 . The black coverage for each chunk is calculated as follows: 0.9091 (0.7253) 0.9634 (0.0502) 0.8118 (0.4208) 0.2509 (0.1054) 0.4636 (0.3814) 0.6615 (0.0879) Our Similarity vs. ( Cosine Similarity ) Figure 8: Validation of our similarity against cosine similar- ity in the frequency domain. The numbers on lines is the similarity between the two representations. The most unrea- sonable value of the cosine similarity is highlighted in red. \ud835\udc45\ud835\udc4f = \u00cd\ud835\udf14 \ud835\udc56=0 \u00cd\ud835\udf14 \ud835\udc57=0(1 \u2212\ud835\udc4f\ud835\udc56,\ud835\udc57) \ud835\udf142 , (2) where \ud835\udc4f\u2208R\ud835\udf14\u00d7\ud835\udf14 is one chunk of the chunked",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S54",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "latent representation (e.g., \ud835\udc43\ud835\udc4f1 and \ud835\udc43\ud835\udc4f2 above),\ud835\udc4f\ud835\udc56,\ud835\udc57 is the\ud835\udc56-th row and\ud835\udc57-th column element of \ud835\udc4f, and \ud835\udf14 are the size of \ud835\udc4f, respectively. Then, we can define and calculate the similarity based on the black coverage \ud835\udc45\ud835\udc4f1 and \ud835\udc45\ud835\udc4f2 in frequency domain as follows: \ud835\udc46\ud835\udc56\ud835\udc5a = 1 \u2212\ud835\udc40\ud835\udc46\ud835\udc38(\ud835\udc45\ud835\udc4f1,\ud835\udc45\ud835\udc4f2). (3) For ease of understanding, we represent the entire process of calculating the similarity between two different latent representa- tions (\ud835\udc431, \ud835\udc432) by \ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5d1,\ud835\udc5d2). We show some results to demonstrate the effectiveness of our similarity measurement method against the coisine similarity in Figure 8. After solving the latent representation acquisition and similarity measure problems, we can easily apply contrastive learning to our FCL-GAN. Specifically, given an anchor \ud835\udc5d, a positive exemplar \ud835\udc5d+and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S55",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "several negative exemplars \ud835\udc5d\u2212, we include the following contrastive loss in training: L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61 (\ud835\udc5d,\ud835\udc5d+,\ud835\udc5d\u2212)= \u2212\ud835\udc59\ud835\udc5c\ud835\udc54[ \ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5d,\ud835\udc5d+)/\ud835\udf0f \ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5d,\ud835\udc5d+)/\ud835\udf0f +\u00cd\ud835\udc41 \ud835\udc5b=1 \ud835\udc52\ud835\udc60\ud835\udc56\ud835\udc5a(\ud835\udc5d,\ud835\udc5d\u2212)/\ud835\udf0f)], (4) where \ud835\udc41 is the number of negative exemplars and \ud835\udf0f is the tempera- ture coefficient that is set to 0.07 in all experiments. 3.4 Loss function Except for the contrastive loss L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61, we also introduce adversarial loss L\ud835\udc4e\ud835\udc51\ud835\udc63, cycle-consistency loss L\ud835\udc50\ud835\udc50 and TV regularization L\ud835\udc61\ud835\udc63 for deblurring. L\ud835\udc61\ud835\udc63 is applied only to restored sharp images and other losses to both the sharp and blurred domains. The total loss function is: L\ud835\udc61\ud835\udc5c\ud835\udc61\ud835\udc4e\ud835\udc59=L\ud835\udc4e\ud835\udc51\ud835\udc63+10 \u2217L\ud835\udc50\ud835\udc50+0.1 \u2217L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61+0.1 \u2217L\ud835\udc61\ud835\udc63.",
      "page_hint": null,
      "token_count": 95,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S56",
      "paper_id": "arxiv:2204.07820v2",
      "section": "experiments",
      "text": "4.1 Experimental Settings Datasets. In this paper, we evaluate each BID method on four widely-used image datasets, incuding a natural image dataset GoPro [25], a human-aware blurring dataset HIDE [ 33], a human face FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring ACM Conference\u201922, January, 2022, City, Country Table 1: Performance comparison on two benchmark datasets: GoPro [25] and HIDE [33]. Datasets GoPro HIDE Runtime (ms) Model Size (MB)Metrics PSNR/SSIM\u2191CSE (Ratio)\u2193NIQE\u2193PSNR/SSIM\u2191CSE (Ratio)\u2193 Gong et al. [10] 26.40/0.863 / / / / / 39.00 Deep supervised DeepDeblur [25] 29.08/0.914 / 4.921 25.73/0.874 / 4330 89.40",
      "page_hint": null,
      "token_count": 97,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S57",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "CycleGAN [52] 22.54/0.720 2.676 4.359 21.81/0.690 3.300 12.55 107.90 Deep unsupervised DualGAN [44] 22.86/0.722 4.384 4.176 / / / 324.20",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S58",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "Ours 24.84/0.771 1 3.924 23.43/0.732 1 10.86 24.56 GT Blurred Krishnan et al. [16] CycleGAN [52] UID-GAN [20] Ours Figure 9: Visual comparison of image deblurring on the GoPro dataset [25]. GT Blurred Krishnan et al. [16] CycleGAN [52] OursUID-GAN [20] Figure 10: Visual comparison of image deblurring on the CelebA dataset [19]. Table 2: Performance comparison on CelebA [19]. Metrics PSNR/SSIM\u2191CSE (Ratio)\u2193 Pan et al. [27] 15.16/0.380 / Xu et al. [42] 16.84/0.470 / Optimization- Pan et al. [28] 17.34/0.520 / based methods Pan et al. [29] 17.59/0.540 / Krishnan et al. [16] 18.51/0.560 / Deep supervi- DeblurGAN [17] 18.86/0.540 / sed methods DeepDeblur [25] 18.26/0.570 / CycleGAN [52] 19.40/0.560 1.918 Deep unsuper- UID-GAN [20] 20.81/0.650 2.279 vised methods",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S59",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "Ours 21.07/0.652 1 dataset CelebA (domain-specific) [19], and a real-world blur dataset RealBlur (namely, RealBlur-J and RealBlur-R) [32]. Evaluation Metrics. We use two widely-used reference metrics (PSNR and SSIM [38]), one non-reference metric (NIQE [24]) and one chromatic aberration metric (i.e., color sensitive error, CSE [51]) for evaluations. We also compare the model size to measure the lightweight property. Besides, to measure the real-time performance of each model, we compare the inference time on Nvidia RTX 2080 Ti. In our experimental results: the symbol \u2191means the higher, the better, while the symbol \u2193means the lower, the better. Compared Methods. We compare our FCL-GAN with 13 methods, including five optimization-based methods: [ 27], [28], [29], [42], [16]; four data-driven deep supervised",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S60",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "methods:[10], DeblurGAN [17], DeepDeblur [25], SRN [35]; three data-driven deep unsuper- vised methods: DualGAN [44], CycleGAN [52], UID-GAN [20]. We prefer to use the pre-trained model. However, if the settings are inconsistent or there is no pre-trained model, we will retrain it using the code provided by the authors. Implementation Details. The proposed baseline model is imple- mentated based on PyTorch version 1.10 and Nvidia RTX 3090 with 24G memory. We set 80 epochs for training, using Adam [15] with \ud835\udefd1=0.5 and \ud835\udefd2=0.999 for optimization. The initial learning rate was set to 0.0001, which was reduced by half every 20 epochs. ACM Conference\u201922, January, 2022, City, Country Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng Wang",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S61",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "GT Blurred Krishnan et al. [16] CycleGAN [52] UID-GAN [20] Ours Figure 11: Visual comparison of image deblurring on the RealBlur-J dataset[32]. Table 3: Performance comparison on RealBlur [32]. Datasets RealBlur-J RealBlur-R Metrics PSNR/SSIM\u2191CSE\u2193PSNR/SSIM\u2191CSE\u2193 Super- DeblurGAN [17] 27.97/0.834 / 33.79/0.903 / vised DeepDeblur [25] 27.87/0.827 / 32.51/0.841 / CycleGAN [52] 19.79/0.633 2.898 12.38/0.242 3.024 Unsup- UID-GAN [20] 22.87/0.671 1.394 16.64/0.323 2.985 ervised Ours 25.35/0.736 1 28.37/0.663 1 4.2 Experimental Results 1) Result on GoPro. We train our model on the training set of GoPro and evaluate it on the test set, as shown in Table 1. Note that CSE metric is calculated based on the deblurred image and ground- truth for evaluating the performance of each deep unsupervised deblur model",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S62",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "on each dataset, and we use the ratio of the CSE result of other methods to our model for comparison. We see that our FCL- GAN substantially outperforms the existing deep unsupervised BID",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S63",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "significant advantage in inference time and model size. 2) Result on HIDE. To quantitatively and qualitatively compare the generalization ability of each model, we directly apply the model pre-trained on GoPro to the HIDE dataset. As shown in Table 1 and Figure 9, our proposed FCL-GAN has more strong generalization ability and outperforms all data-driven deep unsupervised methods. We are also surprised that our FCL-GAN obtains highly competitive results from some deep supervised deblur models. 3) Result on CelebA. To evaluate the ability of each method for processing domain-specific blurred images, we train and test each model on CelebA face dataset. From Table 2 and Figure 10, our",
      "page_hint": null,
      "token_count": 108,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S64",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "all unsupervised methods again, and deep unsupervised methods tend to outperform some deep supervised methods in this case. 4) Result on RealBlur. To examine the ability of each model to handle real-world blurs, we directly apply the model pre-trained on GoPro to the RealBlur dataset, as shown in Table 3 and Figure 11. From Table 3, we see that our method outperforms all unsupervised methods. Figure 11 shows that our method can better handle real- world blurring images. Table 4: Ablation studies for loss functions on the GoPro dataset [25] and CelebA dataset [19]. Datasets Model \ud835\udc64/\ud835\udc5cL\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61 \ud835\udc64/\ud835\udc5cL\ud835\udc61\ud835\udc63 Ours GoPro PSNR/SSIM 24.56/0.749 24.73/0.765 24.84/0.771 CelebA PSNR/SSIM 20.83/0.648 21.01/0.651 21.07/0.652 Table 5: Ablation studies for different combinations of basic metas and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S65",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "residual metas on GoPro [25]. Combination (a)+(d) (a)+(e) (b)+(d) (b)+(e) PSNR/SSIM 23.61/0.736 24.84/0.771 19.40/0.635 20.11/0.641 Combination (c)+(d) (c)+(e) (c)+(f) PSNR/SSIM 23.47/0.729 24.55/0.765 24.20/0.762 4.3 Ablaiton Studies 1) Effectiveness of loss function. To verify the effectiveness of the used loss functions in training, we ablate L\ud835\udc61\ud835\udc63 and L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61 on the GoPro and CelebA datasets. From the experimental results in Table 4, we see that both L\ud835\udc61\ud835\udc63 and L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61 have positive effects, and removing L\ud835\udc50\ud835\udc61\ud835\udc60\ud835\udc61 has a more negative impact on the performance. 2) Effectiveness of designed basic meta and residual meta. In this study, we design three different forms for verification, as shown in Figure 4. For the basic meta, we consider the following three forms: (a) Conv-BN-ReLU, (b) Conv-IN-ReLU, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S66",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "(c) Conv-ReLU. While for the residual meta, we also consider three forms: (d) is the basic form of resblock [13]; (e) is a simplified version of (d); (f) is the most widely used form in the field of deblurring [7, 25, 48]. Table 5 shows the performance of various collaborative effects of basic meta and residual meta. We have the following conclusions which can be applied to unsupervised deblurring: 1) for the gain on performance: BN > norm-free > IN; 2) the introduction of IN substantially degrades the performance; 3) the simplified version of Residual meta performs better. Besides, norm-free seems to have the same effect as BN. However, norm-free leads to instability of training in experiments, as shown in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S67",
      "paper_id": "arxiv:2204.07820v2",
      "section": "method",
      "text": "Figure 4 (g). 3) Effectiveness of LED structure. We compare the deblurring performance, model size, and inference time of the three structures in Figure 5. Table 6 describes the comparison results. As can be seen, our LED is more light, infers faster, and performs better. FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring ACM Conference\u201922, January, 2022, City, Country Table 6: Ablation studies for structures on GoPro [25]. Structures PSNR/SSIM Model Size Runtime Encoder-decoder 24.66/0.763 50.30 (MB) 15.19 (ms) Single-scale 24.69/0.770 50.30 (MB) 76.94 (ms) Our LED 24.84/0.771 24.56 (MB) 10.86 (ms)",
      "page_hint": null,
      "token_count": 95,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S68",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "We have discussed the limitations of existing unsupervised deep BID methods, and technically proposed a lightweight and real-time unsupervised BID baseline (FCL-GAN). We analyze the blurred and sharp images in the frequency domain and introduce frequency- domain contrastive learning to obtain superior performance. Qual- itative and quantitative results show that our method achieves SOTA performance for the unsupervised BID and is even highly competitive with some supervised deep BID models on the domain- specific case. In terms of lightweight and real-time inference per- formance, our FCL-GAN method outperforms all existing image deblurring models (no matter supervised or unsupervised). Further- more, our method only needs 0.011s to process a high-resolution im- age (1280x720) on an Nvidia RTX 2080Ti. In future, we",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S69",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "will consider the deployment issue of our lightweight model, and also explore new stategies to furher imrpove the deblurring performance. 6 ACKNOWLEDGMENTS This work is partially supported by the National Natural Science Foundation of China (62072151, 61732007, 61932009 and 62020106007), and the Anhui Provincial Natural Science Fund for Distinguished Young Scholars (2008085J30). Zhao Zhang is the corresponding author of this paper. REFERENCES [1] Yuanchao Bai, Huizhu Jia, Ming Jiang, Xianming Liu, Xiaodong Xie, and Wen Gao. 2020. Single-Image Blind Deblurring Using Multi-Scale Latent Structure Prior. IEEE Trans. Circuits Syst. Video Technol.30, 7 (2020), 2033\u20132045. [2] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender Konukoglu. 2020. Contrastive learning of global and local features for medical image segmentation with limited annotations. In",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S70",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Proceedings of the Annual Conference on Neural Information Processing Systems, virtual. [3] Huaijin G. Chen, Jinwei Gu, Orazio Gallo, Ming-Yu Liu, Ashok Veeraragha- van, and Jan Kautz. 2018. Reblur2Deblur: Deblurring videos via self-supervised learning. In Proceedings of the IEEE International Conference on Computational Photography, Pittsburgh, PA, USA. 1\u20139. [4] Liang Chen, Faming Fang, Tingting Wang, and Guixu Zhang. 2019. Blind Image Deblurring With Local Maximum Gradient Prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA. 1742\u20131750. [5] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. 2021. HINet: Half Instance Normalization Network for Image Restoration. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, virtual. 182\u2013192. [6]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S71",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Xiang Chen, Jinshan Pan, Kui Jiang, Yufeng Li, Yufeng Huang, Caihua Kong, Longgang Dai, and Zhentao Fan. 2022. Unpaired Deep Image Deraining Using Dual Contrastive Learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. [7] Sung-Jin Cho, Seo-Won Ji, Jun-Pyo Hong, Seung-Won Jung, and Sung-Jea Ko. 2021. Rethinking Coarse-to-Fine Approach in Single Image Deblurring. In Proceedings of the IEEE International Conference on Computer Vision, Montreal, QC, Canada. 4621\u20134630. [8] Bo Dai and Dahua Lin. 2017. Contrastive Learning for Image Captioning. In Proceedings of the Annual Conference on Neural Information Processing Systems, Long Beach, CA, USA. 898\u2013907. [9] Nanqing Dong, Matteo Maggioni, Yongxin Yang, Eduardo P\u00e9rez-Pellitero, Ales Leonardis, and Steven McDonagh. 2021. Residual Contrastive Learning for Joint",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S72",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Demosaicking and Denoising. CoRR abs/2106.10070 (2021). [10] Dong Gong, Jie Yang, Lingqiao Liu, Yanning Zhang, Ian D. Reid, Chunhua Shen, Anton van den Hengel, and Qinfeng Shi. 2017. From Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA. 3806\u20133815. [11] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde- Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Proceedings of the Annual Conference on Neural Information Processing Systems, Montreal, Quebec, Canada. 2672\u20132680. [12] Ishaan Gulrajani, Faruk Ahmed, Mart\u00edn Arjovsky, Vincent Dumoulin, and Aaron C. Courville. 2017. Improved Training of Wasserstein GANs. In Pro-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S73",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "ceedings of the Annual Conference on Neural Information Processing Systems, Long Beach, CA, USA. 5767\u20135777. [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA. 770\u2013778. [14] Neel Joshi, C. Lawrence Zitnick, Richard Szeliski, and David J. Kriegman. 2009. Image deblurring and denoising using color priors. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Miami, Florida, USA. 1550\u20131557. [15] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimiza- tion. In Proceedings of the 3rd International Conference on Learning Representations, San Diego, CA, USA. [16] Dilip Krishnan, Terence Tay,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S74",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "and Rob Fergus. 2011. Blind deconvolution using a normalized sparsity measure. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, USA. 233\u2013240. [17] Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Jiri Matas. 2018. DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA. 8183\u20138192. [18] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. 2019. DeblurGAN-v2: Deblurring (Orders-of-Magnitude) Faster and Better. In Pro- ceedings of the IEEE International Conference on Computer Vision, Seoul, Korea (South). 8877\u20138886. [19] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S75",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "the IEEE International Conference on Computer Vision, Santiago, Chile. 3730\u20133738. [20] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. 2019. Unsupervised Domain- Specific Deblurring via Disentangled Representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA. 10225\u201310234. [21] Nimisha T. M, Vijay Rengarajan, and Rajagopalan Ambasamudram. 2018. Semi- Supervised Learning of Camera Motion from A Blurred Image. In Proceedings of the IEEE International Conference on Image Processing, Athens, Greece. 803\u2013807. [22] Xintian Mao, Yiming Liu, Wei Shen, Qingli Li, and Yan Wang. 2021. Deep Residual Fourier Transformation for Single Image Deblurring.CoRR abs/2111.11745 (2021). [23] Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial Nets. CoRR abs/1411.1784 (2014). [24] Anish Mittal, Anush Krishna Moorthy, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S76",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Alan Conrad Bovik. 2012. No- Reference Image Quality Assessment in the Spatial Domain. IEEE Trans. Image Process. 21, 12 (2012), 4695\u20134708. [25] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. 2017. Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA. 257\u2013265. [26] Thekke Madam Nimisha, Sunil Kumar, and A. N. Rajagopalan. 2018. Unsupervised Class-Specific Deblurring. In Proceedings of the 15th European Conference, Munich, Germany. 358\u2013374. [27] Jin-shan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. 2014. Deblurring Face Images with Exemplars. In Proceedings of the 13th European Conference, Zurich, Switzerland. 47\u201362. [28] Jin-shan Pan, Zhe Hu, Zhixun Su, and Ming-Hsuan Yang. 2014. Deblurring Text",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S77",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Images via L0-Regularized Intensity and Gradient Prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH, USA. 2901\u20132908. [29] Jin-shan Pan, Deqing Sun, Hanspeter Pfister, and Ming-Hsuan Yang. 2016. Blind Image Deblurring Using Dark Channel Prior. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA. 1628\u20131636. [30] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young Chun. 2020. Multi- Temporal Recurrent Neural Networks for Progressive Non-uniform Single Image Deblurring with Incremental Temporal Training. In Proceedings of the 16th Euro- pean Conference, Glasgow, UK. 327\u2013343. [31] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. 2019. Progressive Image Deraining Networks: A Better and Simpler Baseline. In Proceedings",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S78",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "of the IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA. 3937\u20133946. ACM Conference\u201922, January, 2022, City, Country Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Yi Yang, and Meng Wang [32] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. 2020. Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms. In Proceed- ings of the 16th European Conference, Glasgow, UK. 184\u2013201. [33] Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling, Tingfa Xu, and Ling Shao. 2019. Human-Aware Motion Deblurring. InProceedings of the IEEE International Conference on Computer Vision, Seoul, Korea (South). 5571\u20135580. [34] Maitreya Suin, Kuldeep Purohit, and A. N. Rajagopalan. 2020. Spatially-Attentive Patch-Hierarchical Network for Adaptive Motion Deblurring. In Proceedings of the IEEE",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S79",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Conference on Computer Vision and Pattern Recognition, Seattle, W A, USA. 3603\u20133612. [35] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. 2018. Scale- Recurrent Network for Deep Image Deblurring. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA. 8174\u20138182. [36] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi Tsai, and Chia-Wen Lin. 2021. BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring. CoRR abs/2101.07518 (2021). [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of the Annual Conference on Neural Information Process- ing Systems, Long Beach, CA, USA. 5998\u20136008. [38] Zhou Wang,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S80",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity.IEEE Trans. Image Process. 13, 4 (2004), 600\u2013612. [39] Yanyan Wei, Zhao Zhang, Yang Wang, Mingliang Xu, Yi Yang, Shuicheng Yan, and Meng Wang. 2021. DerainCycleGAN: Rain Attentive CycleGAN for Single Image Deraining and Rainmaking. IEEE Trans. Image Process.30 (2021), 4788\u20134801. [40] Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. 2021. Contrastive Learning for Compact Single Image Dehazing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, virtual. 10551\u201310560. [41] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo Li, and Ping Luo. 2021. DetCo:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S81",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Unsupervised Contrastive Learning for Object Detection. In Proceedings of the IEEE International Conference on Computer Vision, Montreal, QC, Canada. 8372\u20138381. [42] Li Xu, Shicheng Zheng, and Jiaya Jia. 2013. Unnatural L0 Sparse Representation for Natural Image Deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA. 1107\u20131114. [43] Yanyang Yan, Wenqi Ren, Yuanfang Guo, Rui Wang, and Xiaochun Cao. 2017. Im- age Deblurring via Extreme Channels Prior. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA. 6978\u20136986. [44] Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. 2017. DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy. 2868\u20132876.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S82",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "[45] Syed Waqas Zamir, Aditya Arora, Salman H. Khan, Munawar Hayat, Fahad Shah- baz Khan, and Ming-Hsuan Yang. 2021. Restormer: Efficient Transformer for High-Resolution Image Restoration. CoRR abs/2111.09881 (2021). [46] Syed Waqas Zamir, Aditya Arora, Salman H. Khan, Munawar Hayat, Fahad Shah- baz Khan, Ming-Hsuan Yang, and Ling Shao. 2021. Multi-Stage Progressive Image Restoration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, virtual. 14821\u201314831. [47] Honglun Zhang, Wenqing Chen, Hao He, and Yaohui Jin. 2019. Disentangled Makeup Transfer with Generative Adversarial Network. CoRR abs/1907.01144 (2019). [48] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr Koniusz. 2019. Deep Stacked Hierarchical Multi-Patch Network for Image Deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S83",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "Long Beach, CA, USA. 5978\u20135986. [49] Jiahui Zhang, Shijian Lu, Fangneng Zhan, and Yingchen Yu. 2021. Blind Image Super-Resolution via Contrastive Representation Learning. CoRR abs/2107.00708 (2021). [50] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bj\u00f6rn Stenger, Wei Liu, and Hongdong Li. 2020. Deblurring by Realistic Blurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA. 2734\u2013 2743. [51] Suiyi Zhao, Zhao Zhang, Richang Hong, Mingliang Xu, Haijun Zhang, Meng Wang, and Shuicheng Yan. 2021. Unsupervised Color Retention Network and New Quantization Metric for Blind Motion Deblurring. (2021). [52] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the IEEE International Conference",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2204_07820v2:S84",
      "paper_id": "arxiv:2204.07820v2",
      "section": "conclusion",
      "text": "on Computer Vision, Venice, Italy. 2242\u20132251.",
      "page_hint": null,
      "token_count": 6,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.954152798487604,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 10,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 4838,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5071,
        "empty": false
      },
      {
        "page": 3,
        "chars": 6773,
        "empty": false
      },
      {
        "page": 4,
        "chars": 6364,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5064,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4906,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2943,
        "empty": false
      },
      {
        "page": 8,
        "chars": 4516,
        "empty": false
      },
      {
        "page": 9,
        "chars": 8941,
        "empty": false
      },
      {
        "page": 10,
        "chars": 4990,
        "empty": false
      }
    ],
    "quality_score": 0.9542,
    "quality_band": "good"
  }
}