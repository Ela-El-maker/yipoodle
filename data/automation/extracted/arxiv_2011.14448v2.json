{
  "paper": {
    "paper_id": "arxiv:2011.14448v2",
    "title": "Improved Handling of Motion Blur in Online Object Detection",
    "authors": [
      "Mohamed Sayed",
      "Gabriel Brostow"
    ],
    "year": 2020,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "We wish to detect specific categories of objects, for online vision systems that will run in the real world. Object detection is already very challenging. It is even harder when the images are blurred, from the camera being in a car or a hand-held phone. Most existing efforts either focused on sharp images, with easy to label ground truth, or they have treated motion blur as one of many generic corruptions.   Instead, we focus especially on the details of egomotion induced blur. We explore five classes of remedies, where each targets different potential causes for the performance gap between sharp and blurred images. For example, first deblurring an image changes its human interpretability, but at present, only partly improves object detection. The other four classes of remedies address multi-scale texture, out-of-distribution testing, label generation, and conditioning by blur-type. Surprisingly, we discover that custom label generation aimed at resolving spatial ambiguity, ahead of all others, markedly improves object detection. Also, in contrast to findings from classification, we see a noteworthy boost by conditioning our model on bespoke categories of motion blur.   We validate and cross-breed the different remedies experimentally on blurred COCO images and real-world blur datasets, producing an easy and practical favorite model with superior detection rates.",
    "pdf_path": "data/automation/papers/arxiv_2011.14448v2.pdf",
    "url": "https://arxiv.org/pdf/2011.14448v2",
    "doi": null,
    "arxiv_id": "2011.14448v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.784843+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2011_14448v2:S1",
      "paper_id": "arxiv:2011.14448v2",
      "section": "body",
      "text": "Improved Handling of Motion Blur in Online Object Detection Mohamed Sayed Gabriel Brostow University College London visual.cs.ucl.ac.uk/pubs/handlingMotionBlur/",
      "page_hint": null,
      "token_count": 17,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S2",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "We wish to detect speci\ufb01c categories of objects, for on- line vision systems that will run in the real world. Object de- tection is already very challenging. It is even harder when the images are blurred, from the camera being in a car or a hand-held phone. Most existing efforts either focused on sharp images, with easy to label ground truth, or they have treated motion blur as one of many generic corruptions. Instead, we focus especially on the details of egomotion induced blur. We explore \ufb01ve classes of remedies, where each targets different potential causes for the performance gap between sharp and blurred images. For example, \ufb01rst deblurring an image changes its human interpretability, but at present, only partly",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S3",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "improves object detection. The other four classes of remedies address multi-scale texture, out-of- distribution testing, label generation, and conditioning by blur-type. Surprisingly, we discover that custom label gen- eration aimed at resolving spatial ambiguity, ahead of all others, markedly improves object detection. Also, in con- trast to \ufb01ndings from classi\ufb01cation, we see a noteworthy boost by conditioning our model on bespoke categories of motion blur. We validate and cross-breed the different remedies ex- perimentally on blurred COCO images and real-world blur datasets, producing an easy and practical favorite model with superior detection rates. 1. Introduction A little motion blur is present in most hand-held pho- tography. Blur is ever harder to ignore because images are increasingly captured on the move,e.g.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S4",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "by a gimbaled robot or from an autonomous vehicle. Precisely these on-the-go situations prompt us to explore: how much does motion blur severity impact object detection? What can be done about it? Detection is important because it underpins many other tasks, such as tracking and re-identi\ufb01cation, and our initial scope is further narrowed to egomotion induced blur. Unsurprisingly, the severity of the blur correlates with detection failure [2]. Fig. 1 shows an example. An ideal al- gorithm will make that degradation more gradual, and could Figure 1. a) Original sharp MS COCO [25] image with object de- tections. b) Same image with signi\ufb01cant linear motion-blur, with COCO ground-truth. c) Failed predictions from original Faster- RCNN. d) Predictions from network with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S5",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "our proposed model. someday enable a model that surpasses even a human\u2019s abil- ity to see through blur. Instead of a single breakthrough, it is more likely that a combination of approaches is needed. Much like the \u201cdevil in the details\u201d papers [6, 7], the task speci\ufb01cs and pipeline likely make a difference. Our main contribution is an empirical exploration of \ufb01ve classes of remedies. These remedies are selected to cope with \ufb01ve proposed causes for reduced detection accuracy. The \ufb01ve cause/remedy pairs explored here are: 1) Is the en- tire image too blurry to be useful? Deblur test image \ufb01rst. 2) Is texture mismatch along blur axes confusing the model? Spatially transform image to compensate. 3) Does test-time blur",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S6",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "differ from training data? Train model for out-of- distribution robustness, and/or perform test-time tuning of network. 4) Are the training labels incorrect? Customize labels to match detection-in-blur task and reconsider labels used for testing. 5) Are egomotion blur types too diverse? Treat detection in blur as a multi-task problem. Overall, we propose a new model that focuses on the remedies from (4) and (5), and set a new standard for online object detection in egomotion-induced blur. 2. Related Work Deblurring: A close topic with valuable data and poten- tial insights is image deblurring. The \ufb01rst canonical method arXiv:2011.14448v2 [cs.CV] 30 Mar 2021 Figure 2. We explore the baseline with \ufb01ve categories of remedies across both train and test time. 1)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S7",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Deblurring: Deblur the input using Nah et al. [30] before passing it to the detector at test time. 2) Squint: In\ufb02uenced by [19, 36], we undersample the incoming image on each axis, according to known blur kernel shape during training and testing (assuming oracle knows test kernel). We forward pass through the backbone and carry out the reverse sampling operation on outgoing activations, before passing them to the detection head. 3) Out-of-distribution: Treat motion blur as an out-of-distribution robustness problem and use two leading methods as remedies: 3a) AugMix: Augment training images as in AugMix [16] with two main \ufb02avors: all augmentations from the AugMix paper, or only non-spatial augmentations. We then use the model normally at test time. 3b)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S8",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Using minibatch statistics: As in [42], modify the batch normalization statistics at every layer in the network to be a weighted average of the incoming minibatch (size one) statistics and the original training-averaged batch normalization statistics. 4) Label Modi\ufb01cation Under blur, the spatial extent and center of the bounding box are ambiguous. We experiment with training with labels expanded to include the superset of object locations given by the extent of the blur kernel\u2019s reach, under blur augmentation and when using spatial augmentation in AugMix. We also report results for both expanded COCO minival labels and without. 5) Blur Augmentation Augment COCO images using either 5a) a random selection of kernels across all possible motion-blurs we can generate, or 5b)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S9",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "training networks specialized on speci\ufb01c blur-kernel varieties. At test time, we use either a network trained under general blur augmentation, or a system that incorporates a bag of models and a blur estimation module that selects the appropriate specialized network for the task. for image deconvolution comes from Richardson [38] and Lucy [27] where a known point spread function (PSF) - the blur kernel - is used to iteratively minimize an energy func- tion to \ufb01nd a maximum likelihood estimate of the original image. Deblurring can be non-blind where the blur kernel is known [41], or it can be blind, where the kernel is either \ufb01rst estimated [11, 43, 22] - usually optimized with the \ufb01nal re- sult [8] -",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S10",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "or the entire deblurring method is non-interpretable and runs end to end [30, 48]. Deblurring can also assume either a uniform blur kernel throughout an image [11, 43] or variable nonuniform blur either due to camera egomo- tion (rotation, zoom) [51], depth-of-\ufb01eld effects [47], or dy- namic object motion blur [18, 30, 22]. Previous work has made use of an L0 sparse represen- tation [52], dark image regions [31], and multiple frames in a video [44]. More invasive methods exploit hard- ware, including using a coded shutter [35], inertial measure- ments [21], \ufb02ash frame information integration [56], bursts of blurry images [1], high and low frame rate cameras [46], or an event driven camera tied to an RGB sensor [32].",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S11",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Some deep learning deblurring methods are interpretable [43, 5, 53, 45], but most are end-to-end [48, 23, 54, 30] with the recent state-of-the-art by Nah et al. trained on the high frame rate GOPRO dataset [30]. We explore using a state- of-the-art deblurring method as a preprocessing step, and measure the overall effectiveness of such a baseline. Boracchi et al. [3] generate a statistical model for motion blur kernel generation for benchmarking image restoration model performance. The blur kernels they generate are pa- rameterized to simulate camera shake and exposure, making their kernel generation method a good candidate for synthe- sized blur augmented training. Although deblurring\u2019s aesthetics driven approach means that there are competitive methods for extracting high fre- quency",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S12",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "information in blur, use in an online vision appli- cation might be impractical, especially given that networks are very sensitive to changes in training distribution. Blur and Scene Understanding Tasks:Directly related to this work, Vasiljevic et al. [50] explore the effect of blur on ImageNet [10] classi\ufb01cation performance and blur aug- mentation strategies using a set of synthetically generated blur kernels; however they use a limited set of 100 17\u00d717 pre-generated \ufb01xed length motion blur kernels and a restric- tive image resolution of384\u00d7384 during training and eval- uation. They experiment with different blur types and \ufb01ne grained blur augmentation for classi\ufb01cation, but only a seg- mentation dividing blur types - and not across blur exposure with different kernel types -",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S13",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "is considered; only defocus blur is explored for image segmentation. For image segmenta- tion, they evaluate their networks using a soft boundary for accuracy, but do not explore the effect of spatial ambiguity on \ufb01ne-tuning networks for blur during training, especially since only defocus blur (no shift in barycenter naturally) is used for \ufb01ne-tuning the segmentation task. Vasiljevic et al. note that knowing blur information apriori could be helpful, but don\u2019t explore such a blur estimator. Overall, we \ufb01nd that building explicit robustness into vi- sion models for dealing with realistic camera motion blur needs more exploration, especially for spatial tasks. Out-of-Distribution Robustness: Recent work [15, 14] treats image corruptions (brightness, contrast, snow, noise, blur) as out-of-distribution samples compared to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S14",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "the in-distribution clean images a network was trained on. ImageNet-C [15] is a variant of the ImageNet classi\ufb01ca- tion dataset that contains images corrupted by 15 differ- ent types of canonical image corruptions, and is used as a benchmark for out-of-distribution model performance. Cru- cially, ImageNet-C - and others such as ImageNet-R [14] and ImageNet-A [17] - are not meant to be trained on. In- stead, the argument is that a model\u2019s ability to generalize to images outside of the training set\u2019s distribution can be measured by evaluating its performance on these datasets. Although ImageNet-C contains motion blur corruptions, the method only considers straight line motion blur ker- nels. Michaelis et al. [28] use the same corruptions from ImageNet-C to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S15",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "produce a robustness benchmark for detec- tion, by augmenting MS COCO [25]. COCO-C also in- cludes straight line blurred images, but changes in labels under the spatial ambiguity brought upon by blur are not addressed. We call this type of naive blur \u2019Non-Centered,\u2019 and we show why it\u2019s important for spatial reasoning. In the data augmentation space, AutoAugment [9] \ufb01nds an optimal augmentation policy for a model and dataset pair achieving state-of-the-art accuracy for classi\ufb01cation datasets, but requires 15,000 compute hours on an NVIDIA Tesla P100 for training ImageNet. Rusaket al. [40] propose an adverserial noise training scheme for increasing clas- si\ufb01cation model accuracy and robustness on ImageNet-C, mainly combating pixel noise and not blur. AugMix [16] is an augmentation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S16",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "strategy for improving classi\ufb01cation model robustness to out-of-distribution images. It involves alpha blending copies of a training image that have been corrupted by a random chain of image augmentations. They use the same corruptions in [9], including both pixel level value changes and spatial augmentations. Although the AugMix paper also doesn\u2019t discuss how spatial augmenta- tion should affect spatial labels, we explore the effective- ness of AugMix for blur robustness after making decisions on how spatial labels should be changed. Schneider et al. [42] analyze the effect of normalizing activations in batch normalization layers using a weighted average of the statistics of both the source training set and the minibatch. Their method achieves state-of-the-art on ImageNet-C and improves ImageNet-C robustness",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S17",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "on vanilla Resnet-50 classi\ufb01cation models, even with a mini- batch of size one. While these methods are a promising way of increasing model robustness to unseen corruptions, the aim of the pro- posed work is to explore the speci\ufb01c impact of motion blur on detection, and so we focus our effort on manufacturing the most realistic blur kernels available in the literature. 3. Designing Detection Models for Motion Blur To improve online object detection, we propose a uni- \ufb01ed framework that allows us to measure the impact of different remedies and their combinations. The frame- work is based on a state-of-the-art object detector, Faster- RCNN [37], with training and testing on data derived from the MS COCO [25] detection dataset.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S18",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "The baseline and data are explained in this section. The proposed remedies are ex- plained in detail in Sec. 4, and are evaluated in Sec. 5. Fig- ure 2 illustrates both the baseline model, and the different enhanced alternatives. 3.1. Detection Baseline For reproducibility, we use the pretrained Faster R- CNN variant trained on COCO, available through Pytorch\u2019s torchvision [33] library, as a baseline for all our experi- ments. We use a ResNet-50 [13] backbone with a Feature Pyramid Network (FPN) [24]. This baseline achieves 58.5 mAP@0.5 and 37.0 mAP@0.5:0.95 on the COCO test set. While other models achieve better accuracy on the COCO minival set, we choose this framework for its accessibility and as a good baseline representation of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S19",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "a canonical detec- tion framework with top-10 performance for the backbone\u2019s size [20, 26]. 3.2. Selecting Data for Training and Testing Ideally, we\u2019d select data with detection labels for im- ages exhibiting motion blur. Due to the way MS COCO is gathered [25], there are very few blurry images in the dataset. This leaves us with the task of generating synthet- ically blurred COCO images for both training and evalu- ation. Related but not directly applicable here, there are multiple real-world image datasets for deblurring. These were generated using high frame rate video [30] or shutter tied cameras [39]. They either don\u2019t contain enough images for training and evaluating detection models ([39] only con- tains 5500 images) and/or lack object",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S20",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "annotations. Zhanget al. [55] generate blurry images as part of a GAN architec- ture for deblurring. Although they train the blur generation module using a discriminator trained on real world blurry images, it is not trivial to modify labels, given spatial am- biguity, since camera motion is not made explicit. Brooks & Barron [4] use multiple adjacent images (as few as two) to generate realistic motion blur. But to use that would re- quire a video or stereo dataset with ground truth labels for the detection task. This leaves methods that synthesize blurry images via convolution with synthetic motion blur kernels [41, 3, 50, 28, 15]. ImageNet-C [15] and COCO-C [28] contain im- ages blurred using straight line motion blur",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S21",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "exclusively, with no control over simulated camera shake. Vasiljevic et al. [50] use a limited set of motion-blur kernels since they are constrained by a \ufb01xed length spline formation model. Boracchi & Foi [3] describe a method that allows con- trol over different characteristics of a camera\u2019s trajectory through space, including the amount of shake and jerk with variable exposure. 3.3. Blur Generation and Space Discretization We adapt the blur kernel generation method from Borac- chi & Foi [3]. We \ufb01x their high level controllable parameter P to one of three values, P1\u22123, representing three distinct Figure 3. Example blur kernels based on Boracchi & Foi\u2019s [3] motion-blur model. All kernels occupy a space of 128 \u00d7 128. In our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S22",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "modi\ufb01cation, used everywhere unless stated otherwise, kernels are centered by shifting the barycenter of all nonzero points to the origin of the \ufb01lter. P1 type kernels tend to be very erratic, while P3 kernels contain mostly rectilinear trajectories representative of when camera ego motion is linear. types of camera motion. We also modulate exposure via early camera trajectory clipping. First, we generate a trajectory by \ufb01nding a random path in 2D space. We assign an initial velocity vector v0, drawn at random from a unit circle, and a position in space x0 for the camera. At every step, the camera\u2019s velocity vector is updated by the acceleration vector, \u2206v = P(\u2206vg \u2212Ixt), (1) where \u2206vg is random acceleration with elements",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S23",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "drawn from N(0, \u03c32). Ixt is an inertial tendency for the camera to stay where it is, and P \u2208P1\u22123 is the high level anxi- ety parameter we \ufb01xed above. P3 has the highest random velocity change on every step. Further, to model a camera jerk, with a randomly sampled indicator function, the accel- eration update also includes a component equal to twice the current velocity vector in a random direction, so \u2206v = \u2206v + 2P|v|\u2206vj, (2) where \u2206vj is sampled from the unit circle. Again, with a high P, there is a higher chance of a jerk happening and a shakier camera. When starting a trajectory, I, \u03c32, and jare drawn once from uniform random distributions to increase",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S24",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "variability under the same class of blur P \u2208P1\u22123. Note that this leads to some overlap between kernels generated across different Ps. In summary, the type of camera behavior falls into one of three classes: 1) P1 simulates a very nervous camera, 2) P2 for back and forth behavior, and 3) P3 simulates mostly straight rectilinear motion-blur. To simulate exposure, we stop the motion path early using the exposure factor (trajec- tory length) E. We discretize exposure to one of 5 values, E1\u22125. Examples of these kernels can be seen in Fig. 3. Sub- pixel interpolation produces kernels for convolving sharp images. 3.4. Implementation Details Kernel Generation: To speed up training, a corpus of 12,000 blur kernels is generated for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S25",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "every pair of {P1\u22123}\u00d7{E1\u22125}, for a total of 180,000 possible motion blur kernels. However, random kernels are generated on the \ufb02y during evaluation for each combination of blur type and exposure, with \ufb01xed seeds for reproducibility. Trajectory length is 96 and blur kernels \ufb01t in 128 \u00d7128 \ufb01lters. Blurring: Unlike in [15, 50, 28], we don\u2019t resize im- ages to a \ufb01xed size before blurring. Instead each image is convolved separately with re\ufb02ection padding, to account for what would otherwise be real world data. We opt not to resize our blur kernels to match image size as a way of simulating changes in focal length. We implement sparse convolution on the GPU for applying blur kernels. As per Sec. 4.4,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S26",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "we make sure to center our motion-blur kernels by translating their barycenters to the center of the \ufb01lters. Training: All networks start from a base Resnet-50FPN pretrained on COCO. We use an FPN framework that out- puts activations at four scales from the backbone. There was no apparent difference in blur augmented performance when training all \ufb01ve blocks vs. \ufb01xing the weights of the \ufb01rst two. 4. Proposed Remedies for Improved Detection Suspecting speci\ufb01c underlying causes for the adverse ef- fects of blur on detection, we now propose bespoke reme- dies. Where appropriate, some of the remedies are also crossbred, and experimental results appear in Sec. 5. 4.1. Deblurring as a Pre-process Image deblurring is useful for aesthetic purposes, but",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S27",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "could also aid other vision tasks. To test this remedy, we use the recent deblurring model from the GoPro dataset pa- per, [30], before passing the result to the detector. Deblur- ring is a slow process, by 12\u00d7in this case, so heavy opti- mizations would be needed for an online robot. 4.2. Reconciling Texture Information With Scale When motion-blur is biased to one major direction over another, it removes more high frequency information (and texture) in that direction. It is reasonable to expect a net- work is not natively designed for this imbalance. CNNs usually understand texture and shape information across multiple scales under the same aspect ratio, but we\u2019re also asking the network to deal with a texture imbalance",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S28",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "along the blur kernel\u2019s major axis. In\ufb02uenced by the work on Spatial Transformers [19] (which proved a slightly inferior baseline) and neural sampling layers [36], we instead un- dersample the incoming image along the principal compo- nents of the blur kernel. The reverse operation is carried out, using reciprocal scaling factors, on every activation output from the backbone. This \u201cSquint\u201d process is done at both train and test time. For best-case testing, an oracle is as- sumed to know the blur kernel. 4.3. Training vs. Test Distribution We consider treating complex motion-blur as an out-of- distribution corruption as in [15, 28, 16, 42], and use two promising methods from the OOD literature. We use Aug- Mix [16] as a training",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S29",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "time remedy. We propose three \ufb02a- vors, the \ufb01rst is a purely pixel level version where we aug- ment pixel intensities only. The second applies all spatial augmentations as well as suggested in [16], but does not rec- oncile the shifts in bounding box changes. The third is an \u201cExpanded\u201d version following Sec. 4.4, where we change COCO labels at train time to match the superset of where an object is shifted to across branches. AugMix roughly ap- proximates blur when augmentations are selected that trans- late an image before concatenating with other branches. Further, at test time, we use covariate shift adaptation from the upcoming [42]. The \ufb01rst step is to get a weighted average of the incoming activation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S30",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "statistics of the mini- batch (n = 1for online inference) and the source statistics of the model where N = 16, \u00b5= n N + n\u00b5t + N N + n\u00b5s,and (3) \u03c32 = n N + n\u03c32 t + N N + n\u03c32 s. (4) We then use these new normalization statistics for batch normalization in all network layers. 4.4. Customizing Labels When an image is motion blurred, objects are no longer con\ufb01ned to the bounding boxes they had occupied in the sharp image. The objective may no longer be to estimate that original bounding box. See Fig. 4. We discuss two remedies for this problem, that apply when training under augmentation and for evaluation. Kernel Centering The start",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S31",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "point of a motion blur path corresponds to the exposure at t = 0. Any path that leads away from the center, as in Fig. 4(b), will offset the blurred version of the object in some direction, so the \u201cground truth\u201d bounding box is no longer centered on the blurred object. This introduces a mismatch between the blurred in- put and its label. This mismatch is created in [15, 28], introducing label ambiguity and noise in training. We center a kernel using a weighted average of the ker- nel\u2019s nonzero points. The aim is to have the detection frame- work learn to localize objects based on where they are, on average, during the exposure. This remedy is similar to how",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S32",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "[39] aligned images from paired long/short exposure cam- eras to train for deblurring. In our case, the training loss is noisier when training on non-centered kernels, and the drop Figure 4. a) An image from the MS COCO [25] trainset with an associated bounding box label. b) The same image but blurred and with the same non-translated bounding box, now introducing a training/evaluation mismatch. The object may have been there at the start of the exposure (or end if the kernel smeared the other way), but this is certainly not true for the rest of the image. c) The \ufb01rst remedy, centering the kernel by shifting the barycenter of the nonzero points to the center of the \ufb01lter. d) The",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S33",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "object is smeared outside of the original COCO bounding box, so the label is expanded using the \ufb01lter\u2019s max/min points to capture the superset of object locations. in accuracy can be up to 8-10mAP@50 (see \u201dNon-Centered Augmented\u201d and \u201dStandard Augmentated\u201d in Fig. 5) points with the most severe blur. All networks shown here will be trained and evaluated with centered kernels, except when explicitly mentioned. We include ablation experiments with no-centering in the supplemental. Expanding Target Boxes Compared to the original bounding box, the expanded label can cover the superset of pixels where an object projected during an exposure; see Fig. 4(d). A worst-case scenario could occur without this correction during training: for a small object, the sharp im- age\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S34",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "label could seemingly miss the blurred object entirely due to IOU cutoffs. As a remedy, for every generated cen- tered kernel, we \ufb01nd the maximum offsets for non zero ker- nel elements in both 2D axes,x\u2212,x+,y\u2212,y+, and use them to expand the boundaries of COCO bounding box labels. The new bounding box labels (top left and width/height) are now \u02c6bx = bx \u2212|x\u2212| \u02c6by = by \u2212|y\u2212| \u02c6bw = bw + |x\u2212|+ |x+| \u02c6bh = bh + |y\u2212|+ |y+|. (5) We train variants of our networks with these expanded boxes alongside kernel centering. During test time, we eval- uate these networks using expanded bounding boxes. 4.5. Specializing for Categories of Blur The \ufb01nal category of remedies explores if egomotion in-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S35",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "duced blur is perhaps multiple problems masquerading as one. We explore training blur specialized networks on spe- ci\ufb01c partitioned segments of the blur space, as if categories of blur are mutltiple distinct tasks. The \ufb01ndings on recogni- tion in [50] show that specialized networks can sometimes achieve higher task accuracy on their respective blur types than general blur augmented networks. Two Specialized Meta-Models We make two sets of specialized networks, that differ in how the motion kernels are clustered into categories. First, motion blur is grouped based on the type of kernel P, alone, leading to a bespoke network for each of P1\u22123 with a fourth generalist network trained for all types and exposures. The second grouping creates three networks",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S36",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "specialized at each P but exclusively on long exposure blur. One fur- ther network handles all low exposure blur. As per [12], networks are biased toward texture. Instead of using this knowledge to create more corruption robust networks, it is exploited here to make more shape biased networks for sub- stantial motion blur. Blur Estimation and Network Selection A ResNet-18 blur estimator module is added, and runs 10\u00d7faster than the detection framework. The estimator categorizes the blur present in the image at test time. One network is trained on 16 classes (sharp and the combinations of all exposures and blur types) and the other network focuses on the separation between speci\ufb01c blur types at high exposures and general blur at",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S37",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "low exposure (four classes). Details of how these estimators are trained are in the supplemental materiel. 5. Comparisons and Evaluation We report COCO minival results for all proposed reme- dies, at both test time and train time. Detection accuracy at mAP@50 is reported for all below-listed models and vari- ants in Fig. 5 and Fig. 6, where the former uses COCO orig- inal labels, and the latter uses expanded labels. We also re- port accuracy results on two pseudo-real blur datasets, GO- PRO [30] and REDS [29], and a real-world blur dataset, RealBlur [39], obtained using shutter tied cameras. These datasets don\u2019t have box annotations, so we utilize a state- of-the-art high accuracy detector, DetectoRS [34], to obtain pseudo-groundtruth bounding-boxes",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S38",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "for evaluation. For evaluating expanded bounding boxes, we generate our own GOPRO testset using grountruth sharp frames and use \ufb02ow Figure 5. Non-expanded Standard Labeling COCO minival mAP@50 accuracy values across the three blur types and then exposures. Almost all methods and hybrids improve beyond the Original network. There is no bene\ufb01t in augmenting for blur, and then using either minibatch statistics or a deblurer network. Figure 6. Expanded Labeling COCO minival mAP@50 accuracy values across the three blur types and then exposures. The Spec by Exposure network (\u201cOurs\u201d) excels at both ends of the exposure extremes, likely due to the biased training and specialized networks it enjoys. All expanded-box trained networks (except AugMix trained on expanded labels via spatial",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S39",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "augmentation) perform better than their standard counterparts. Networks augmented with both blur and non-spatial AugMix perform well at low exposures and dataset generalization; in Table 1 we show performance for Spec By Exposure that makes use of this for improved performance on other datasets. computed using [49] for bounding-box expansion. Names in the \ufb01gures are explained below, and map to the \ufb01ve remedy categories. Qualitative video here: visual. cs.ucl.ac.uk/pubs/handlingMotionBlur. \u2022 Standard Augmented and Expanded Labels were trained on non-expanded but centered COCO labels, and expanded and centered COCO labels respectively. Both were trained on a 10/90 mixture of sharp to blurry images across all blur types. \u2022 Deblur then original and Deblur then Standard Augmented are both modes of operation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S40",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "where the im- age is \ufb01rst deblurred using [30] then run through either the original network or a Standard Augmented net- work respectively. \u2022 Squint and Squint Expanded Labels come from Sec. 4.2, and have been trained either using standard labels under blur or expanded labels, respectively. \u2022 AugMix [16] Hendrycks et al . [16] As described in Sec. 4.3, we evaluate a non spatial version, AugMix PixelLevel, a spatial version without label expansion, AugMix [16], and a version trained with expanded labels as per augmentations and evaluated with blur based label expansions AugMix Expanded Labels. \u2022 Standard Augmented w/MiniBatch and Expanded Labels w/MiniBatch follow Schneider et al. [42] and use modi\ufb01ed minibatch normalization with N = 16 and n=",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S41",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "1as in Sec. 4.3 with networks that have been augmented for blur using either standard labels or ex- panded labels respectively. \u2022 Standard Augmented w/ NonSpatial AugMix and Expanded Labels w/ NonSpatial AugMix have been trained by \ufb01rst transforming the image using non spa- tial AugMix then blurring the image and training with expanded labels. NonSpatial AugMix augmentation helps when generalizing to other datasets at low ex- posure blur. GOPRO [30] RealBlur [39] REDS [29] GOPRO Expanded* Model Sharp Blurry Sharp Blurry Sharp Blurry Sharp Blurry Spec by Exposure w/ \u2020 34.89 28.15 43.23 36.37 39.86 32.55 38.06 30.99 Low-Exp Net w/ NonSpatial AugMix \u2020 34.89 27.87 43.23 36.28 39.86 32.53 38.06 30.92 Spec by Exposure 34.80 27.05 42.91 35.26",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S42",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "40.06 31.51 36.79 31.14 Spec by Type 32.42 27.50 40.91 36.11 36.96 30.33 35.26 31.01 Low-Exposure Net 34.80 26.91 42.91 35.20 40.06 31.47 36.79 31.60 Standard Aug w/ NonSpatial AugMix\u2021 32.16 27.05 42.77 36.82 35.61 28.72 35.18 29.94 Standard Augmented 32.42 26.54 40.93 35.63 36.96 29.24 35.26 30.75 Non-Centered Augmented 33.08 25.66 40.47 34.47 36.82 28.80 36.74 28.47 Standard Aug w/ Minibatch 30.06 23.78 35.01 31.32 28.99 24.94 33.32 26.79 Original w/ NonSpatial AugMix 33.68 19.10 43.90 29.63 43.38 23.52 36.63 22.16 Original w/ Minibatch 31.78 16.73 38.54 27.17 36.03 22.18 33.45 20.50 Deblur then Standard Augmented 12.53 5.14 32.53 28.90 34.53 28.42 34.54 29.72 Deblur then Original 10.58 2.24 31.45 28.76 40.56 31.84 35.74 26.70 Original 35.85 19.64 42.39",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S43",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "29.00 42.02 24.38 36.33 22.30 Table 1. mAP@0.5 for models trained on COCO images with synthesized blur and evaluated on real world blur datasets using predictions on sharp images using DetectoRS [34] as groundtruth. While NonSpatial AugMix doesn\u2019t improve performance for blur augmentation on the COCO minival, it does increase the performance of the COCO trained Low-Exposure\u2020 and Standard Augmented\u2021 networks on real world datasets. Spec by Exposure w/\u2020 utilizes the NonSpatial AugMix augmentation version of the Low-Exposure network. *Expanded augmented trained versions of the networks are used on this expanded labels test-set. \u2022 Spec by Type is the \ufb01rst bag of specialists from Sec. 4.5 where the Standard Augment network is used when no blur is detected. Spec by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S44",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Exposure is the second bag of specialists where each of three networks specializes in one P, but only at long ex- posures, and one network handles all short exposures and sharp images, Low-Exposure Augment. Again a blur estimator trained for these classes selects the right network. Spec by Type Expanded Labels and Spec by Exposure Expanded Labelsare obviously variants trained on expanded labels. These networks and asso- ciated mode of operation outperform the rest due to their exploiting of the texture vs. bias trade-off and the use of an accurate blur estimator. Notably, the network responsible for sharp and low exposure blur recovers the accuracy lost on sharp images usually associated with blur augmentation networks. 6. Discussion We achieve state-of-the-art",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S45",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "object detection results for egomotion-blurred images. We have succeeded in identify- ing that two factors adversely affect detection in such im- ages. The \ufb01rst is that labels for sharp images should be cus- tomized for the motion-blur domain. In our remedies, that means translating and expanding the bounding box labels to match the blurred versions of relevant objects. The second is that categories of motion blur are distinct enough for the model to be trained for each blur-category separately. In- terestingly, the second factor is the opposite of what [50] found with recognition tasks, where mixing blur-types dur- ing training was effective. Through our \u201cdifferential diagnosis\u201d approach, the other three factors explored here seem unpromising for explain- ing the destructiveness",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S46",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "of blur on CNN-based detection. These negative results are not conclusive, as the remedies may simply be immature. For example, better deblurring may eventually restore missing texture at all scales. In the future, to reduce the memory footprint of our favored solution, the blur-selector and distinct exposure- speci\ufb01c models could be combined into one multi-task model. They are already end-to-end differentiable, but then they could share layers. Further progress in this direction could bene\ufb01t from a distilled dataset that allows for detec- tion labels and blur from real data, perhaps through the use of event driven cameras or multi-camera datasets. One clear limitation is that even sharp images have < 60 mAP@0.5 detection accuracy with a realtime- capable backbone, before",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S47",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "blur hurts the situation further. Depth or disparity data would help address scenes with dy- namic blur, since the blur kernels are depth-dependent. 360- cameras, augmented as proposed here for COCO, could be bene\ufb01cial for dealing with sharp or blurred target objects that are partially outside a typical camera\u2019s \ufb01eld of view. The impact of our approach could be especially helpful in particular applications, such as drone-based following, where even brief interruptions in tracking can ruin a \ufb01lm. Acknowledgements The authors would like to thank Matthew Johnson and Peter Hedman for their helpful and informative suggestions and discussions. Mohamed is funded by a Microsoft Research PhD Scholarship. References [1] Miika Aittala and Fr \u00b4edo Durand. Burst image deblurring us- ing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S48",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "permutation invariant convolutional neural networks. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 731\u2013747, 2018. 3 [2] Gedas Bertasius, Lorenzo Torresani, and Jianbo Shi. Object detection in video with spatiotemporal sampling networks. In Proceedings of the European Conference on Computer Vi- sion (ECCV), September 2018. 1 [3] Giacomo Boracchi and Alessandro Foi. Modeling the perfor- mance of image restoration from motion blur. IEEE Trans- actions on Image Processing, 21(8):3502\u20133517, 2012. 3, 4 [4] Tim Brooks and Jonathan T Barron. Learning to synthe- size motion blur. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6840\u2013 6848, 2019. 4 [5] Ayan Chakrabarti. A neural approach to blind motion de- blurring. In European",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S49",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "conference on computer vision, pages 221\u2013235. Springer, 2016. 3 [6] K. Chat\ufb01eld, V . Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In British Machine Vision Conference , 2011. 1 [7] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convo- lutional nets. In British Machine Vision Conference, 2014. 1 [8] Liang Chen, Faming Fang, Tingting Wang, and Guixu Zhang. Blind image deblurring with local maximum gradient prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1742\u20131750, 2019. 2 [9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S50",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 113\u2013123, 2019. 3 [10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 3 [11] Rob Fergus, Barun Singh, Aaron Hertzmann, Sam T Roweis, and William T Freeman. Removing camera shake from a single photograph. In ACM SIGGRAPH 2006 Papers, pages 787\u2013794. 2006. 2 [12] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.arXiv preprint arXiv:1811.12231, 2018. 6 [13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S51",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 4 [14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada- vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robust- ness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020. 3 [15] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. arXiv preprint arXiv:1903.12261, 2019. 3, 4, 5 [16] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. 2, 3, 5, 7",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S52",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "[17] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein- hardt, and Dawn Song. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019. 3 [18] Tae Hyun Kim, Byeongjoo Ahn, and Kyoung Mu Lee. Dy- namic scene deblurring. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 3160\u20133167, 2013. 2 [19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural infor- mation processing systems, pages 2017\u20132025, 2015. 2, 5 [20] Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi Feng, and Rong Qu. A survey of deep learning- based object detection. IEEE Access , 7:128837\u2013128868, 2019. 4 [21] Neel Joshi, Sing Bing Kang, C Lawrence Zitnick, and Richard Szeliski. Image deblurring using",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S53",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "inertial mea- surement sensors. ACM Transactions on Graphics (TOG) , 29(4):1\u20139, 2010. 3 [22] BR Kapuriya, Debasish Pradhan, and Reena Sharma. De- tection and restoration of multi-directional motion blurred objects. Signal, Image and Video Processing , 13(5):1001\u2013 1010, 2019. 2 [23] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji \u02c7r\u00b4\u0131 Matas. Deblurgan: Blind mo- tion deblurring using conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8183\u20138192, 2018. 3 [24] Tsung-Yi Lin, Piotr Doll \u00b4ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyra- mid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2117\u20132125, 2017. 4 [25] Tsung-Yi",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S54",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision , pages 740\u2013755. Springer, 2014. 1, 3, 4, 6 [26] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik \u00a8ainen. Deep learning for generic object detection: A survey. International journal of computer vision, 128(2):261\u2013318, 2020. 4 [27] Leon B Lucy. An iterative technique for the recti\ufb01cation of observed distributions. The astronomical journal , 79:745, 1974. 2 [28] Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver Bringmann, Alexander S Ecker, Matthias Bethge, and Wieland Brendel. Benchmarking ro- bustness in object detection: Autonomous driving when win-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S55",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "ter is coming. arXiv preprint arXiv:1907.07484, 2019. 3, 4, 5 [29] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super- resolution: Dataset and study. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Work- shops, June 2019. 6, 8, 4 [30] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 3883\u20133891, 2017. 2, 3, 4, 5, 6, 7, 8 [31] Jinshan Pan, Deqing Sun, Hanspeter P\ufb01ster, and Ming- Hsuan Yang. Blind image deblurring using dark channel prior. In Proceedings",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S56",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1628\u20131636, 2016. 2 [32] Liyuan Pan, Cedric Scheerlinck, Xin Yu, Richard Hartley, Miaomiao Liu, and Yuchao Dai. Bringing a blurry frame alive at high frame-rate with an event camera. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6820\u20136829, 2019. 3 [33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S57",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024\u20138035. Curran Asso- ciates, Inc., 2019. 4 [34] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switch- able atrous convolution. arXiv preprint arXiv:2006.02334 , 2020. 6, 8, 4 [35] Ramesh Raskar, Amit Agrawal, and Jack Tumblin. Coded exposure photography: motion deblurring using \ufb02uttered shutter. In ACM SIGGRAPH 2006 Papers, pages 795\u2013804. 2006. 3 [36] Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Ma- tusik, and Antonio Torralba. Learning to zoom: a saliency- based sampling layer for neural networks. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 51\u201366, 2018. 2, 5 [37] Shaoqing Ren, Kaiming He,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S58",
      "paper_id": "arxiv:2011.14448v2",
      "section": "abstract",
      "text": "Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information pro- cessing systems, pages 91\u201399, 2015. 3 [38] William Hadley Richardson. Bayesian-based iterative",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S59",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "[39] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho. Real-world blur dataset for learning and benchmarking de- blurring algorithms. In European Conference on Computer Vision, pages 184\u2013201. Springer, 2020. 4, 5, 6, 8 [40] Evgenia Rusak, Lukas Schott, Roland Zimmermann, Ju- lian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. Increasing the robustness of dnns against image corruptions by playing the game of noise. arXiv preprint arXiv:2001.06057, 2020. 3 [41] Uwe Schmidt, Carsten Rother, Sebastian Nowozin, Jeremy Jancsary, and Stefan Roth. Discriminative non-blind deblur- ring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 604\u2013611, 2013. 2, 4 [42] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bring- mann, Wieland Brendel, and Matthias Bethge. Improving robustness",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S60",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "against common corruptions by covariate shift adaptation. Advances in Neural Information Processing Sys- tems, 33, 2020. 2, 3, 5, 7 [43] Christian J Schuler, Michael Hirsch, Stefan Harmeling, and Bernhard Sch\u00a8olkopf. Learning to deblur. IEEE transactions on pattern analysis and machine intelligence , 38(7):1439\u2013 1451, 2015. 2, 3 [44] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video deblurring for hand-held cameras. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition, pages 1279\u20131288, 2017. 2 [45] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn- ing a convolutional neural network for non-uniform motion blur removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S61",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "769\u2013777, 2015. 3 [46] Yu-Wing Tai, Hao Du, Michael S Brown, and Stephen Lin. Image/video deblurring using a hybrid camera. In2008 IEEE Conference on Computer Vision and Pattern Recognition , pages 1\u20138. IEEE, 2008. 3 [47] Chang Tang, Xinzhong Zhu, Xinwang Liu, Lizhe Wang, and Albert Zomaya. Defusionnet: Defocus blur detection via re- currently fusing and re\ufb01ning multi-scale deep features. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition (CVPR), June 2019. 2 [48] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji- aya Jia. Scale-recurrent network for deep image deblurring. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8174\u20138182, 2018. 2, 3 [49] Zachary Teed and Jia Deng. Raft:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S62",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "Recurrent all-pairs \ufb01eld transforms for optical \ufb02ow. In European Conference on Computer Vision, pages 402\u2013419. Springer, 2020. 7, 4, 5 [50] Igor Vasiljevic, Ayan Chakrabarti, and Gregory Shakhnarovich. Examining the impact of blur on recognition by convolutional networks. arXiv preprint arXiv:1611.05760, 2016. 3, 4, 5, 6, 8 [51] Oliver Whyte, Josef Sivic, Andrew Zisserman, and Jean Ponce. Non-uniform deblurring for shaken images. Inter- national journal of computer vision , 98(2):168\u2013186, 2012. 2 [52] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural l0 sparse representation for natural image deblurring. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 1107\u20131114, 2013. 2 [53] Xiangyu Xu, Jinshan Pan, Yu-Jin Zhang, and Ming-Hsuan Yang. Motion blur kernel estimation via",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S63",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "deep learning.IEEE Transactions on Image Processing, 27(1):194\u2013205, 2017. 3 [54] J. Zhang, J. Pan, J. Ren, Y . Song, L. Bao, R. W. H. Lau, and M. Yang. Dynamic scene deblurring using spatially variant recurrent neural networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2521\u2013 2529, 2018. 3 [55] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realis- tic blurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2737\u2013 2746, 2020. 4 [56] Shaojie Zhuo, Dong Guo, and Terence Sim. Robust \ufb02ash deblurring. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 2440\u2013 2447. IEEE,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S64",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "2010. 3 Improved Handling of Motion Blur in Online Object Detection Supplementary Material 1. Blur Discretization and Blur Space Segmentation 1.1. Discretization In the main paper, P and E are held to discrete values. P1\u22123 are [0.005,0.001,0.00005], where a lower value for P, P3 for example, gives a more rectilinear blur. Since there are underlying random factors initialized for every blur kernel that are only in\ufb02uenced by P, some overlap exists between the type of blur kernels produced across different Ps. Exposures E1\u22125 are [1/25,1/10,1/5,1/2,1]. Note that for blur trained networks, we don\u2019t resize images to a canonical size before blurring; this acts as a mild regularizer and helps creates specialists that are \ufb02exible across a range of blur levels.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S65",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "All mAP@50 scores are reported on the COCO minival set (5000 images). We use a \ufb01xed seed for every evaluation when generating blur kernels. While our proposed model was trained with those discrete blur settings, the space of camera-induced blur is not so neatly quantized. To explore a larger cross-section of the continuous blur space, we evaluated a sweep across a random selection of exposures (horizontal axis) and blur types (vertical axis), comparing the original network against our Specialized by Exposure Expanded Labels. Each marker plotted in Fig. 1 is an evaluation on 2000 images from the COCO minival set. It visually summarizes that for sharp and barely-blurred images, our approach is negligibly better than the original model. But for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S66",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "essentially all other settings of induced motion blur, our model does measurably better. Figure 1. Comparison of the original model (ResNet50FPN trained on COCO) and our best model evaluated on expanded labels across a random selection of P and E values. Each marker is a representation of the accuracy (mAP@50) on an evaluation of 2000 images from the COCO minival. For the \ufb01rst two graphs (left to right), the greener the marker the closer it is to an mAP@50 of 61%. The redder it is, the closer it is to an mAP@50 of 0%. For the third graph, we visualize the difference between both networks; the greener the marker the larger the diffrence in mAP@50 between our best solution and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S67",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "the original network. The bluer the marker the less the difference is in accuracy. Naturally, at lower exposures, the original network holds up well, but as the exposure is ramped up, and particularly with more rectilinear blur (low P value), the difference is much larger. 1.2. Segmenting Blur Space: By Type vs. By Exposure All general augmented networks (non-specialized) are trained with a mixture of sharp COCO images (10%) and a random selection of blurry images across P1\u22123 and E1\u22125 (90%). Spec by Type networks are also trained on the same ratio, but are \ufb01xed to a speci\ufb01c P. The low exposure network in Spec by Exposure is trained on 25% sharp images and 75% blurry Figure 2. Standard augmented",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S68",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "specialized networks performance across different blur types and exposures, evaluated on standard labels. Figure 3. Expanded augmented specialized networks performance across different blur types and exposures, evaluated on expanded labels. Note how here and in Fig. 2, the each blur type specialized network tends to be better than its peers, especially at higher exposures. The high exposure HE networks outperform the rest at their respective blur type specialty at high exposures. images from P1\u22123 and E1\u22123; the three others are trained on 100% blurry images exclusively from a a speci\ufb01c P and E4,5. The performance of these networks separately across blur levels is displayed in Fig. 2 and Fig. 3. 1.3. Blur Estimators We use two \ufb02avors of a ResNet18",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S69",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "classi\ufb01cation network, one for each type of bag of specialists. For Spec By Type, the blur estimator is trained to classify an image as belonging to one of 16 classes, clean or one of {P1\u22123 \u00d7E1\u22125}; it achieves 84% accuracy. For Spec by Exp, images are classi\ufb01ed to one of four classes - clean and exposures in {E1\u22123}, {P1 \u00d7E4,5}, {P2 \u00d7E4,5}, or {P3 \u00d7E4,5}; this \ufb02avor achieves 93% accuracy. Blur estimators are trained for 12 epochs with an initial learning rate of 0.02 (20 images) attenuated by a factor of 10 for each epoch in [3, 7, 10]. Unlike blur augmentation for detection, we resize images to a canonical size of 1333 \u00d7800 before blurring. 2. Zero Centering Ablation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S70",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "We show how kernel/label centering improves training and test-time accuracy. The main paper features results of evaluat- ing on centered labels that match the barycenter of the kernel. In Fig. 4 we evaluate on non-aligned kernels and labels as well. Training and evaluating on centered kernels aligned to detection labels produces better scores, likely because the typically non-centered kernels are offset relative to the training bounding boxes. The Non-Centered model achieves the same scores when evaluated with and without centered kernels, indicating that the network has likely learned to \ufb01nd a vague localization and misses boxes altogether that it ought to have detected. Figure 4. Comparison of different training and evaluation strategies. Results are averaged across the blur types P1\u22123.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S71",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "Evaluating and training on kernels aligned to detection labels (Standard Augmentation and centered labels) scores best. 3. Expanded Labels and What the Network Outputs Fig. 5 shows an example image with motion blur, and the output from both theStandard Augmented and the Expanded Augmented networks. The Expanded augmented network learns to predict bounding boxes that capture the superset of all spatial locations an object occupied during an exposure. This seems to be an easier objective for the network to learn. While one could argue that downstream tasks may prefer original-sized bounding boxes as shown computed by the Standard Augmented model, there is no good compromise there: the middle of the blurred object could be a \u201cstale\u201d image-space location compared to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S72",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "where the object is at the end of the exposure, in, e.g. a tracking-by-detection task. In qualitative examples, the expanded networks manage to detect bounding boxes that are otherwise missed by their standard counterparts, see Fig.5. Figure 5. Left: Groundtruth image with COCO labels. Middle: Network output from the Standard Augment network. Right: Network output from the Expanded Augment network. Expanded augment networks learn to output boxes that represent the superset of all locations an object has been at during an exposure. 4. Minibatch Normalization as Schneider et al. [42] In this late-breaking NeurIPS 2020 paper, results are reported for minibatch normalization on networks already trained with augmentation for blurry images. As per their algorithm, we perform minibatch normalization by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S73",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "\ufb01nding the statistics of the activations of an input example, \u00b5and \u03c3, and computing a weighted sum with the training statistics using N = 16and n= 1. This is done progressively in one forward pass. In Fig. 6, results are reported for the performance of the original model with this modi\ufb01cation. We also experimented with \ufb01nding an accurate estimate of the target distribution for blurry images by running a large portion of the train set under a speci\ufb01c type of blur and exposure as many times as there are batchnorm layers in the network with n = 2048, and using that as normalization statistics, but this did not constitute an improvement. Despite the appeal of this test-time approach, object detection",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S74",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "was not substantially better off with it, so we excluded it from our \ufb01nal model. Figure 6. Comparison of using minibatch normalization on both the original network and blur augmented networks. For only this graph: solid lines are evaluation runs on expanded labels and dashed lines are evaluated on standard labels; the exception here is the original model which is evaluated on expanded labels. Results are averaged across P1\u22123. 5. Defocus And Motion Blur Our models are more resilient to camera defocus blur than the original. P1 is close to simulating defocus blur since the camera trajectory loops in place. We Gaussian blur each motion-blur kernel with a random \u03c3 across all blur types and exposures. We report results in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S75",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "Fig. 7. 6. Real-World Blur Datasets We evaluate our models on on two pseudo-real blur datasets, GOPRO [30] and REDS [29], and a real-world blur dataset, RealBlur [39], obtained using shutter tied cameras. These datasets don\u2019t have box annotations, so we utilize a state-of-the- art high accuracy detector, DetectoRS [34], to obtain pseudo-groundtruth bounding-boxes for evaluation. For evaluating expanded bounding boxes, we generate our own GOPRO testset using grountruth sharp frames and use \ufb02ow computed using [49] for bounding-box expansion. We stick to the canonical train and test sets when available. However when either the train and sets combined don\u2019t contain enough images for reliable evaluation or when we need to estimate \ufb02ow on source high-frame rate images, we synthesize",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S76",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "our own set of blurry images using sharp images from the datasets. We perform standard evaluation on RealBlur, GOPRO, and REDS. RealBlur sharp and blurry frames are naturally aligned during capture and the alignment is further re\ufb01ned in the post process described in the paper [39]. GOPRO and REDS report a sharp frame as one in the middle of the window to synthesize a blurry frame. Although this doesn\u2019t necessarily equate to centering the blur kernel since movement can be asymmetric on either side of the sharp frame, we use this as our \u201cstandard\u201d evaluation as it\u2019s the closest approximation given the data. For RealBlur, we use both train and test sets (4,738 pairs) and set to the con\ufb01dence threshold",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S77",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "on pseudo-groundtruth boxes to 0.6. For REDS, we sample 5,000 frames from the train and validation, set the con\ufb01dence threshold to 0.4, and allow only a maximum of 20 images without bounding boxes. For GOPRO, we use the combination of the train and test sets (3214 pairs) and set the con\ufb01dence threshold to 0.4. For expanded evaluation, we synthesize 5,442 blurry GOPRO frames using the method and window size limits outlined in [30], and we set the con\ufb01dence threshold on pseudo-groundtruth bounding boxes to 0.6. We omit empty scenes with Figure 7. Models evaluated on motion blur with and without defocusing. E is motion blur extent; each point averages blur types P1\u22123. Defocus is simulated by Gaussian blurring each motion-blur",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S78",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "kernel with a random \u03c3. Defocus hurts, but our model still performs well, especially compared to no-centering and the original network. no COCO object classes, namely GOPR0374_11_00, GOPR0374_11_01, GOPR0374_11_02, GOPR0374_11_03, GOPR0857_11_00, GOPR0868_11_00, GOPR0868_11_02, GOPR0871_11_00, and GOPR0396_11_00. During this process, we note the sharp frames used to synthesize blurry frames and obtain low-resolution \ufb02ow \ufb01elds using RAFT [49] to estimate where objects have moved during the exposure. We use low-resolution as apposed to the re\ufb01ned maps to avoid artefacts at object boundaries. We advect each bounding box corner using the estimated \ufb02ow \ufb01elds both forwards and backwards on either side of the sharp frame, stopping at the assigned blur window size. We then assign the bounding box corners to the super-set",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S79",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "of both the original points and the advected points. These new boxes are estimates of the super-set location of where an object has been in an exposure, and are used during expanded evaluation. 7. Qualitative Results You can \ufb01nd a video with qualitative results and a visual explanation of our methodvisual.cs.ucl.ac.uk/pubs/ handlingMotionBlur/. There, we show real world examples where our model, based on the two proposed remedies, manages to detect objects in many places where the original model fails, especially when the ratio of camera motion to object size is high. Following on from the quantitative experiments in the paper and here, we synthesize blurry COCO images (in the same spirit as [28, 15]) and show sample results in the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S80",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "video. 8. Results Tables Table 1 and Table 2 contain the raw results used to generate Fig. 5 and Fig. 6 in the paper. Table 3 and Table 4 show raw numbers for generating Fig. 2 and Fig. 3. Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Deblur then Original 55.50 49.18 42.13 30.31 12.72 6.26 Deblur then Standard Augmented 53.90 51.47 48.44 40.35 23.85 15.86 Squint 55.65 54.30 51.76 46.21 37.24 31.39 AugMix (Non Expanded) 59.34 53.13 38.07 20.70 13.63 8.21 AugMix PixelLevel 58.93 51.68 32.10 14.84 9.12 4.48 Original w/ MiniBatch, N = 16, n = 1 52.10 46.53 31.25 16.10 8.86 4.40 Standard Augmented w/ MiniBatch, N = 16, n =",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S81",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "1 48.60 47.70 44.25 37.79 27.84 20.92 Non-Centered Augmented 55.91 53.80 49.22 40.77 31.00 25.66 Standard Augmented w/ NonSpatial Augmix 55.77 54.15 51.95 46.53 38.41 31.67 Standard Augmented 56.51 54.93 52.44 46.85 37.56 31.37 Spec By Type 56.50 55.39 52.33 47.78 39.81 33.84 Spec By Exposure (Ours) 58.55 56.57 53.83 47.74 40.21 35.93 Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Deblur then Original 55.50 49.18 42.13 30.31 12.72 6.26 Deblur then Standard Augmented 53.90 51.47 48.44 40.35 23.85 15.86 Squint 55.65 54.30 51.76 46.21 37.24 31.39 AugMix (Non Expanded) 59.34 53.13 38.07 20.70 13.63 8.21 AugMix PixelLevel 58.93 51.68 32.10 14.84 9.12 4.48 Original w/ MiniBatch, N = 16, n = 1 52.10",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S82",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "46.53 31.25 16.10 8.86 4.40 Standard Augmented w/ MiniBatch, N = 16, n = 1 48.60 47.70 44.25 37.79 27.84 20.92 Non-Centered Augmented 55.91 53.80 49.22 40.77 31.00 25.66 Standard Augmented w/ NonSpatial Augmix 55.77 54.15 51.95 46.53 38.41 31.67 Standard Augmented 56.51 54.93 52.44 46.85 37.56 31.37 Spec By Type 56.50 55.39 52.33 47.78 39.81 33.84 Spec By Exposure (Ours) 58.55 56.57 53.83 47.74 40.21 35.93 Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Deblur then Original 55.50 49.18 42.13 30.31 12.72 6.26 Deblur then Standard Augmented 53.90 51.47 48.44 40.35 23.85 15.86 Squint 55.65 54.30 51.76 46.21 37.24 31.39 AugMix (Non Expanded) 59.34 53.13 38.07 20.70 13.63 8.21 AugMix PixelLevel 58.93 51.68",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S83",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "32.10 14.84 9.12 4.48 Original w/ MiniBatch, N = 16, n = 1 52.10 46.53 31.25 16.10 8.86 4.40 Standard Augmented w/ MiniBatch, N = 16, n = 1 48.60 47.70 44.25 37.79 27.84 20.92 Non-Centered Augmented 55.91 53.80 49.22 40.77 31.00 25.66 Standard Augmented w/ NonSpatial Augmix 55.77 54.15 51.95 46.53 38.41 31.67 Standard Augmented 56.51 54.93 52.44 46.85 37.56 31.37 Spec By Type 56.50 55.39 52.33 47.78 39.81 33.84 Spec By Exposure (Ours) 58.55 56.57 53.83 47.74 40.21 35.93 Table 1. Raw numbers from Fig. 5 in the paper. Non-expanded labels used during evaluation. Results are on the COCO minival set under different blur parameters and exposure. From top to bottom, the blur type changes from P1 to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S84",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "P2 to P3. Networks trained with blur augmentation would be trained on non-expanded labels. Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49 9.41 5.20 Deblur then Original 55.50 49.50 41.07 28.32 12.19 6.24 Squint Expanded Labels 56.15 56.25 54.53 50.09 42.92 37.66 AugMix Expanded Labels 51.80 46.62 34.21 18.99 11.32 6.15 AugMix PixelLevel 58.93 51.50 33.26 16.49 9.41 0.05 Original w/ MiniBatch, N = 16, n = 1 52.10 46.99 31.77 16.92 9.71 5.07 Expanded Labels w/ MiniBatch, N = 16, n = 1 47.20 45.39 39.61 30.26 20.23 13.88 Expanded Labels 56.65 56.42 54.86 50.57 43.60 38.35 Expanded Labels w/ NonSpatial Augmix 56.33 55.99 54.54 50.32 43.10 37.85 Spec By Type Expanded Labels 56.70 56.75",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S85",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "55.23 51.59 45.55 40.81 Spec By Exposure Expanded Labels (Our Best) 58.62 58.01 56.40 50.97 46.37 43.78 Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49 9.41 5.20 Deblur then Original 55.50 49.50 41.07 28.32 12.19 6.24 Squint Expanded Labels 56.15 56.25 54.53 50.09 42.92 37.66 AugMix Expanded Labels 51.80 46.62 34.21 18.99 11.32 6.15 AugMix PixelLevel 58.93 51.50 33.26 16.49 9.41 0.05 Original w/ MiniBatch, N = 16, n = 1 52.10 46.99 31.77 16.92 9.71 5.07 Expanded Labels w/ MiniBatch, N = 16, n = 1 47.20 45.39 39.61 30.26 20.23 13.88 Expanded Labels 56.65 56.42 54.86 50.57 43.60 38.35 Expanded Labels w/ NonSpatial Augmix 56.33 55.99 54.54 50.32 43.10 37.85 Spec By Type Expanded",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S86",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "Labels 56.70 56.75 55.23 51.59 45.55 40.81 Spec By Exposure Expanded Labels (Our Best) 58.62 58.01 56.40 50.97 46.37 43.78 Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49 9.41 5.20 Deblur then Original 55.50 49.50 41.07 28.32 12.19 6.24 Squint Expanded Labels 56.15 56.25 54.53 50.09 42.92 37.66 AugMix Expanded Labels 51.80 46.62 34.21 18.99 11.32 6.15 AugMix PixelLevel 58.93 51.50 33.26 16.49 9.41 0.05 Original w/ MiniBatch, N = 16, n = 1 52.10 46.99 31.77 16.92 9.71 5.07 Expanded Labels w/ MiniBatch, N = 16, n = 1 47.20 45.39 39.61 30.26 20.23 13.88 Expanded Labels 56.65 56.42 54.86 50.57 43.60 38.35 Expanded Labels w/ NonSpatial Augmix 56.33 55.99 54.54 50.32 43.10 37.85 Spec",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S87",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "By Type Expanded Labels 56.70 56.75 55.23 51.59 45.55 40.81 Spec By Exposure Expanded Labels (Our Best) 58.62 58.01 56.40 50.97 46.37 43.78 Table 2. Raw numbers from Fig. 6 in the paper. Expanded labels used during evaluation. Results are on the COCO minival set under different blur parameters and exposure. From top to bottom, the blur type changes from P1 to P2 to P3. Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Low-Exposure Augmented 58.55 56.57 53.83 47.83 29.32 18.64 P1 Standard Augmentated 57.04 55.62 53.06 46.76 35.74 28.43 P2 Standard Augmentated 56.53 55.06 52.67 47.78 39.76 33.63 P3 Standard Augmentated 55.88 54.14 51.89 47.12 33.53 23.64 P1HE Standard Augmentated 41.98 46.20 47.66",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S88",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "45.32 38.62 31.85 P2HE Standard Augmentated 34.19 38.69 41.68 42.15 40.13 35.88 P3HE Standard Augmentated 14.84 19.81 28.30 33.73 30.67 24.56 Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Low-Exposure Augmented 58.55 56.57 53.83 47.83 29.32 18.64 P1 Standard Augmentated 57.04 55.62 53.06 46.76 35.74 28.43 P2 Standard Augmentated 56.53 55.06 52.67 47.78 39.76 33.63 P3 Standard Augmentated 55.88 54.14 51.89 47.12 33.53 23.64 P1HE Standard Augmentated 41.98 46.20 47.66 45.32 38.62 31.85 P2HE Standard Augmentated 34.19 38.69 41.68 42.15 40.13 35.88 P3HE Standard Augmentated 14.84 19.81 28.30 33.73 30.67 24.56 Variant Clean E1 E2 E3 E4 E5 Original 58.50 50.95 32.59 15.75 8.75 4.58 Low-Exposure Augmented 58.55 56.57 53.83 47.83 29.32 18.64 P1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S89",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "Standard Augmentated 57.04 55.62 53.06 46.76 35.74 28.43 P2 Standard Augmentated 56.53 55.06 52.67 47.78 39.76 33.63 P3 Standard Augmentated 55.88 54.14 51.89 47.12 33.53 23.64 P1HE Standard Augmentated 41.98 46.20 47.66 45.32 38.62 31.85 P2HE Standard Augmentated 34.19 38.69 41.68 42.15 40.13 35.88 P3HE Standard Augmentated 14.84 19.81 28.30 33.73 30.67 24.56 Table 3. Raw numbers for standard augmented specialists performance (Fig. 2). Results are on the COCO minival set under different blur parameters and exposure. From top to bottom, the blur type changes from P1 to P2 to P3. Networks are trained and evaluated on non-expanded \u201cstandard\u201d labels under blur augmentation, with the exception of Original. Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S90",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "9.41 5.20 Low-Exposure Expanded Labels 58.62 58.06 56.38 50.97 33.95 22.40 P1 Expanded Labels 57.39 57.13 55.67 50.78 41.93 35.50 P2 Expanded Labels 56.68 56.34 55.30 51.62 45.54 40.80 P3 Expanded Labels 56.80 56.28 55.06 51.22 38.61 29.34 P1HE Expanded Labels 40.99 47.00 49.85 49.13 44.55 39.40 P2HE Expanded Labels 18.84 38.67 43.47 46.02 46.12 43.72 P3HE Expanded Labels 14.84 23.90 32.92 40.05 36.14 30.67 Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49 9.41 5.20 Low-Exposure Expanded Labels 58.62 58.06 56.38 50.97 33.95 22.40 P1 Expanded Labels 57.39 57.13 55.67 50.78 41.93 35.50 P2 Expanded Labels 56.68 56.34 55.30 51.62 45.54 40.80 P3 Expanded Labels 56.80 56.28 55.06 51.22 38.61 29.34 P1HE Expanded Labels 40.99 47.00",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S91",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "49.85 49.13 44.55 39.40 P2HE Expanded Labels 18.84 38.67 43.47 46.02 46.12 43.72 P3HE Expanded Labels 14.84 23.90 32.92 40.05 36.14 30.67 Variant Clean E1 E2 E3 E4 E5 Original 58.50 51.50 33.26 16.49 9.41 5.20 Low-Exposure Expanded Labels 58.62 58.06 56.38 50.97 33.95 22.40 P1 Expanded Labels 57.39 57.13 55.67 50.78 41.93 35.50 P2 Expanded Labels 56.68 56.34 55.30 51.62 45.54 40.80 P3 Expanded Labels 56.80 56.28 55.06 51.22 38.61 29.34 P1HE Expanded Labels 40.99 47.00 49.85 49.13 44.55 39.40 P2HE Expanded Labels 18.84 38.67 43.47 46.02 46.12 43.72 P3HE Expanded Labels 14.84 23.90 32.92 40.05 36.14 30.67 Table 4. Raw numbers for expanded augmented specialists performance (Fig. 3). Results are on the COCO minival set under different blur",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2011_14448v2:S92",
      "paper_id": "arxiv:2011.14448v2",
      "section": "method",
      "text": "parameters and exposure. From top to bottom, the blur type changes fromP1 to P2 to P3. Networks are trained and evaluated on expanded labels under blur augmentation, with the exception of Original.",
      "page_hint": null,
      "token_count": 32,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9648166334750585,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 20,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3891,
        "empty": false
      },
      {
        "page": 2,
        "chars": 2844,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5969,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4758,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5243,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4356,
        "empty": false
      },
      {
        "page": 7,
        "chars": 2665,
        "empty": false
      },
      {
        "page": 8,
        "chars": 4889,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5764,
        "empty": false
      },
      {
        "page": 10,
        "chars": 6057,
        "empty": false
      },
      {
        "page": 11,
        "chars": 555,
        "empty": false
      },
      {
        "page": 12,
        "chars": 2864,
        "empty": false
      },
      {
        "page": 13,
        "chars": 2141,
        "empty": false
      },
      {
        "page": 14,
        "chars": 2092,
        "empty": false
      },
      {
        "page": 15,
        "chars": 3075,
        "empty": false
      },
      {
        "page": 16,
        "chars": 1947,
        "empty": false
      },
      {
        "page": 17,
        "chars": 2716,
        "empty": false
      },
      {
        "page": 18,
        "chars": 2385,
        "empty": false
      },
      {
        "page": 19,
        "chars": 1839,
        "empty": false
      },
      {
        "page": 20,
        "chars": 1751,
        "empty": false
      }
    ],
    "quality_score": 0.9648,
    "quality_band": "good"
  }
}