{
  "paper": {
    "paper_id": "arxiv:2602.07860v1",
    "title": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images",
    "authors": [
      "Fei Yu",
      "Shudan Guo",
      "Shiqing Xin",
      "Beibei Wang",
      "Haisen Zhao",
      "Wenzheng Chen"
    ],
    "year": 2026,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.   In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.   We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/",
    "pdf_path": "data/automation/papers/arxiv_2602.07860v1.pdf",
    "url": "https://arxiv.org/pdf/2602.07860v1",
    "doi": null,
    "arxiv_id": "2602.07860v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.784981+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2602_07860v1:S1",
      "paper_id": "arxiv:2602.07860v1",
      "section": "body",
      "text": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images Fei Yu1, Shudan Guo1, Shiqing Xin1, Beibei Wang2, Haisen Zhao1\u2020, Wenzheng Chen3 1Shandong University 2Nanjing University 3Peking University",
      "page_hint": null,
      "token_count": 25,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S2",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recover- ing geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both nat- ural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid mo- tion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) in- effective. In this paper, we propose a novel inverse rendering ap- proach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S3",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "a fast barycentric coordinate solver, which sig- nificantly reduces computational overhead and achieves a speedup of up to4.57\u00d7, enabling efficient and photorealis- tic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facil- itating shape recovery through inverse rendering. We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realis- tic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme transla- tional and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page can be found athttps://maxmilite.github.io/rec-from- ultrafast-blur/. 1. Introduction",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S4",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "Estimating the shape of an object from image collections is crucial for numerous applications, including film produc- tion, gaming, and AR/VR. As a long-standing goal in com- puter vision and graphics, extensive research has leveraged \u2020Corresponding author. Translation Rotation Static Low-speed High-speed Figure 1.Ultra-fast motion bluris common in real-world scenar- ios. Top: A ball undergoing translational motion [32]. Bottom: A spinning top in rotation [1]. In this paper, our goal is to recover 3D shapes from the high-speed translational and rotational motion. geometric and learning-based priors for object shape recov- ery [5, 9, 11, 14, 22]. However, most existing methods focus on static objects or those with low-speed motion [4, 23, 43], leaving shape recovery of high-speed objects largely under-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S5",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "explored. In this work, we investigate an extremely challenging question: Can we recover the shape of an object under- going ultra-fast motion? Fast motion is prevalent in real- world scenarios, such as flying balls in sports, rotating machinery, or high-speed robotics. While reducing expo- sure time can mitigate blur, it often leads to extremely low signal-to-noise ratios in low-light conditions, making mo- tion blur physically unavoidable in many practical scenar- ios. However, extreme motion blur severely distorts the ob- ject\u2019s appearance, often obscuring the underlying shape. As shown in Fig. 1, the object\u2019s shape is barely perceptible in the captured blurry images, making traditional multi-view geometry-based methods, such as Structure from Motion (SfM) [9, 11], ineffective. SfM-based techniques rely on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S6",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "sharp feature correspondences across views, but when mo- tion blur obscures these features, shape recovery becomes highly challenging. Alternatively, recovering 3D shape from 2D image col- lections can be formulated as an inverse problem, where the objective is to optimize the shape so that its render- 1 arXiv:2602.07860v1 [cs.CV] 8 Feb 2026 ings match the observed images [22]. Leveraging this paradigm, we formulate shape recovery under ultra-fast mo- tion as an inverse rendering problem, where both geometry and appearance are estimated by simulating the blurry pro- cess. Typically, motion blur can be approximated by render- ing multiple static frames and averaging them [32\u201334, 39]. However, this method becomes computationally expensive for ultra-fast translational or rotational motions. As shown in Fig.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S7",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "2, generating realistic motion blur under such condi- tions requires synthesizing and averaging over 50 individual static frames per blurry image, leading to excessive render- ing costs and memory consumption. We carefully analyze the computational bottleneck in motion blur synthesis. While a single barycentric compu- tation is inexpensive, we identify that the repetitive calcula- tion of these weights required for temporal integration be- comes a primary source of inefficiency. This is because barycentric weights must be computed for every pixel with respect to all triangles, and the synthesis of motion blur further amplifies the computational cost by requiring these computations across all sampled frames, leading to a sig- nificant overhead. To address this issue, inspired by ana- lytic motion approximation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S8",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "techniques [10], we propose a fast barycentric coordinate solver that significantly reduces computational complexity. By integrating this solver into our differentiable rasterization framework, our approach achieves significant speedup while preserving the accuracy of motion blur simulation. Furthermore, we reformulate the rendering process in a soft, fully differentiable manner, al- lowing gradients to propagate through motion-blurred im- ages to the underlying 3D shapes. With its differentiable capabilities, our framework en- ables 3D shape recovery through an inverse rendering pipeline: Beginning with an initial 3D shape, we render motion-blurred images and compare them with the observed ground-truth (GT) images. The shape is then iteratively re- fined by minimizing the discrepancy between the rendered and GT images. This analysis-by-synthesis approach allows for shape",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S9",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "recovery from multi-view blurred images, even under extreme translational and rotational motion. We evaluate our method on a wide range of testing cases across various shapes and categories. Our method successfully recovers shapes from heavily blurred images caused by ultra-fast motion. Additionally, we demonstrate 3D shape recovery from real-world motion-blurred images, showcasing the effectiveness of our method in challenging real-world scenarios. Our work pushes the boundaries of 3D recovery from ultra-fast motion-blurred images. 2. Related Work 2.1. General Deblurring Methods Motion blur arises when multiple scene contents are pro- jected onto the same pixel due to motion during image cap- (a) Static (b) 15 Samples (c) 30 Samples (d) 50 Samples (e) 100 Samples (f) 180 Samples Figure 2. Motion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S10",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "blur is typically synthesized by rendering and averaging multiple frames. However, for extreme motion, a large number of frames are required to achieve realistic results. Here, we illustrate a bicycle undergoing extreme translation. Noticeable artifacts appear when using fewer samples, and at least 50 frames are needed to produce a realistic motion-blurred image. ture [43]. This blur can originate from various sources, in- cluding camera motion, object motion, or long exposures in low-light conditions. Typically, motion blur is mod- eled as a convolution of a clean image with a blur ker- nel. Numerous methods have been developed to address this issue, leveraging various priors such as total varia- tion (TV) and phase information [27], deep neural net- works [12, 13,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S11",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "26, 36, 44, 45], generative adversarial net- works [19, 20, 42], and, more recently, diffusion mod- els [2, 6, 7, 38, 39]. However, these methods primarily focus on low-speed motion, where the blur kernels remain rela- tively small. Furthermore, most approaches are confined to 2D image space, making them ineffective for handling more complex, non-linear motion patterns, such as rotations. 2.2. Shape Recovery from Blurry Images or Videos Our objective is to recover 3D shapes from blurry im- ages of objects undergoing extremely fast motion [10, 37]. Objects exhibiting motion blur are often categorized as Fast Moving Objects (FMOs) [30]. Prior works have ex- plored reconstructing both shape and motion from images or videos [17, 18, 31]. Rozumnyi et al.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S12",
      "paper_id": "arxiv:2602.07860v1",
      "section": "abstract",
      "text": "[33, 34] pioneered",
      "page_hint": null,
      "token_count": 3,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S13",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "from blurry images and videos. These approaches effec- tively leverage neural network-based learned priors, such as the DeFMO network [32], to predict per-timestamp static silhouettes of the object. Combined with differentiable ren- dering techniques [5, 22], they jointly estimate shape and motion via optimization, enabling robust solutions across a broad range of practical scenarios. While impressive results are achieved, these methods also largely rely on accurately estimating object motion and static silhouettes, which might struggle with challenging, ultra-fast motion inputs. In this scenario, the resulting blur 2 introduces an unprecedented level of visual ambiguity, mak- ing the accurate recovery of static object silhouettes par- ticularly challenging for [32]. In contrast, our method enables shape recovery under significantly more extreme high-speed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S14",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "motion conditions, demonstrating new capabili- ties in reconstructing objects undergoing ultra-fast motion. 2.3. Inverse Rendering Shape recovery through inverse rendering has made rapid progress in recent years. A variety of 3D representa- tions, including meshes, neural radiance fields (NeRF) [24], and Gaussian splatting [15], have been combined with differentiable rendering techniques such as mesh render- ing [5, 14, 21, 22], volume rendering [8], and surface ren- dering [28] to jointly estimate shape, texture, lighting, and material directly from images [25, 40]. However, these",
      "page_hint": null,
      "token_count": 83,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S15",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "their applicability to motion-blurred scenes remains limited. In this paper, we propose a differentiable, rasterization- based renderer specifically designed to handle ultra-fast mo- tion. Our approach extends inverse rendering to extreme motion-blurred conditions, making it a promising solution for high-speed shape recovery. 3. Method We now describe our method. We first provide the prelimi- naries of traditional rasterization algorithms in Sec. 3.1 and analyze their computational bottleneck. We then present our solution: a fast barycentric coordinate solver in Sec. 3.2. With this new solver, we detail our differentiable motion- blur rendering algorithm in Sec. 3.3. 3.1. Preliminaries of Rasterization Rasterization is a fundamental rendering technique that projects 3D triangle meshes onto a 2D image plane. Typ- ically, it operates on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S16",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a per-pixel basis by computing its barycentric coordinates with respect to each triangle. For a screen pixelp i and a projected triangle faceF j with three vertices \u0002v0 v1 v2 \u0003 , we denote the barycentric coordi- nates ofp i with respect toF j asw= \u0002w0 w1 w2 \u0003T , which satisfies the equation: pi =w 0v0 +w 1v1 +w 2v2.(1) The vectorwis then used to interpolate vertex attributes, such as colors or texture UV mappings. Traditional differentiable rasterizers (e.g., [5, 22]) com- putewby solving a linear system. For example, if we define pi = \u0002 u v1 \u0003T and each vertexv= \u0002x y1 \u0003T , the triangleF j can be expressed as: Fj = \uf8ee \uf8f0 x0 x1 x2",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S17",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "y0 y1 y2 1 1 1 \uf8f9 \uf8fb.(2) The barycentric coordinates can then be obtained by solving: Fjw=p i \u21d2w=F \u22121 j pi.(3) If allw 0, w1, w2 fall within the range[0,1], the pixelp i is covered by the triangleF j. Its final color is then deter- mined using the Z-buffer algorithm, which selects the clos- est surface among all overlapping triangles.",
      "page_hint": null,
      "token_count": 62,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S18",
      "paper_id": "arxiv:2602.07860v1",
      "section": "discussion",
      "text": "putation constitutes a significant computational bottleneck in the context of differentiable motion-blur rasterization. Since barycentric weights for every pixel must be computed with respect to relevant triangles, the cost scales linearly with the number of temporal samples required to generate smooth, realistic blur effects. To overcome this limitation, we propose a fast barycentric coordinate solver that dras- tically reduces computational complexity and significantly accelerates rendering speed. 3.2. Fast Barycentric Coordinate Solver Traditional rasterization methods [5, 22] synthesize motion blur by rendering multipleKframes and averaging them. However, by assuming that each triangle moveslinearlyin time, we propose a fast barycentric coordinate solver that avoids the heavy cost of repeatedKbarycentric computa- tion. Consider a 3D mesh objectMmoving linearly from time T= 0toT= 1,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S19",
      "paper_id": "arxiv:2602.07860v1",
      "section": "discussion",
      "text": "where our goal is to renderKframes at time stepsT= 0 K\u22121 , 1 K\u22121 , 2 K\u22121 , . . . ,K\u22121 K\u22121 . For a triangle Fj moving fromT= 0toT= 1, at a specific timeT=t, its time-dependent vertex positions can be defined as: Fj(t) = \u0002v0(t)v 1(t)v 2(t)\u0003 .(4) Since we assume linear motion, each vertex position fol- lows linear interpolation between the starting pointv(0)and ending pointv(1): v(t) = (1\u2212t)v(0) +tv(1).(5) Thus, the matrix representation ofF j(t)can be repre- sented as: Fj(t) = \uf8ee \uf8f0 x0(t)x 1(t)x 2(t) y0(t)y 1(t)y 2(t) 1 1 1 \uf8f9 \uf8fb.(6) We then compute the barycentric weightsw(t)as: w(t) =F j(t)\u22121pi = adj(Fj(t)) det(Fj(t)) pi,(7) whereadj(F j(t))anddet(F j(t))are the adjugate matrix and determinant ofF j(t).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S20",
      "paper_id": "arxiv:2602.07860v1",
      "section": "discussion",
      "text": "3 Moreover, with the assumption of linear motion, they could be written as quadratic functions oft: w(t) = A1t2 +A 2t+A 3 a1t2 +a 2t+a 3 ,(8) whereA 1,A2,A3, a1, a2, a3 are precomputed3\u00d71vectors and values that are independent oftand depend solely on Fj(0),F j(1)andp i. Consequently, for the totalKframes, these coefficients (A1,A2,A3, a1, a2, a3) can be computed only once, and the barycentric coordinatew(t)can then be evaluated using Eq. (8). This allows for efficient barycen- tric computation without per-frame solving barycentric lin- ear equations. The full derivation is provided in Section B. 3.3. Differentiable Rasterization With our fast solver, we now describe our differentiable motion-blur rasterization, which is built on prior state-of- the-art differentiable rasterization works SoftRas [22] and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S21",
      "paper_id": "arxiv:2602.07860v1",
      "section": "discussion",
      "text": "DIB-R [5]. We first decompose the entire motion into several segments, assuming that inside each segment, all faces move linearly, which is a common assumption in previous motion-blur simulation work [10, 29, 37]. Note that our",
      "page_hint": null,
      "token_count": 36,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S22",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "posed of rotation and translation, as long as it can be di- vided into linear motion segments (see Sec. L.1). For linear motions, such as translation, larger segments can be used, whereas for non-linear motions, like rotation, smaller seg- ments are employed. In our experiments, we find that even for the extreme rotational motion, we can divide the full ro- tation into 12 segments and render smooth results. Next, for each segment, we treat its start and end as keyframes and render intermediate frames with our fast solver. These rendered frames are averaged to generate the segment blurry image. Subsequently, all segment images are further averaged to produce the final blurry image. Following DIB-R [5], we also separately processfore- groundpixels",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S23",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "(covered by one or more faces) andback- groundpixels (not covered by any faces) attributes. Foreground PixelsFor foreground pixels, we perform barycentric interpolation for each frame at timeT=ton the closest covering face using the Z-buffer: I(t) =w 0(t)c0 +w 1(t)c1 +w 2(t)c2,(9) wherecrepresents vertex attributes (e.g., vertex colors or texture UV coordinates).",
      "page_hint": null,
      "token_count": 51,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S24",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "angle, in differentiable rasterization it is assumed that it could be influenced by all triangles. Similarly, we extend the probability of a triangleF j influencing a pixelp i to a time-dependent version: Aj i (t) = exp \u0012 \u2212d(pi,F j(t)) \u03b4 \u0013 ,(10) whereA j i (t)is the time-dependent probability,\u03b4is a hyper- parameter [22], andd(p i,F j(t))is the squared Euclidean distance, which can be defined as d(pi,F j(t)) = min p\u2208Fj(t) ||pi \u2212p|| 2 2 .(11) The core of the Euclidean distance calculation lies in findingp\u2208F j(t)that is closest top i. By replacingpwith another formp=F j(t)\u02c6w, where we constrain\u02c6w\u2208[0,1]3 to ensurep\u2208F j(t), finding the closestpis equivalent to finding\u02c6w\u2217, which can be written as: \u02c6w\u2217 = arg min \u02c6w\u2208[0,1]3 \u2225Fj(t)w(t)\u2212F j(t)\u02c6w\u22252",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S25",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "2 ,(12) where we also replacep i =F j(t)w(t). However, we find that evaluating Equation (12) requires additional computational resources for computingF j(t) across frames. To further accelerate the computation, we approximateF j(t)with eitherF j(0)orF j(1), depending on whethertis closer to the start or the end: \u02c6w\u2217=arg min \u02c6w\u2208[0,1]3 \u2225Fj(X)w(t)\u2212Fj(X)\u02c6w\u22252 2, X= \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 0t\u22640.5 1t>0.5 (13) Eq. (13) requires only the evaluation ofF(0) j ,F (1) j , which significantly reduces computational cost while introducing only minor approximation errors. The full derivation is pro- vided in Sec. B. Finally, with the computed\u02c6w\u2217, we obtain d(pi,F j(t)) =\u2225F j(t)w(t)\u2212F j(t)\u02c6w\u2217)\u22252 2 .(14) We then combine the probabilistic influence of all trian- gle faces on a particular pixel as Ai(t)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S26",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "= 1\u2212 Y j \u0010 1\u2212A j i (t) \u0011 .(15) Gradient ComputationEquations (9) and (15) are fully differentiable [5, 22]. Therefore, our method supports backpropagation by propagating gradients through each time-dependent intermediate frame, and ultimately into the keyframes, ensuring efficient optimization in inverse ren- dering tasks. 4. Analysis In this section, we evaluate the effectiveness of our method through extensive synthetic experiments. We implemented our method based on SoftRas [22] but split the pixels into foreground and background, following DIB-R [5]. We pro- vide the implementation details in Section C. We first analyze the ultra-fast motion blur synthesis ef- fect in Sec. 4.1, including both forward rendering and back- ward gradients. Then, in Sec. 4.2, we present the computa-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S27",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "tion speed, demonstrating significant acceleration over prior methods. 4 Translation Rotation 10 Samples 50 Samp. 12 Samp. 60 Samp. 240 Samp. Forward SoftRas Ours Gradient SoftRas Ours Figure 3. Forward rendering & backward gradient visualization for ultra-fast motion-blur synthesis. Our rendered images and gra- dients exhibit a high degree of similarity to those generated by SoftRas across various motion cases and sample numbers. Scene settings and render details are provided in Sections E and I. 4.1. Qualitative Validation We first present the synthesis of ultra-fast motion blur effects, including both translational and rotational move- ments. We show forward rendering results and their back- ward gradients. As a reference, we also apply SoftRas [22] to render with the same settings (e.g.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S28",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "the same number of sampled frames) and compute the corresponding gradients. The default hyperparameter values provided in SoftRas are used for this comparison. The comparison results are shown in Fig. 3. Across all test cases, our rendered images and gradients exhibit a high degree of similarity to those generated by SoftRas, which demonstrates the effectiveness of our method in realistic and differentiable motion blur synthesis. In theory, un- der linear motion, our foreground pixel renderings should be identical to those of SoftRas, whereas our background pixel computation shows slight discrepancies, primarily due to our Euclidean distance approximation (as detailed in Eqs. (12) and (13)). However, we show that these minor discrepancies have negligible impact on gradient computa- tion of our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S29",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "method (Fig. 3 Bottom). 4.2. Speed Comparison Our method is significantly more efficient than traditional blur synthesis methods,e.g., applying SoftRas to render and average multiple frames. To evaluate the running speed, we randomly select 50 models from ShapeNet [3], each con- taining an average of 5,536 faces. We apply random rota- tions to each model, render128\u00d7128front-view motion- blurred silhouette and color images, and measure the time required for both forward rendering and gradient computa- tion in a single pass. We render objects undergoing linear translation with a varying number of samples. The evalua- 0 50 100 150 200 250Number of Samples 0 200 400 600 800 1000 1200Time (ms) y 4.78x y 1.28x y 1.04x x y SoftRasNvdiffrastOurs Figure 4.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S30",
      "paper_id": "arxiv:2602.07860v1",
      "section": "background",
      "text": "Forward + gradient computation timing results. The slope represents the average time of sampling once. The lower the better. Our method achieves speedups of up to 4.57\u00d7and 1.23\u00d7 compared to SoftRas and Nvdiffrast, respectively. tion considers the average time required for forward render- ing and backward gradient computation across all models. All experiments are conducted on a single NVIDIA RTX 4090 GPU with 24GB of memory. More details are pro- vided in Section I. The timing results are summarized in Fig. 4. Our",
      "page_hint": null,
      "token_count": 83,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S31",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "1.23\u00d7over Nvdiffrast. Regarding Nvdiffrast [21], it is a highly performant OpenGL-optimized rasterization library, whereas our implementation is built on the SoftRas imple- mentation. Nevertheless, our method is still faster than Nvdiffrast, and incorporating our method into Nvdiffrast would likely bring further acceleration. Moreover, we ob- serve that Nvdiffrast produces weak gradient signals, as its gradients are computed only near edges. This limitation can lead to slower convergence or even failure in extreme shape recovery tasks. In contrast, our method enables global gra- dient propagation across all triangle primitives, facilitating smoother optimization. Further discussion is provided in Sec. 6.4. 5. Shape Optimization from Blurred Images With our differentiable rendering pipeline, we now present inverse rendering applications,i.e., recovering 3D shapes from ultra-fast",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S32",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "motion-blurred images. Thanks to our effi- cient framework, our method supports rendering more sam- ples, resulting in smoother forward rendering effects and better backward gradients across the optimization process. We demonstrate the effectiveness and advantages of our",
      "page_hint": null,
      "token_count": 37,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S33",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "to recover the 3D shape of ultra-fast moving objects from two representative types of motion blur: multi-view trans- lational and rotational blurred images. Similar to other inverse rendering tasks, we assume known rendering parameters for each image, including its camera viewpoint and blur settings (translation or rotation speed). 5 Input [33] Ours G.T. (a) (b) (c) (d) Figure 5. Qualitative results on geometry and color optimization. (a) One of blurred input images. (b) State of the Art [33] result. (c) Our optimization result. (d) Ground-truth object. Our method yields significant superior results than the state-of-the-art work. Note that the object trajectories are synthesized to test the solver\u2019s robustness to diverse motion vectors, rather than simulating real- istic physical dynamics. 5.1.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S34",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Translational Recovery We begin with the task of optimizing a 3D shape under- going linear translation. Given multi-view RGBA transla- tional blurred images (consisting of RGB colorIand trans- parency\u03b1) as input, our method optimizes a meshMwith vertex positionsVand a texture mapCsuch that the ren- dered images, \u02c6I,\u02c6\u03b1=R(V, C), match the input. The optimization ofVandCis performed by minimiz- ing the image loss and regularization terms. We adopt the L1 loss for both colorI, \u02c6Iand transparency\u03b1,\u02c6\u03b1, formu- lated as: Limg =",
      "page_hint": null,
      "token_count": 79,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S35",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "I\u2212 \u02c6I",
      "page_hint": null,
      "token_count": 2,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S36",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "1 +\u2225\u03b1\u2212\u02c6\u03b1\u22251 .(16) Similar to [5, 22], we incorporate a smoothness lossL s and a Laplacian lossL L to regularize the deformation ofV (details provided in Section G). The final loss function is defined as: L=L img +\u03bb sLs +\u03bb LLL.(17) 5.2. Rotational Recovery We further apply our method to a more challenging task: re- constructing fast-rotating objects from motion-blurred im- ages. Similarly, given multi-view imagesIas input, our",
      "page_hint": null,
      "token_count": 68,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S37",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "served object.",
      "page_hint": null,
      "token_count": 2,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S38",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "3D IoU\u2191Static PSNR\u2191Blurred PSNR\u2191 [33] 0.152 11.63 13.58 Ours 0.679 19.20 31.89 Table 1. Quantitative comparison for shape optimization from blurred images. We compare the best performance between our",
      "page_hint": null,
      "token_count": 29,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S39",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "cantly superior performance compared to [33]. Due to the highly non-convex nature of the rotation op- timization problem, we observe that directly optimizing mesh vertices rarely yields well-shaped objects (illustrated in Sec. F). To mitigate this issue, we optimize a Signed Dis- tance Function (SDF) representation instead. We construct an SDF fieldSfollowing [41], and extract a meshMusing FlexiCubes [35] in a differentiable manner. Our differen- tiable rendererRis then employed to generate correspond- ing rotation images \u02c6I, which are subsequently used to opti- mizeSvia loss functions. Given the complexity of the SDF representation, we render grayscale images in this setting and focus on shape recovery. We retain the sameL img loss (from Eq. (17)) for im- age consistency. For SDF regularization,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S40",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "we utilize the loss termsL crit andL reg from [41] and [35], respectively (details in Section G). The final loss function is defined as: L=L img +\u03bb critLcrit +\u03bb regLreg.(18) 6. Experiments In this section, we present qualitative and quantitative ex- periments to evaluate the effectiveness and efficiency of our",
      "page_hint": null,
      "token_count": 50,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S41",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "images. In Secs. 6.1 and 6.2, we present translational and rotational blurred shape recovery, respectively. Fur- thermore, we validate our method\u2019s practical applicability through real-world motion blur data in Sec. 6.3. Finally, we conduct an ablation study in Sec. 6.4, to demonstrate the benefits of our method compared to the widely adopted codebase SoftRas [22] and Nvdiffrast [21] under extreme motion blur conditions. All hyperparameter settings and more results are provided in Sections I, J and L. 6.1. Translational Recovery In this experiment, we present our method\u2019s 3D shape re- construction performance for objects undergoing transla- tional motion, specifically benchmarking against the state- of-the-art [33]. We perform optimization on 25 selected shapes from ShapeNet, and evaluate both the geometry and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S42",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "color recovery. For geometry quantitative evaluation, we voxelize the predicted and ground truth meshes into323 vol- umes, and compute the 3D IoU. For color quantitative eval- uation, we compare the PSNR of multi-view static novel- view-synthesis (NVS) of the objects. Quantitative evalua- 6 Input [33] Ours G.T. (a) (b) (c) (d) Figure 6. Qualitative results for optimization on rotating objects. (a) One of blurred input images. (b) State of the Art [33] result. (c) Our optimization result. (d) Ground-truth object. Our method also yields a significant superior result than the state-of-the-art work. tion results are presented in Tab. 1, while qualitative results are illustrated in Fig. 5. We assess our method\u2019s fundamental capability in 3D shape recovery from highly motion-blurred",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S43",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "images by benchmarking against the state-of-the-art work [33]. As presented in Tab. 1 and Fig. 5, our method demonstrates a significant advantage in translational recovery. Notably, [33]\u2019s reliance on the learning prior [32] to predict static silhouettes fundamentally limits its performance in the chal- lenging blurry input. In contrast, our method successfully recovers meaningful 3D shape and appearance. More de- tails and analysis are provided in Section K.1. 6.2. Rotational Recovery In this experiment, we evaluate shape recovery for ob- jects undergoing ultra-fast rotational motion, benchmark- ing against the state-of-the-art [33]. Here, we observe that multiple feasible 3D shape solutions may correspond to the same blurred image; a detailed analysis is provided in Section H. Consequently, traditional 3D evaluation metrics",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S44",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "(e.g., 3D IoU, Chamfer Distance) are not suitable. There- fore, we assess the similarity between the rotational-blurred images rendered from the reconstructed objects and the ground truth, quantified using PSNR. Similar to Sec. 6.1, for rotational motion, a similar trend of superior performance is observed for our method com- pared to [33]. As shown in Tab. 1 and Fig. 6, our method achieves a significantly superior performance and succeeds in a high-quality 3D reconstructions even under extreme ro- (a) Experimental (b) Image (d) Ours Setup Captured Result Captured [33] Ours G.T. Figure 7. Real-world experiment. (a) Our experimental setup. (b) The original image captured. (c) Ours optimization result. The remaining rows: More real-world examples. Quantitatively, our method achieves a superior",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S45",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "blurred PSNR score of 24.52 dB com- pared to 12.51 dB for [33]. Our method is capable of reconstruct- ing 3D objects from real-world motion-blurred images. tational scenarios. 6.3. Real-World Results Next, we evaluate our method on real-world images. We capture a front view of 3D-printed rotating objects at 100 Hz with a camera exposure time of1/100s. Since the data was captured in a controlled studio environment with a black background, we extract the object alpha masks based on pixel intensity thresholds to serve as supervision signals. After preprocessing, including cropping and brightness cor- rection, we perform rotational shape optimization using the same settings as described in Sec. 5.2.",
      "page_hint": null,
      "token_count": 109,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S46",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "3D print technique allows us to establish ground truth for evaluation. As summarized, our method achieves a supe- rior blurred PSNR score of 24.52 dB compared to 12.51 dB for [33], demonstrating a significant improvement. Qualita- tively, due to the imperfect pose and noise introduced in real data, the recovered shape exhibits slight artifacts compared 7 Figure 8. Optimization results for objects undergoing translation and rotation. We draw optimization time v.s. performance curves for our method and SoftRas, where each point indicate different number of samples. to the synthetic recovery results. Nevertheless, our method successfully recovers reasonable shapes, demonstrating its capability to handle real-world data effectively. 6.4. Ablation Study Finally, we conduct an ablation study to validate our key design",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S47",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "choices. Specifically, we analyze the efficiency im- provements of our method compared to the codebase Sof- tRas, and assess the robustness and gradient quality of our",
      "page_hint": null,
      "token_count": 26,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S48",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "treme motion blur scenarios. Comparison with SoftRasOur method is built upon the SoftRas framework. Therefore, we compare our method against SoftRas in terms of optimization time and recon- struction quality. As shown in Fig. 8, several interesting points can be observed. First, increasing the number of samples improves performance but also increases optimiza- tion time. This is reasonable as more samples result in better blur simulation, yielding better shape recovery per- formance and longer time. Second, our method exhibits strong efficiency over SoftRas. Given the same number of samples, our method achieves significantly faster optimiza- tion time. Conversely, for the same optimization time, our method yields superior reconstruction quality. Comparison with NvdiffrastNext, we present a compar- ison of our method\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S49",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "gradient quality and robustness against Nvdiffrast. We first quantitatively assess the convergence behaviors of both methods as a function of the number of iterations. As illustrated in Fig. 9, our method demonstrates a significantly faster convergence rate. This is attributable to the stronger and more stable gradients generated by our",
      "page_hint": null,
      "token_count": 50,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S50",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "produced by Nvdiffrast. Beyond convergence speed, we observe that Nvdiffrast [21] frequently encounters catastrophic failures, leading to the inability to reconstruct valid meshes. For example, in our experiments on rotational recovery, Nvdiffrast failed to successfully complete the process (manifested as program crashes) in any of the 10 repeated attempts within 8 out of 25 test data cases, which even precluded a quantitative com- parison with our method. Furthermore, even in cases where Nvdiffrast manages to complete the reconstruction, its out- put quality often exhibits lower fidelity compared to ours, Figure 9. Comparison of convergence rates between our method and Nvdiffrast w.r.t. number of iterations. Labels denote numbers of iterations. The convergence rate of Nvdiffrast is significantly slower than that of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S51",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "our method, which we attribute to its weaker pixel-wise gradients. (a) Nvdiffrast (b) Ours (c) Nvdiffrast (d) Ours Figure 10. Failure cases of Nvdiffrast. (a, c) Nvdiffrast optimiza- tion results. (b, d) Ours optimization results. In this task, our",
      "page_hint": null,
      "token_count": 39,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S52",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "narios, whereas Nvdiffrast frequently fails, or producing distorted and uneven objects. as illustrated in Fig. 10. In stark contrast, our method con- sistently achieves successful, well-shaped, and high-fidelity reconstructions. More analysis is provided in Section K.2. 7. Discussion",
      "page_hint": null,
      "token_count": 38,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S53",
      "paper_id": "arxiv:2602.07860v1",
      "section": "limitations",
      "text": "era poses and motion parameters. Additionally, our physical formation model assumes linear motion segments and a lin- ear, noise-free photometric response. While effective, these assumptions may deviate from in-the-wild scenarios char- acterized by complex non-linear motion, camera response functions (tone mapping), or sensor noise. We provide a comprehensive discussion on these limitations and future works in Section M.",
      "page_hint": null,
      "token_count": 58,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S54",
      "paper_id": "arxiv:2602.07860v1",
      "section": "conclusion",
      "text": "rendering approach for 3D shape recovery from ultra-fast motion-blurred images. Our fast barycentric coordinate solver accelerates rendering while preserving accuracy, en- abling efficient and fully differentiable shape reconstruc- tion. Experimental results validate the effectiveness of our",
      "page_hint": null,
      "token_count": 36,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S55",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "3D reconstruction under ultra-fast motion blur. 8. Acknowledgements This work is supported in part by grants from National Key Research and Development Program of China (Grant No. 2024YFB3309500), National Natural Science Foundation of China (Grant No. U23A20312, 62472257). 8 References [1] Moritz B \u00a8acher, Emily Whiting, Bernd Bickel, and Olga Sorkine-Hornung. Spin-it: Optimizing moment of inertia for spinnable objects.ACM Transactions on Graphics (TOG), 33(4):1\u201310, 2014. 1 [2] Weimin Bai, Siyi Chen, Wenzheng Chen, and He Sun. Blind inversion using latent diffusion priors.arXiv preprint arXiv:2407.01027, 2024. 2 [3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.arXiv preprint arXiv:1512.03012, 2015. 5",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S56",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "[4] Wenbo Chen and Ligang Liu. Deblur-gs: 3d gaussian splat- ting from camera motion blurred images.Proceedings of the ACM on Computer Graphics and Interactive Techniques, 7 (1):1\u201315, 2024. 1 [5] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith, Jaako Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to pre- dict 3d objects with an interpolation-based differentiable ren- derer. InNeurIPS, 2019. 1, 2, 3, 4, 6 [6] Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sam- pling for general noisy inverse problems.arXiv preprint arXiv:2209.14687, 2022. 2 [7] Hyungjin Chung, Jeongsol Kim, Sehui Kim, and Jong Chul Ye. Parallel diffusion models of operator and image for blind inverse problems. InProceedings of the IEEE/CVF Con- ference",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S57",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "on Computer Vision and Pattern Recognition, pages 6059\u20136069, 2023. 2 [8] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. V ol- ume rendering.ACM Siggraph Computer Graphics, 22(4): 65\u201374, 1988. 3 [9] Yasutaka Furukawa, Carlos Hern \u00b4andez, et al. Multi-view stereo: A tutorial.Foundations and Trends\u00ae in Computer Graphics and Vision, 9(1-2):1\u2013148, 2015. 1 [10] Carl Johan Gribel, Michael C Doggett, and Tomas Akenine- M\u00a8oller. Analytical motion blur rasterization with compres- sion.High Performance Graphics, 10, 2010. 2, 4 [11] Richard Hartley and Andrew Zisserman.Multiple view ge- ometry in computer vision. Cambridge university press, 2003. 1 [12] Meiguang Jin, Givi Meishvili, and Paolo Favaro. Learning to extract a video sequence from a single motion-blurred image. InProceedings of the IEEE Conference on Computer Vision",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S58",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "and Pattern Recognition, pages 6334\u20136342, 2018. 2 [13] Meiguang Jin, Zhe Hu, and Paolo Favaro. Learning to extract flawless slow motion from blurry videos. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8112\u20138121, 2019. 2 [14] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu- ral 3d mesh renderer. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 3907\u2013 3916, 2018. 1, 3 [15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk \u00a8uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering.ACM Transactions on Graphics (ToG), 42(4):1\u201314, 2023. 3 [16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2014. 5 [17] Jan Kotera, Denys Rozumnyi, Filip Sroubek, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S59",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Jiri Matas. Intra-frame object tracking by deblatting. InProceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0\u20130, 2019. 2 [18] Jan Kotera, Ji \u02c7r\u00b4\u0131 Matas, and Filip \u02c7Sroubek. Restoration of fast moving objects.IEEE Transactions on Image Processing, 29:8577\u20138589, 2020. 2 [19] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji \u02c7r\u00b4\u0131 Matas. Deblurgan: Blind mo- tion deblurring using conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8183\u20138192, 2018. 2 [20] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better. InProceedings of the IEEE/CVF Interna- tional Conference on Computer Vision (ICCV), 2019. 2 [21] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S60",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering.ACM Transac- tions on Graphics (ToG), 39(6):1\u201314, 2020. 3, 5, 6, 8 [22] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras- terizer: A differentiable renderer for image-based 3d reason- ing.The IEEE International Conference on Computer Vision (ICCV), 2019. 1, 2, 3, 4, 5, 6 [23] Yiren Lu, Yunlai Zhou, Disheng Liu, Tuo Liang, and Yu Yin. Bard-gs: Blur-aware reconstruction of dynamic scenes via gaussian splatting. InProceedings of the Computer Vision and Pattern Recognition Conference, pages 16532\u201316542, 2025. 1 [24] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra- mamoorthi, and R Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. InEuropean conference on computer vision,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S61",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "2020. 3 [25] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi- dler. Extracting Triangular 3D Models, Materials, and Light- ing From Images. InProceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 8280\u20138290, 2022. 3 [26] Jinshan Pan, Haoran Bai, and Jinhui Tang. Cascaded deep video deblurring using temporal sharpness prior. InProceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3043\u20133051, 2020. 2 [27] Liyuan Pan, Richard Hartley, Miaomiao Liu, and Yuchao Dai. Phase-only image based kernel estimation for single im- age blind deblurring. InProceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 6034\u20136043, 2019. 2 [28] Hanspeter Pfister,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S62",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Matthias Zwicker, Jeroen Van Baar, and Markus Gross. Surfels: Surface elements as rendering primi- tives. InProceedings of the 27th annual conference on Com- puter graphics and interactive techniques, pages 335\u2013342, 2000. 3 9 [29] Mads JL R\u00f8nnow, Ulf Assarsson, and Marco Fratarcangeli. Fast analytical motion blur with transparency.Computers & Graphics, 95:36\u201346, 2021. 4 [30] Denys Rozumnyi, Jan Kotera, Filip Sroubek, Lukas Novotny, and Jiri Matas. The world of fast moving objects. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5203\u20135211, 2017. 2 [31] Denys Rozumnyi, Jan Kotera, Filip Sroubek, and Jiri Matas. Sub-frame appearance and 6d pose estimation of fast mov- ing objects. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6778\u2013",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S63",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "6786, 2020. 2 [32] Denys Rozumnyi, Martin R Oswald, Vittorio Ferrari, Jiri Matas, and Marc Pollefeys. Defmo: Deblurring and shape recovery of fast moving objects. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3456\u20133465, 2021. 1, 2, 3, 7, 5, 6 [33] Denys Rozumnyi, Martin R Oswald, Vittorio Ferrari, and Marc Pollefeys. Shape from blur: Recovering textured 3d shape and motion of fast moving objects.Advances in Neu- ral Information Processing Systems, 34:29972\u201329983, 2021. 2, 6, 7, 5, 8 [34] Denys Rozumnyi, Martin R Oswald, Vittorio Ferrari, and Marc Pollefeys. Motion-from-blur: 3d shape and motion es- timation of motion-blurred objects in videos. InProceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 15990\u201315999, 2022.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S64",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "2 [35] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization.ACM Trans. Graph., 42(4), 2023. 6, 3, 4, 5 [36] Wang Shen, Wenbo Bao, Guangtao Zhai, Li Chen, Xiongkuo Min, and Zhiyong Gao. Blurry video frame interpolation. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 5114\u20135123, 2020. 2 [37] Konstantin Shkurko, Cem Yuksel, Daniel Kopta, Ian Mallett, and Erik Brunvand. Time interval ray tracing for motion blur. IEEE transactions on visualization and computer graphics, 24(12):3225\u20133238, 2017. 2, 4 [38] Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S65",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "problems with latent diffusion models via hard data consistency.arXiv preprint arXiv:2307.08123, 2023. 2 [39] Radim Spetlik, Denys Rozumnyi, and Ji \u02c7r\u00b4\u0131 Matas. Single- image deblurring, trajectory and shape recovery of fast mov- ing objects with denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Winter Conference on Appli- cations of Computer Vision, pages 6857\u20136866, 2024. 2 [40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 3 [41] Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Peng- Shuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, and Changhe Tu. Neural-singular-hessian: Implicit neural representation of unoriented point clouds by enforcing sin- gular hessian.ACM Transactions on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S66",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Graphics (TOG), 42 (6):1\u201314, 2023. 6, 4 [42] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn Stenger, Wei Liu, and Hongdong Li. Deblurring by realistic blurring. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2737\u20132746, 2020. 2 [43] Kaihao Zhang, Wenqi Ren, Wenhan Luo, Wei-Sheng Lai, Bj\u00a8orn Stenger, Ming-Hsuan Yang, and Hongdong Li. Deep image deblurring: A survey.International Journal of Com- puter Vision, 130(9):2103\u20132130, 2022. 1, 2 [44] Zhihang Zhong, Ye Gao, Yinqiang Zheng, and Bo Zheng. Efficient spatio-temporal recurrent neural network for video deblurring. InComputer Vision\u2013ECCV 2020: 16th Euro- pean Conference, Glasgow, UK, August 23\u201328, 2020, Pro- ceedings, Part VI 16, pages 191\u2013207. Springer, 2020. 2 [45] Shangchen Zhou, Jiawei Zhang, Jinshan Pan, Haozhe",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S67",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Xie, Wangmeng Zuo, and Jimmy Ren. Spatio-temporal filter adaptive network for video deblurring. InProceedings of the IEEE/CVF international conference on computer vision, pages 2482\u20132491, 2019. 2 10 Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images Supplementary Material A. Appendix Overview In this appendix, we provide a comprehensive explanation of the technical details and additional experimental results of our work. We start with the derivation of our method in Section B, followed by implementation details in Section C. Section D discusses segmentation analysis, and Section E covers vi- sualization details. Scene settings are outlined in Section I. We then address limitations with a failure case of rotational optimization in Section F and detail all loss terms in Sec- tion G. Section H",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S68",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "analyzes the suitability of 3D losses for evaluation. Finally, Section J provides hyperparameter set- tings, Sec. K provides further analysis of baselines, and Sec- tion L presents more results. B. Derivation of Our Method In this section, we show the derivation of our method. Barycentric Coordinate SolverWe detail the derivation ofA 1,A 2,A 3,a 1,a 2,a 3. First we introduce the definition ofadj(F j(t))and det(Fj(t)). For a3\u00d73matrixA, given by: A= \uf8ee \uf8f0 a11 a12 a13 a21 a22 a23 a31 a32 a33 \uf8f9 \uf8fb,(A1) its determinantdet(A)is computed as: det(A) =a 11",
      "page_hint": null,
      "token_count": 90,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S69",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a22 a23 a32 a33",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S70",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "\u2212a12",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S71",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a21 a23 a31 a33",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S72",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "+a13",
      "page_hint": null,
      "token_count": 1,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S73",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a21 a22 a31 a32",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S74",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "(A2) where each2\u00d72determinant (called a minor) is computed as:",
      "page_hint": null,
      "token_count": 9,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S75",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a b c d",
      "page_hint": null,
      "token_count": 4,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S76",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "=ad\u2212bc.(A3) For the matrixA, its adjugate matrixadj(A)can be de- fined by: adj(A) = \uf8ee \uf8f0 C11 C21 C31 C12 C22 C32 C13 C23 C33 \uf8f9 \uf8fb,(A4) whereC ij are defined as: Cij = (\u22121)i+jMij andM ij is the determinant of the minor matrix obtained by deleting thei-th row andj-th column ofA. IfAis invertible (i.e.,det(A)\u0338= 0), the inverse ofAcan be expressed in terms of its adjugate matrix: A\u22121 = adj(A) det(A).(A5) Now we consider derivation on a triangle. For a triangle matrixF j(t)represented as: Fj(t) = \uf8ee \uf8f0 x0(t)x 1(t)x 2(t) y0(t)y 1(t)y 2(t) 1 1 1 \uf8f9 \uf8fb,(A6) where the vertex positionv(t)can be defined by: v(t) = (1\u2212t)v(0) +tv(1),(A7) its determinantdet(F j(t))is det(Fj(t)) =x 0(t) \u0000 y1(t)\u2212y 2(t) \u0001 \u2212x",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S77",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "1(t) \u0000 y0(t)\u2212y 2(t) \u0001 +x 2(t) \u0000 y0(t)\u2212y 1(t) \u0001 , (A8) and the adjugate matrixadj(A)is adj(Fj (t)) =\uf8ee \uf8f0 y1(t)\u2212y 2(t)x 2(t)\u2212x 1(t)x 1(t)y2(t)\u2212x 2(t)y1(t) y2(t)\u2212y 0(t)x 0(t)\u2212x 2(t)x 2(t)y0(t)\u2212x 0(t)y2(t) y0(t)\u2212y 1(t)x 1(t)\u2212x 0(t)x 0(t)y1(t)\u2212x 1(t)y0(t) \uf8f9 \uf8fb . (A9) Given a pixelp i = \u0002u v1 \u0003T , we have w(t) =F j(t)\u22121pi = adj(Fj(t))\u00d7p i det(Fj(t)) = A1t2 +A 2t+A 3 a1t2 +a 2t+a 3 (A10) By Eqs. (A6) to (A10), we can representA 1,A 2,A 3, a1,a 2,a 3 usingx i(0/1), yi(0/1), u, v: A1 = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 \u2212(x2(0)\u2212x 2(1))(y1(0)\u2212y 1(1)) +(x1(0)\u2212x 1(1))(y2(0)\u2212y 2(1)) ((x2(0)\u2212x 2(1))(y0(0)\u2212y 0(1)) \u2212(x0(0)\u2212x 0(1))(y2(0)\u2212y 2(1))) (\u2212((x1(0)\u2212x 1(1))(y0(0)\u2212y 0(1))) +(x0(0)\u2212x 0(1))(y1(0)\u2212y 1(1))) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb ,(A11) 1 A2 = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S78",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "u(\u2212y1(0) +y2(0) +y1(1)\u2212y 2(1)) +v(x1(0)\u2212x 2(0)\u2212x 1(1) +x2(1)) +(y2(0)x1(1)\u2212y 1(0)x2(1)+ x2(0)(2y1(0)\u2212y 1(1))+ x1(0)(\u22122y2(0) +y2(1))) u(y0(0)\u2212y 2(0)\u2212y 0(1) +y2(1)) +v(\u2212x0(0) +x2(0) +x0(1)\u2212x 2(1)) +(\u2212(y2(0)x0(1)) +y0(0)x2(1)+ x2(0)(\u22122y0(0) +y0(1))+ x0(0)(2y2(0)\u2212y 2(1))) u(\u2212y0(0) +y1(0) +y0(1)\u2212y 1(1)) +v(x0(0)\u2212x 1(0)\u2212x 0(1) +x1(1)) +(y1(0)x0(1)\u2212y 0(0)x1(1)+ x1(0)(2y0(0)\u2212y 0(1))+ x0(0)(\u22122y1(0) +y1(1))) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb ,(A12) A3 = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 u(y1(0)\u2212y 2(0)) +v(\u2212x1(0) +x2(0)) +(\u2212(x2(0)y1(0)) +x1(0)y2(0)) u(\u2212y0(0) +y2(0)) +v(x0(0)\u2212x 2(0)) +(x2(0)y0(0)\u2212x 0(0)y2(0)) u(y0(0)\u2212y 1(0)) +v(\u2212x0(0) +x1(0)) +(\u2212(x1(0)y0(0)) +x0(0)y1(0)) \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb ,(A13) a1 = \u2212(y1(0)x0(1)) +y2(0)x0(1) \u2212y2(0)x1(1) +y0(0)(x1(1)\u2212x 2(1)) +y1(0)x2(1)\u2212x 1(1)y0(1) +x2(1)y0(1) +x0(1)y1(1)\u2212x 2(1)y1(1) +x2(0)(y0(0)\u2212y 1(0)\u2212y 0(1) +y1(1)) +x1(0)(\u2212y0(0) +y2(0) +y0(1)\u2212y 2(1)) \u2212x0(1)y2(1) +x1(1)y2(1) +x0(0)(y1(0)\u2212y 2(0)\u2212y 1(1) +y2(1)), (A14) a2 = y1(0)x0(1)\u2212y 2(0)x0(1) +y2(0)x1(1)\u2212y 1(0)x2(1) +y0(0)(\u2212x1(1) +x2(1)) +x2(0)(\u22122y0(0) + 2y1(0) +y0(1)\u2212y 1(1)) +x0(0)(\u22122y1(0) + 2y2(0) +y1(1)\u2212y 2(1)) +x1(0)(2y0(0)\u22122y 2(0)\u2212y 0(1) +y2(1)), (A15)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S79",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "a3 = x2(0)(y0(0)\u2212y 1(0)) +x0(0)(y1(0)\u2212y 2(0)) +x1(0)(\u2212y0(0) +y2(0)). (A16) Euclidean Distance ApproximationIn this section, we detail the derivation of\u02c6w\u2217. Given a triangleF= \u0002v0 v1 v2 \u0003 = \uf8ee \uf8f0 x0 x1 x2 y0 y1 y2 1 1 1 \uf8f9 \uf8fb and pixel barycentric coordinatesw= \u0002w0 w1 w2 \u0003 , we consider finding\u02c6w\u2217 = \u0002w\u2217 0 w\u2217 1 w\u2217 2 \u0003 such that \u02c6w\u2217 = arg min \u02c6w\u2208[0,1]3 ||Fw\u2212F\u02c6w||2 2 .(A17) If the pixel is inside the triangle, it\u2019s obvious that\u02c6w\u2217 = w, so we only consider the scenario where the pixel is out- side the triangle. First we calculate the pixel positionp= \u0002u v1 \u0003T = Fw. Ifpis outside the triangleF, the closest pointp \u2217 must lie on one of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S80",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "the triangle\u2019s edges. Therefore, we need to compute the closest point frompto each of the 3 edges of the triangle and select the one with the minimum distance. For each edgev ivj, we first compute the parametert such that the projection (closest) pointp \u2032 =v i +t(vj \u2212vi). We have t= (u\u2212x i)(xj \u2212x i) + (v\u2212y i)(yj \u2212y i) (xj \u2212x i)2 + (yj \u2212y i)2 .(A18) If0\u2264t\u22641, the projection point lies on the edge, and the barycentric coordinates ofp \u2032 can be represented asw \u2032 = \u0002 w \u2032 0 w \u2032 1 w \u2032 2 \u0003 , wherew \u2032 i = 1\u2212t \u2032 , w \u2032 j = t,the rest one= 0. Ift <0, thenp \u2032 =v",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S81",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "i,w \u2032 i = 1,the rest= 0. Ift >1, thenp \u2032 =v j,w \u2032 j = 1,the rest= 0. Perform these computations for the three edgesv 0v1, v1v2,v 2v0, then choose thep \u2032 with the smallest distance. Its barycentric coordinatesw \u2032 are the desired solution\u02c6w\u2217. C. Implementation Details In this section, we present implementation details of our",
      "page_hint": null,
      "token_count": 57,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S82",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "follow DIB-R [5] and separately compute the foreground and background pixels. Moreover, in the original Softras implementation, the probability mapA i j is defined as: Aj i = sigmoid \u0012 \u2212d(pi,F j) \u03b4 \u0013 .(A19) We change it to exponential function for smoother gradi- ents [5]: Aj i = exp \u0012 \u2212d(pi,F j) \u03b4 \u0013 .(A20) In addition, we find that enablingAggregate Functionin SoftRas [22] results in a total reconstruction failure in the optimization task, so we disable it in all of our experiments. D. Segmentation Analysis In the main paper, we decompose the entire rotation into 12 segments. In this section, we will illustrate the quality of forward rendered images and backward gradients with respect to the number of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S83",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "segments used.",
      "page_hint": null,
      "token_count": 2,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S84",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "12 segments (e.g., 6 segments) leads to severe artifacts in the forward rendering. Conversely, employing more than 12 segments increases the computational cost significantly, with only marginal improvement in rendering quality. Therefore, as a trade-off between rendering quality and computational efficiency, 12 segments are chosen in our ex- periments. 2 6Segments12Seg.24Seg.48Seg. Forward SoftRas Ours Gradient SoftRas Ours Figure A1. Impact of Segment Count on Rendering and Gradient Quality. Forward rendering and backward gradient visualization demonstrating the effect of different segment counts on motion- blur synthesis. As illustrated, using fewer than 12 segments (e.g., 6 segments) introduces severe artifacts in forward rendering and compromises gradient quality. Increasing the segment count to 12 significantly improves both rendering smoothness and gradient accuracy. E.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S85",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "Visualization Details In this section, we provide detailed explanations of the ren- dering process for the images presented in Fig. 3. All forward images are rendered using the same cam- era parameters and blur settings (i.e., translation or rotation speed) as those used in our main experiments. For transla- tional motion, we do not decompose the motion into seg- ments. For rotational motion, the entire rotation circle is al- ways decomposed into 12 segments. Consequently, for ro- tational motion blurred with a total of 12, 60 or 240 samples, these are respectively distributed as 1, 5 and 20 samples per segment, given our decomposition into 12 segments. For gradient images, we compute gradients with respect to the X-positions of all",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S86",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "vertices. After obtaining these per- vertex gradient scalars, we then render these scalars into single-channel grayscale images, which are subsequently color-mapped using the Viridis color-map for visualization. All images are rendered at a resolution of512\u00d7512pix- els. F. Failures of Mesh in Rotational Optimization In rotational optimization, we observe that directly optimiz- ing mesh vertices fails to recover well-shaped objects. One (a) G.T. (b) Mesh Low Smo.(c) Mesh High Smo. (d) SDF Figure A2. The results of rotational optimization. Mesh repre- sentation fails to recover a well-shaped Spot cow, no matter how smooth it is. Instead, SDF representation recovers a significantly better Spot cow. such failure case is illustrated in Fig. A2. The mesh repre- sentation consistently fails to recover a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S87",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "well-shaped object, even when incorporating smoothing regularization during optimization. In contrast, the results obtained with the SDF representation are substantially superior. G. Details of Loss Terms In this section, we provide detailed definitions of additional loss terms not explicitly covered in our main paper. Laplacian LossOur definition of Laplacian loss follows [22]. For each vertexv, letN(v)be the set of adjacent vertices ofv. The Laplacian loss is then defined as: LL = X v",
      "page_hint": null,
      "token_count": 73,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S88",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "\u03b4v \u2212 1 |N(v)| X v\u2032\u2208N(v) \u03b4v\u2032",
      "page_hint": null,
      "token_count": 7,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S89",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "2 .(A21) where\u03b4 v denotes the predicted movement of vectorv. This Laplacian loss encourages adjacent vertices to move consis- tently, thereby promoting mesh deformation smoothness. Smoothness LossOur definition of Smoothness loss is the same as [22]. For all two neighboring faces sharing the edgee i, let\u03b8 i be the dihedral angle between the two faces. We have Ls = X ei (cos(\u03b8i) + 1)2 .(A22) This smoothness loss encourages adjacent faces to have similar normal directions, thereby penalizing sharp edges. Regularization Loss in FlexiCubesWe incorporate the regularization loss provided in FlexiCubes [35]. It is defined as: Lreg =\u03bb devLdev +\u03bb signLsign.(A23) ForL dev, it is defined as: Ldev = X v\u2208V MAD[{|v\u2212u e|2 :u e \u2208 N(v)}],(A24) 3 whereVdenotes the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S90",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "set of voxel grid vertices,| \u00b7 |2de- notes Euclidean distance, MAD(Y) = 1 |Y| Py\u2208Y|y\u2212 mean(Y)|is the Mean Absolute Deviation, andN(v)de- notes the set of adjacent vertices ofv. This term penalizes the variability of distances between a vertexvand its neigh- borsu e \u2208 N(v). ForL sign, it is defined as: Lsign = X (sa,sb)\u2208Eg H(\u03c3(s a),sign(s b)),(A25) whereE g denotes the set of all edges(a, b)where the scalar function values(s a, sb)at grid verticesa, bhave dif- fering signs (i.e., cross the zero-level set).Hand\u03c3de- note the cross-entropy and sigmoid functions, respectively. This term discourages the appearance of spurious geomet- rical structures or internal cavities in regions where explicit shape supervision is absent. We use the same weight parameters\u03bb dev, \u03bbsign as",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S91",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "speci- fied in [35]. Regularization Loss in Neural-Singular-HessianWe use the regularization loss provided in Neural-Singular- Hessian [41]. It is defined as: Lcrit =\u03bb EikonalLEikonal +\u03bb singularHLsingularH.(A26) The Eikonal lossL Eikonal is defined as: LEikonal = Z P ||(||\u2207f(x)||2 \u22121)|| 1 dx,(A27) wheref(\u00b7)denotes the SDF function andPdenotes the set of sampling points. The Eikonal loss encourages the gradi- ent magnitude of the SDF field to be 1, which is crucial for maintaining global smoothness and a valid SDF property. The singular Hessian lossL singularH is defined as: LsingularH = Z Pnear ||det(H f (x))||1 dx,(A28) wheref(\u00b7)denotes the SDF function,P near denotes the set of sampling points located near the zero-level set (surface), anddet(H f (x))signifies the determinant of the Hessian matrixHf(x).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S92",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "The Hessian matrix is defined as the Jaco- bian of the gradient off: Hf (x) = \uf8ee \uf8f0 fxx(x)f xy(x)f xz(x) fyx(x)f yy(x)f yz(x) fzx(x)f zy(x)f zz(x) \uf8f9 \uf8fb.(A29) We set the initial weighting parameters as\u03bb Eikonal = 50 53 and\u03bb singularH = 3 53 . The same decay policy as described in [41] is adopted. (a) Cylinder (b) Twisted Cylinder (c) Blur Image (d) Ours Result Figure A3. A cylinder and a twisted cylinder. They share a same rotational blurred image. Given (c) as input, our optimization re- sult is (d). H. Analysis of 3D Losses in Rotational Opti- mization In rotational optimization, we did not employ 3D losses for quantitative evaluation of reconstructed object shapes. The rationale behind",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S93",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "this decision is detailed in this section. In the rotational recovery task, it is common for multiple distinct 3D objects to produce rotational motion-blurred im- ages that are indistinguishable from the input blurred image. An illustrative example is provided in Fig. A3, where the rotational motion-blurred images of both objects in Fig. A3 (a, b) result in the same blurred image shown in Fig. A3 (c). As demonstrated in Fig. A3 (a, b), the geometric discrep- ancies among feasible objects can be substantial, and it is unreasonable to designate any one of these feasible objects as the ground truth. Consequently, evaluating the results using 3D losses or static image losses is not suitable. To the best of our knowledge, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S94",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "most effective evalua- tion for this task is to compute the differences between the rotational motion-blurred images, which is adopted in our main paper. I. Scene Settings In this section, we detail the specific configurations for our scenes, covering object initialization, motion parameters, and camera extrinsic and intrinsic properties. I.1. Object Initialization All 3D objects utilized in our experiments (e.g., for gradient visualization and optimization evaluation) undergo a two- step initialization process. First, Each object is uniformly scaled such that the maximum Euclidean norm of any ver- tex does not exceed 1. Subsequently, each object is rotated around its local X-axis by a random angle uniformly sam- pled from the range[\u221290 \u25e6,90 \u25e6]. I.2. Motion Parameters TranslationFor all translational motion,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S95",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "objects undergo a linear translation along the X-axis. The positionP(t) = (x(t), y(t), z(t))of a vertex that was initially atP 0 = 4 (x0, y0, z0)is defined by: \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 x(t) =x 0 + (0.5\u2212t) y(t) =y 0 z(t) =z 0 fort\u2208[0,1](A30) This leads to the object translating linearly from an X- coordinate ofx 0 + 0.5att= 0tox 0 \u22120.5att= 1. RotationFor rotational motion, objects are rotated around the Y-axis. The angular displacement is\u03b8(t) = 2\u03c0t, wheret\u2208[0,1]. The positionP(t) = (x(t), y(t), z(t))of a vertex that was initially atP 0 = (x0, y0, z0)is defined by: P(t) =R y(2\u03c0t)P0,(A31) whereR y(\u03b8)is the 3D rotation matrix around the Y-axis by an angle\u03b8: Ry(\u03b8) = \uf8eb \uf8ed cos\u03b80\u2212sin\u03b8 0 1 0",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S96",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "sin\u03b80 cos\u03b8 \uf8f6 \uf8f8 (A32) I.3. Camera Following SoftRas, our camera setup employs a stan- dard perspective model. The camera\u2019s eye pointE= (Ex, Ey, Ez), from which it observes the scene origin (0,0,0), is defined by spherical coordinates: a radial dis- tanced, an elevation angle\u03d5, and an azimuth angle\u03b8. The conversion to Cartesian coordinates is given by: \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 Ex =dcos(\u03d5) cos(\u03b8) Ey =dcos(\u03d5) sin(\u03b8) Ez =dsin(\u03d5) (A33) For all experiments,d= 2.232, and\u03d5\u2208 {\u221260 \u25e6, \u221230\u25e6,0 \u25e6,30 \u25e6,60 \u25e6}. The azimuth angle\u03b8varies with the motion type: (1) For translational motion,\u03b8\u2208 {\u2212315\u25e6,\u2212270 \u25e6,\u2212225 \u25e6 ,\u2212180 \u25e6,\u2212135 \u25e6,\u221290 \u25e6,\u221245 \u25e6,0 \u25e6}; (2) For rotational motion,\u03b8= 0 \u25e6. The camera\u2019s intrinsic parameters define a perspective projection with a fixed half-angular field of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S97",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "view\u03b1= 30 \u25e6. A 3D pointP= (x, y, z)in camera coordinates is projected to an image pointP p = (xp, yp)as: xp = x z\u00b7tan(\u03b1) andy p = y z\u00b7tan(\u03b1) (A34) J. Hyperparameter Settings In this section, we detail the hyperparameter settings used in our experiments. J.1. Overall Settings Following [22], we set\u03b4= 1\u00d710 \u22124 in the probability function. Unless otherwise stated, we randomly select 25 objects from ShapeNet [3] for evaluation. The ADAM op- timizer [16] is employed for optimization. Each image is rendered at a resolution of128\u00d7128pixels. All experi- ments are conducted on a single NVIDIA RTX 4090 GPU with 24GB of memory. J.2. Translational Optimization In this experiment, each object is rendered from 40 differ- ent viewpoints.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S98",
      "paper_id": "arxiv:2602.07860v1",
      "section": "results",
      "text": "We set\u03bb S = 3\u00d710 \u22122,\u03bb L = 3\u00d710 \u22124, and\u03b1= 0.01,\u03b2 1 = 0.5,\u03b2 2 = 0.99(following [22]) for the ADAM optimizer. The batch size for input views is set to 16, and each object is optimized for 1000 iterations. A sphere consisting of 1352 vertices and 2700 faces is uti- lized as a template mesh for deformation. We use the same",
      "page_hint": null,
      "token_count": 63,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S99",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "turing. J.3. Rotational Optimization In this experiment, each object is rendered from 5 view- points with varying elevations. The ADAM optimizer is configured with\u03b1= 5\u00d710 \u22124,\u03b2 1 = 0.9, and\u03b2 2 = 0.999. The batch size for input views is set to 5, and each ob- ject is optimized for 1000 iterations. We set the initial \u03bbcrit = 3\u00d710 \u22123, \u03bbreg = 1. The voxel-grid resolution in FlexiCubes [35] is set to 32. In our approach, we decom- pose the entire rotation into 12 segments, all of which are uniformly sampled. We first pretrain the SDF field on an inclined ellipsoid (defined by4x 2 + 2.5y2 + 2.5z2 \u22123yz= 1) for 500 it- erations, followed by an additional 1000 iterations",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S100",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "of opti- mization. K. Further Analysis of Baselines K.1. Shape From Blur [33] In this section, we provide more details and analysis of our comparative experiments against Shape from Blur (SFB) [33]. Experimental Setup Adaptations for ComparisonThe problem formulation and experimental setup of SFB differ from our inverse rendering approach. SFB is designed to recover 3D shape and motion parameters directly from a single RGB blurred image, leveraging a pre-trained neural network (DeFMO, [32]) for intermediate guidance. Specif- ically, SFB takes a single RGB image and an RGB back- ground as input, and does not require or utilize explicit ob- ject motion information (e.g., translation or rotation veloci- ties). Its core optimization loop involves: 5 1. Using DeFMO to predict",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S101",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "instance-level static masks (sil- houettes) for multiple intermediate timestamps from the input. These masks represent the underlying static ap- pearance of the object at various points along its motion path. 2. Optimizing for mesh deformation (starting from a tem- plate mesh) and motion parameters (translationt,\u2206t, rotationr,\u2206r) through a single-view, differentiable ren- dering pipeline. 3. In each optimization iteration, it renders RGB images and silhouette masks for multiple timestamps. 4. The rendered silhouettes are compared against the static masks predicted by DeFMO. 5. All rendered RGB images, masked by their silhouettes and composited with the input background, are averaged to form a synthetic motion-blurred image. This image is then compared against the input RGB image. These losses drive the backward propagation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S102",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "and optimization. In contrast, our method operates on multi-view RGB images with their corresponding non-binary transparency masks (alpha channels). In addition, our method requires and utilizes object motion information (e.g., trajectory, veloci- ties) as input. During optimization, we render multi-view motion-blurred RGBA images by accumulating contribu- tions from the object along its known motion path, which are then compared against the input images for gradient computation. Despite these fundamental differences, SFB remains the most relevant benchmark due to the severe scarcity of al- ternative methods tackling 3D shape recovery from motion blur. To enable a best-effort comparison, we adapted our data for SFB. Specifically, for each input to SFB, we gener- ate a single RGB image by masking our RGB",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S103",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "images with corresponding transparency masks and compositing them onto a plain black background, to minimize the influence of the background to the greatest extent possible. This ensures SFB receives input that best aligns with its expected format (RGB image + background) while making our data com- patible. We kept SFB\u2019s camera parameters consistent with those used in our setup and made no other modifications to SFB\u2019s internal configurations or parameters, aiming for the most straightforward comparison. Why SFB Performs Not So Well in These Extreme Motion ScenariosAs demonstrated in the main paper (Tab. 1), our method significantly outperforms SFB for ultra-fast motion blur reconstruction. This disparity, partic- ularly in extreme motion scenarios, primarily stems from a limitation in SFB\u2019s pipeline:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S104",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "its heavy reliance on the DeFMO [32] neural network for deriving intermediate static masks. DeFMO, while generally effective for typical fast mo- tion blur scenarios, fails when confronted with the highly diffused and ambiguous observations generated by ultra- fast motion. In such extreme cases, DeFMO struggles to accurately predict the static masks at timestamps along the motion path. As illustrated in Fig. A4, the masks produced by DeFMO for our ultra-fast motion blurred images are of- ten highly inaccurate and entirely non-representative of the underlying object\u2019s true silhouette. Since the DeFMO-predicted static masks serve as a fun- damental guidance signal for SFB\u2019s shape and motion re- covery, their inaccuracy directly propagates through the en- tire pipeline. This makes SFB ineffective",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S105",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "for the ultra-fast motion blur reconstruction challenge, despite any richness in the input image data provided. However, we acknowledge that SFB is a pioneering and important work that significantly advances the field of shape-from-blur by introducing a novel, learning-assisted",
      "page_hint": null,
      "token_count": 39,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S106",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "analysis of its limitations merely highlights the unique dif- ficulties posed by extreme motion blur. While our method demonstrates superior performance in this specific setting, the requirement for input transparency masks will be a lim- itation. We believe that addressing the challenges of ex- treme motion blur, particularly managing the ambiguity without explicit transparency, presents a significant and fer- tile ground for future research. K.2. Analysis of Nvdiffrast\u2019s Gradient Computa- tion In the main paper (Section 6.4), we demonstrated Nvd- iffrast\u2019s limited performance in reconstructing shapes from extreme motion blur. This might stem from a fundamental difference in how geometry gradients are computed. Nvdiffrast primarily derives geometry gradients from localized, pixel-wise anti-aliasing signals along triangle edges. This means a vertex\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S107",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "influence on the gradient is concentrated on a few pixels it directly affects. While effi- cient for rendering, these localized gradients are insufficient for optimizing shape deformations from highly ambiguous, severely blurred input images. It leads to slow convergence or catastrophic failures due to a lack of meaningful gradient signals. In contrast, our method, built on from SoftRas [22], en- ables each vertex to influence many pixels across a broader image region, effectively generating global and smoothed gradients. Such gradients provide a more stable signal for shape optimization. This fundamental difference in gradi- ent computation contributes to robust 3D shape recovery in our challenging scenarios. L. More Results In this section, we present additional experimental results. 6 [32] RGB [32]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S108",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "Sil. Input G.T. [32] RGB [32] Sil. Input G.T. [32] RGB [32] Sil. Input G.T. Figure A4.Failures of DeFMO [32] in Extreme Motion Blur.Each group displays:Left.Input motion-blurred image.Right.Three rows presenting results from DeFMO:Top Row.RGB images predicted by DeFMO at various timestamps.Middle Row.Corresponding static masks (silhouettes) predicted by DeFMO.Bottom Row.Ground Truth (G.T.) static masks at the respective timestamps. As illustrated, DeFMO [32] fails to predict accurate static masks under these extreme motion conditions. This fundamental inaccuracy in DeFMO\u2019s prior critically undermines the optimization guidance for SFB [33], ultimately leading to its reconstruction failures in the challenging scenarios. L.1. Parabolic Recovery We provide an evaluation on a more complex motion type: combined translational and rotational motion along a parabolic trajectory. In this",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S109",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "experiment, each vertexP 0 = (x 0, y0, z0)un- dergoes a two-step transformation to define its motion path over timet\u2208[0,1]. The vertex is first rotated around the Y-axis by an angle\u03b8(t) =\u03c0t, wheret\u2208 [0,1]. The intermediate rotated positionP rot(t)is given byP rot(t) =Ry(\u03c0t)P 0. Specifically, forP rot(t) = 7 (a) Illustration (b) Evaluation Figure A5. Shape recovery for complex motion trajectories (parabolic translation + rotation). Labels indicate the correspond- ing number of samples. We achieve better performance than Soft- Ras. (xrot(t), yrot(t), zrot(t)): \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 xrot(t) =x 0 cos(\u03c0t) +z 0 sin(\u03c0t) yrot(t) =y 0 zrot(t) =\u2212x 0 sin(\u03c0t) +z 0 cos(\u03c0t). (A35) Subsequently, a translation vectorT(t) = (Tx(t), Ty(t), Tz(t))is applied to the rotated position Prot(t). Lets(t) =",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S110",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "0.5\u2212t. The components of this translation vector are: \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 Tx(t) =s(t) Ty(t) =\u22124s(t) 2 + 0.5 Tz(t) =s(t). (A36) The final positionP(t) = (x(t), y(t), z(t))at timetis then P(t) =P rot(t) +T(t). Specifically: \uf8f1 \uf8f4\uf8f2 \uf8f4\uf8f3 x(t) =x rot(t) + (0.5\u2212t) y(t) =y rot(t) + (\u22124(0.5\u2212t) 2 + 0.5) z(t) =z rot(t) + (0.5\u2212t). (A37) Illustration and evaluation results are shown in Fig. A5. L.2. Accelerating Existing Pipelines We further demonstrate the potential of our method as an accelerator for existing optimization-based inverse render- ing pipelines. We integrate our method into [33], replac- ing its original rendering component. We evaluate its per- formance by comparing total optimization time and recon- struction quality (TIoU, PSNR, SSIM) against the original",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S111",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "[33]. Quantitative comparison results are presented in Tab. A1. Our integration reduces the optimization time while main- taining comparable reconstruction quality. These results demonstrate our method\u2019s effectiveness in accelerating ex- isting inverse rendering pipelines, thereby enabling them to tackle complex, real-world motion blur scenarios with greater efficiency. Moreover, these results also demonstrate that our method can leverage existing pipelines (e.g., [33]) to handle diverse real-world scenarios.",
      "page_hint": null,
      "token_count": 66,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S112",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "[33] 0.678 26.133 0.736 60.663 [33] + Ours 0.678 26.010 0.731 47.227 Table A1. Evaluation on the FMO real-world benchmark. Note time contains both rendering and data processing steps. Our solver can be integrated into [33]\u2019s pipeline, providing faster optimiza- tion with comparable performance. \u201c+ Ours\u201d denotes replacing the Kaolin DIB-R rasterizer with ours but retaining the texture map- ping module. Since the time cost for per template mesh remains similar, as reported in [33], we follow the best settings but use the V oronoi sphere as the template mesh only, and split the trajectory into 8 segments in our method. We have tried our best to make re- production (the top row) but small discrepancy in performance still exists,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S113",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "which might impact little on our time-oriented evaluation. Results show that with the complement of out method (the bottom row), a speedup can be achieved without significant losses of per- formance. In addition, Kaolin DIB-R is a highly-optimized CUDA renderer, while our method is lack of low level CUDA optimiza- tion. We believe that with more such optimization, our method can achieve a more significant acceleration. M. Detailed Limitations and Future Work In this section, we provide a detailed discussion on the lim- itations of our method and potential directions for future research. Dependency on Known Motion and PosesSimilar to many inverse rendering approaches, our current optimiza- tion pipeline requires known camera intrinsics, poses, and motion information. In unconstrained settings,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S114",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "obtaining these parameters can be challenging. A promising direc- tion is to integrate our differentiable renderer with motion estimation modules (e.g., [33]) to jointly estimate motion trajectories and shape from the input image. We present a preliminary trial of this integration in Sec. L.2. Motion Linearity AssumptionOur fast barycentric solver assumes that motion within each time segment is linear. While this approximation holds for short exposure times, highly complex non-linear motions may introduce er- rors. Addressing this would require modeling higher-order motion trajectories or employing finer temporal segmenta- tion, which we leave for future optimization. Photometric AssumptionsOur rendering model as- sumes a linear photometric relationship between the scene 8 radiance and pixel intensity. However, real-world camera ISPs (Image Signal Processors)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2602_07860v1:S115",
      "paper_id": "arxiv:2602.07860v1",
      "section": "method",
      "text": "typically apply non-linear tone mapping curves (e.g., Gamma correction) to compress high dynamic range data for display. Furthermore, high- speed photography often necessitates high ISO settings to compensate for short exposure times (if not blurring in- tentionally), or operates in low-light conditions where the signal-to-noise ratio is low. Our current model does not explicitly account for non-linear camera response func- tions or sensor noise. Future work could incorporate learn- able camera response functions (CRFs) and noise model- ing to enhance reconstruction robustness in raw, in-the-wild footage. 9",
      "page_hint": null,
      "token_count": 87,
      "paper_year": 2026,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9360862688350277,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 19,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3987,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5221,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4375,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4663,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4425,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4087,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3251,
        "empty": false
      },
      {
        "page": 8,
        "chars": 4504,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5771,
        "empty": false
      },
      {
        "page": 10,
        "chars": 4091,
        "empty": false
      },
      {
        "page": 11,
        "chars": 2685,
        "empty": false
      },
      {
        "page": 12,
        "chars": 3862,
        "empty": false
      },
      {
        "page": 13,
        "chars": 3341,
        "empty": false
      },
      {
        "page": 14,
        "chars": 3991,
        "empty": false
      },
      {
        "page": 15,
        "chars": 3828,
        "empty": false
      },
      {
        "page": 16,
        "chars": 5290,
        "empty": false
      },
      {
        "page": 17,
        "chars": 1213,
        "empty": false
      },
      {
        "page": 18,
        "chars": 4117,
        "empty": false
      },
      {
        "page": 19,
        "chars": 702,
        "empty": false
      }
    ],
    "quality_score": 0.9361,
    "quality_band": "good"
  }
}