{
  "paper": {
    "paper_id": "arxiv:2012.07810v1",
    "title": "Real-Time High-Resolution Background Matting",
    "authors": [
      "Shanchuan Lin",
      "Andrey Ryabtsev",
      "Soumyadip Sengupta",
      "Brian Curless",
      "Steve Seitz",
      "Ira Kemelmacher-Shlizerman"
    ],
    "year": 2020,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "We introduce a real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our technique is based on background matting, where an additional frame of the background is captured and used in recovering the alpha matte and the foreground layer. The main challenge is to compute a high-quality alpha matte, preserving strand-level hair details, while processing high-resolution images in real-time. To achieve this goal, we employ two neural networks; a base network computes a low-resolution result which is refined by a second network operating at high-resolution on selective patches. We introduce two largescale video and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our approach yields higher quality results compared to the previous state-of-the-art in background matting, while simultaneously yielding a dramatic boost in both speed and resolution.",
    "pdf_path": "data/automation/papers/arxiv_2012.07810v1.pdf",
    "url": "https://arxiv.org/pdf/2012.07810v1",
    "doi": null,
    "arxiv_id": "2012.07810v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.074775+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2012_07810v1:S1",
      "paper_id": "arxiv:2012.07810v1",
      "section": "body",
      "text": "Real-Time High-Resolution Background Matting Shanchuan Lin* Andrey Ryabtsev* Soumyadip Sengupta Brian Curless Steve Seitz Ira Kemelmacher-Shlizerman University of Washington {linsh,ryabtsev,soumya91,curless,seitz,kemelmi}@cs.washington.edu Zoom input and background shot Zoom with new background Our Zoom plugin with new background Figure 1: Current video conferencing tools like Zoom can take an input feed (left) and replace the background, often introducing artifacts, as shown in the center result with close-ups of hair and glasses that still have the residual of the original background. Leveraging a frame of video without the subject (far left inset), our method produces real-time, high-resolution background matting without those common artifacts. The image on the right is our result with the corresponding close-ups, screenshot from our Zoom plugin implementation.",
      "page_hint": null,
      "token_count": 117,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S2",
      "paper_id": "arxiv:2012.07810v1",
      "section": "abstract",
      "text": "We introduce a real-time, high-resolution background re- placement technique which operates at 30fps in 4K resolu- tion, and 60fps for HD on a modern GPU. Our technique is based on background matting, where an additional frame of the background is captured and used in recovering the al- pha matte and the foreground layer. The main challenge is to compute a high-quality alpha matte, preserving strand- level hair details, while processing high-resolution images in real-time. To achieve this goal, we employ two neural networks; a base network computes a low-resolution result which is re\ufb01ned by a second network operating at high- resolution on selective patches. We introduce two large- scale video and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our approach yields",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S3",
      "paper_id": "arxiv:2012.07810v1",
      "section": "abstract",
      "text": "higher qual- ity results compared to the previous state-of-the-art in back- ground matting, while simultaneously yielding a dramatic boost in both speed and resolution. Our code and data is available at https://grail.cs.washington.edu/ projects/background-matting-v2/ 1. Introduction",
      "page_hint": null,
      "token_count": 35,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S4",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "effects, now enjoys wide-spread use in video conferencing tools like Zoom, Google Meet, and Microsoft Teams. In ad- dition to adding entertainment value, background replace- *Equal contribution. ment can enhance privacy, particularly in situations where a user may not want to share details of their location and environment to others on the call. A key challenge of this video conferencing application is that users do not typically have access to a green screen or other physical props used to facilitate background replacement in movie special effects. While many tools now provide background replacement functionality, they yield artifacts at boundaries, particu- larly in areas where there is \ufb01ne detail like hair or glasses (Figure 1). In contrast, traditional image matting methods",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S5",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "[6, 16, 17, 30, 9, 2, 7] provide much higher quality re- sults, but do not run in real-time, at high resolution, and frequently require manual input. In this paper, we intro- duce the \ufb01rst fully-automated, real-time, high-resolution matting technique, producing state-of-the-art results at 4K (3840\u00d72160) at 30fps and HD (1920\u00d71080) at 60fps. Our",
      "page_hint": null,
      "token_count": 54,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S6",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "compute the alpha matte and the foreground layer, an ap- proach known as background matting. Designing a neural network that can achieve real- time matting on high-resolution videos of people is ex- tremely challenging, especially when \ufb01ne-grained details like strands of hair are important; in contrast, the previous state-of-the-art method [28] is limited to 512 \u00d7512 at 8fps. Training a deep network on such a large resolution is ex- tremely slow and memory intensive. It also requires large volumes of images with high-quality alpha mattes to gener- alize; the publicly available datasets [33, 25] are too limited. 1 arXiv:2012.07810v1 [cs.CV] 14 Dec 2020 Since it is dif\ufb01cult to collect a high-quality dataset with manually curated alpha mattes in large quantities,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S7",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "we pro- pose to train our network with a series of datasets, each with different characteristics. To this end, we introduce Video- Matte240K and PhotoMatte13K/85 with high-resolution al- pha mattes and foreground layers extracted with chroma- key software. We \ufb01rst train our network on these larger databases of alpha mattes with signi\ufb01cant diversity in hu- man poses to learn robust priors. We then train on publicly available datasets [33, 25] that are manually curated to learn \ufb01ne-grained details. To design a network that can handle high-resolution im- ages in real-time, we observe that relatively few regions in the image require \ufb01ne-grained re\ufb01nement. Therefore, we introduce a base network that predicts the alpha matte and foreground layer at lower resolution along",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S8",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "with an error prediction map which speci\ufb01es areas that may need high- resolution re\ufb01nement. A re\ufb01nement network then takes the low-resolution result and the original image to generate high-resolution output only at select regions. We produce state-of-the-art background matting results in real-time on challenging real-world videos and images of people. We will release our VideoMatte240K and Pho- toMatte85 datasets and our model implementation. 2. Related Work",
      "page_hint": null,
      "token_count": 66,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S9",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "tation or matting. While binary segmentation is fast and ef\ufb01cient, the resulting composites have objectionable arti- facts. Alpha matting can produce visually pleasing compos- ites but often requires either manual annotations or a known",
      "page_hint": null,
      "token_count": 34,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S10",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "that perform background replacement with segmentation or matting. Segmentation. The literature in both instance and se- mantic segmentation is vast and out of scope for this paper, so we will review the most relevant works. Mask RCNN [11] is still a top choice for instance segmentation while DeepLabV3+ [5] is a state-of-the-art semantic segmentation network. We incorporate the Atrous Spatial Pyramid Pool- ing (ASPP) module from DeepLabV3 [4] and DeepLabV3+ within our network. Since segmentation algorithms tend to produce coarse boundaries especially at higher resolutions, Kirillov et al. presented PointRend [15] which samples points near the boundary and iteratively re\ufb01nes the segmen- tation. This produces high-quality segmentation for large image resolutions with signi\ufb01cantly cheaper memory and computation. Our method adopts",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S11",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "this idea to the matting domain via learned re\ufb01nement-region selection and a con- volutional re\ufb01nement architecture that improves the recep- tive \ufb01eld. Speci\ufb01c applications of human segmentation and parsing have also received considerable attention in recent works [34, 19]. Trimap-based matting. Traditional (non-learning based) matting algorithms [6, 16, 17, 30, 9, 2, 7] require manual annotation (a trimap) and solve for the alpha matte in the \u2018unknown\u2019 region of the trimap. Different matting techniques are reviewed in the survey by Wang and Co- hen [32]. Xu et al. [33] introduced a matting dataset and used a deep network with a trimap input to predict the alpha matte. Many recent approaches rely on this dataset to learn matting, e.g., Context-Aware Matting",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S12",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "[13], Index Matting [21], sampling-based matting [31] and opacity propagation- based matting [18]. Although the performance of these",
      "page_hint": null,
      "token_count": 18,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S13",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "recent methods consider coarse [20] or faulty human anno- tations [3] to predict the alpha matte. Matting without any external input. Recent ap- proaches have also focused on matting humans without any external input. Portrait matting without a trimap [36, 29] is one of the more successful applications due to less variabil- ity among portrait images compared to full body humans. Soft segmentation for natural images had also been explored in [1]. Recent approaches like Late Fusion Matting [35] and HAttMatting [25] aim to solve for the alpha matte directly from the image, but these approaches can often fail to gen- eralize as shown in [28]. Matting with a known natural background. Matting with known natural background had been previously",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S14",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "ex- plored in [24], Bayesian matting [7] and Poisson matting [30, 10] which also requires a trimap. Recently Sengupta et al. [28] introduced Background Matting (BGM) where an additional background image is captured and it provides a signi\ufb01cant cue to predict the alpha matte and the foreground layer. Although this method showed high-quality matting",
      "page_hint": null,
      "token_count": 54,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S15",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "and runs only at 8fps. In contrast, we introduce a real-time uni\ufb01ed matting architecture that operates on 4K videos at 30fps and HD videos at 60fps, and produces higher quality results than BGM. 3. Our Dataset Since it is extremely dif\ufb01cult to obtain a large-scale, high-resolution, high-quality matting dataset where the al- pha mattes are cleaned by human artists, we rely on multiple datasets including our own collections and publicly avail- able datasets. Publicly available datasets. The Adobe Image Matting (AIM) dataset [33] provides 269 human training samples and 11 test samples, averaging around 1000 \u00d71000 resolu- tion. We also use a humans-only subset of Distinctions- 646 [25] consisting of 362 training and 11 test samples, averaging around 1700\u00d72000 resolution.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S16",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "The mattes were created manually and are thus high-quality. However 631 training images are not enough to learn large variations in human poses and \ufb01ner details at high resolution, so we in- 2 (a) VideoMatte240K (b) PhotoMatte13K/85 Figure 2: We introduce two large-scale matting datasets containing 240k unique frames and 13k unique photos. troduce 2 additional datasets. VideoMatte240K. We collect 484 high-resolution green screen videos and generate a total of 240,709 unique frames of alpha mattes and foregrounds with chroma-key soft- ware Adobe After Effects. The videos are purchased as stock footage or found as royalty-free materials online. 384 videos are at 4K resolution and 100 are in HD. We split the videos by 479 : 5 to form the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S17",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "train and validation sets. The dataset consists of a vast amount of human subjects, cloth- ing, and poses that are helpful for training robust models. We are releasing the extracted alpha mattes and foregrounds as a dataset to the public. To our knowledge, our dataset is larger than all existing matting datasets publicly available by far, and it is the \ufb01rst public video matting dataset that contains continuous sequences of frames instead of still im- ages, which can be used in future research to develop mod- els that incorporate motion information. PhotoMatte13K/85. We acquired a collection of 13,665 images shot with studio-quality lighting and cameras in front of a green-screen, along with mattes extracted via chroma-key algorithms with manual tuning",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S18",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "and error re- pair. We split the images by 13,165 : 500 to form the train and validation sets. These mattes contain a narrow range of poses but are high resolution, averaging around 2000\u00d72500, and include details such as individual strands of hair. We refer to this dataset as PhotoMatte13K. However privacy and licensing issues prevent us from sharing this set; thus, we also collected an additional set of 85 mattes of sim- ilar quality for use as a test set, which we are releasing to the public as PhotoMatte85. In Figure 2 we show examples from the VideoMatte240K and PhotoMatte13K/85 datasets. We crawl 8861 high-resolution background images from Flickr and Google and split them by 8636 : 200 :",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S19",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "25 to use when constructing the train, validation, and test sets. We will release the test set in which all images have a CC license (see appendix for details). 4. Our Approach Given an image I and the captured background B we predict the alpha matte \u03b1 and the foreground F, which will allow us to composite over any new background by I\u2032= \u03b1F+(1 \u2212\u03b1)B\u2032, where B\u2032is the new background. In- stead of solving for the foreground directly, we solve for foreground residual FR = F \u2212I. Then, F can be re- covered by adding FR to the input image I with suitable clamping: F = max(min(FR + I,1),0). We \ufb01nd this for- mulation improves learning, and allows us to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S20",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "apply a low- resolution foreground residual onto a high-resolution input image through upsampling, aiding our architecture as de- scribed later. Matting at high resolution is challenging, as applying a deep network directly incurs impractical computation and memory consumption. As Figure 4 shows, human mattes are usually very sparse, where large areas of pixels belong to either background ( \u03b1 = 0) or foreground ( \u03b1 = 1), and only a few areas involve \ufb01ner details, e.g., around the hair, glasses, and person\u2019s outline. Thus instead of designing one network that operates on high-resolution images, we intro- duce two networks; one operates at lower-resolution and an- other only operates on selected patches at the original reso- lution based on the prediction",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S21",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "of the previous network. The architecture consists of a base network Gbase and a re\ufb01nement network Gre\ufb01ne . Given the original image I and the captured backgroundB, we \ufb01rst downsample by a factor of cto Ic and Bc. The base network Gbase takes Ic and Bc as input and predicts coarse-grained alpha matte \u03b1c, fore- ground residual FR c , an error prediction mapEc, and hidden features Hc. Then, the re\ufb01nement network Gre\ufb01ne employs Hc, I, and Bto re\ufb01ne \u03b1c and FR c only in regions where the predicted error Ec is large, and produces alpha \u03b1and fore- ground residual FR at the original resolution. Our model is fully-convolutional and is trained to work on arbitrary sizes and aspect ratios.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S22",
      "paper_id": "arxiv:2012.07810v1",
      "section": "results",
      "text": "4.1. Base Network The base network is a fully-convolutional encoder- decoder network inspired by the DeepLabV3 [4] and DeepLabV3+ [5] architectures, which achieved state-of- the-art performance on semantic segmentation tasks in 2017 and 2018. Our base network consists of three modules: Backbone, ASPP, and Decoder. We adopt ResNet-50 [12] for our encoder backbone, which can be replaced by ResNet-101 and MobileNetV2 [27] to trade-off between speed and quality. We adopt the 3 Backbone Base Re\ufb01ner Input 6 \u2715 H \u2715 W Input Downsampled 6 \u2715 H/c \u2715 W/c Coarse Output 1+3+1+32 \u2715 H/c \u2715 W/c ASPP Decoder Coarse Results Upsampled Concat with Input Downsampled 1+3+32+6 \u2715 H/2 \u2715 W/2 Re\ufb01ned Output 1+3 \u2715 H \u2715 W Skip Connections Bilinear Upsampling",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S23",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Source Foreground Residual Alpha Alpha Foreground Residual Error Map Hidden Crop Patch 8 \u2715  8 Replace Patch 4\u00a0\u2715  4 Alpha Patch Foreground Residual Patch Coarse Output Re\ufb01ned OutputInput Foreground Composite Composition Fgr Residual + Source Alpha\u00a0\u2715  Foreground",
      "page_hint": null,
      "token_count": 39,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S24",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Figure 3: The base network Gbase (blue) operates on the downsampled input to produce coarse-grained results and an error prediction map. The re\ufb01nement network Gre\ufb01ne (green) selects error-prone patches and re\ufb01nes them to the full resolution. (a) Coarse (b) Selection (c) Re\ufb01ned Figure 4: We only re\ufb01ne on error-prone regions (b) and directly upsample the rest to save computation. ASPP (Atrous Spatial Pyramid Pooling) module after the backbone following the DeepLabV3 approach. The ASPP module consists of multiple dilated convolution \ufb01lters with different dilation rates of 3,6 and 9. Our decoder network applies bilinear upsampling at each step, concatenated with the skip connection from the backbone, and followed by a 3\u00d73 convolution, Batch Normalization [14], and ReLU ac- tivation [22]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S25",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "(except the last layer). The decoder network outputs coarse-grained alpha matte \u03b1c , foreground residual FR c , error prediction map Ec and a 32-channel hidden fea- tures Hc . The hidden features Hc contain global contexts that will be useful for the re\ufb01nement network. 4.2. Re\ufb01nement Network The goal of the re\ufb01nement network is to reduce redun- dant computation and recover high-resolution matting de- tails. While the base network operates on the whole im- age, the re\ufb01nement network operates only on patches se- lected based on the error prediction map Ec . We perform a two-stage re\ufb01nement, \ufb01rst at 1 2 of the original resolution and then at the full resolution. During inference, we re\ufb01ne k patches, with k",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S26",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "either set in advance or set based on a threshold that trades off between quality and computation time. Given the coarse error prediction mapEc at 1 c of the orig- inal resolution, we \ufb01rst resample it to 1 4 of the original res- olution E4 , s.t. each pixel on the map corresponds to a 4\u00d74 patch on the original resolution. We select the top k pix- els with the highest predicted error from E4 to denote the k 4\u00d74 patch locations that will be re\ufb01ned by our re\ufb01nement module. The total number of re\ufb01ned pixels at the original resolution is 16 k. We perform a two-stage re\ufb01nement process. First, we bilinearly resample the coarse outputs, i.e., alpha matte \u03b1c",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S27",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": ", foreground residual FR c and hidden features Hc , as well as the input image I and background B to 1 2 of the original resolution and concatenate them as features. Then we crop out 8\u00d78 patches around the error locations selected from E4 , and pass each through two layers of 3 \u00d73 convolution with valid padding, Batch Normalization, and ReLU, which reduce the patch dimension to 4\u00d74. These intermediate fea- tures are then upsampled to 8\u00d78again and concatenated with the 8\u00d78 patches extracted from the original-resolution input I and background B at the corresponding location. We then apply an additional two layers of 3 \u00d73 convolu- tion with valid padding, Batch Normalization and ReLU (except the last",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S28",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "layer) to obtain 4 \u00d74 alpha matte and fore- ground residuals results. Finally, we upsample the coarse alpha matte \u03b1c and foreground residual FR c to the origi- nal resolution and swap in the respective 4 \u00d74 patches that have been re\ufb01ned to obtain the \ufb01nal alpha matte \u03b1and fore- ground residual FR . The entire architecture is illustrated in Figure 3. See appendix for the details of implementation. 4.3. Training All matting datasets provide an alpha matte and a foreground layer, which we compose onto multiple high- resolution backgrounds. We employ multiple data augmen- tation techniques to avoid over\ufb01tting and help the model generalize to challenging real-world situations. We apply af\ufb01ne transformation, horizontal \ufb02ipping, brightness, hue, and saturation adjustment,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S29",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "blurring, sharpening, and ran- dom noise as data augmentation to both the foreground and",
      "page_hint": null,
      "token_count": 14,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S30",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "the background to simulate misalignment and create arti- \ufb01cial shadows to simulate how the presence of a subject can cast shadows in real-life environments (see appendix for more details). We randomly crop the images in every minibatch so that the height and width are each uniformly distributed between 1024 and 2048 to support inference at any resolution and aspect ratio. 4 To learn \u03b1w.r.t. ground-truth \u03b1\u2217, we use an L1 loss over the whole alpha matte and its (Sobel) gradient: L\u03b1 = ||\u03b1\u2212\u03b1\u2217||1 + ||\u2207\u03b1\u2212\u2207\u03b1\u2217||1. (1) We obtain the foreground layer from predicted fore- ground residual FR, using F = max(min(FR + I,1),0). We compute L1 loss only on the pixels where \u03b1\u2217>0: LF = ||(\u03b1\u2217>0) \u2217(F \u2212F\u2217))||1. (2) where",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S31",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "that (\u03b1\u2217>0) is a Boolean expression. For re\ufb01nement region selection, we de\ufb01ne the ground truth error map as E\u2217 = |\u03b1 \u2212\u03b1\u2217|. We then compute mean squared error between the predicted error map and the ground truth error map as the loss: LE = ||E\u2212E\u2217||2. (3) This loss encourages the predicted error map to have larger values where the difference between the predicted alpha and the ground-truth alpha is large. The ground-truth error map changes over iterations during training as the predicted al- pha improves. Over time, the error map converges and pre- dicts high error in complex regions, e.g. hair, that would lead to poor composites if simply upsampled. The base network (\u03b1c,FR c ,Ec,Hc) =Gbase(Ic,Bc) op- erates at",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S32",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "1 c of the original image resolution, and is trained with the following loss function: Lbase = L\u03b1c + LFc + LEc . (4) The re\ufb01nement network (\u03b1,FR) = Gre\ufb01ne (\u03b1c,FR c ,Ec,Hc,I,B ) is trained using: Lre\ufb01ne = L\u03b1 + LF. (5) We initialize our model\u2019s backbone and ASPP module with DeepLabV3 weights pre-trained for semantic segmen- tation on ImageNet and Pascal VOC datasets. We \ufb01rst train the base network till convergence and then add the re\ufb01ne- ment module and train it jointly. We use Adam optimizer and c = 4, k = 5,000 during all the training. For training only the base network, we use batch size 8 and learning rate [1e-4, 5e-4, 5e-4] for backbone, ASPP, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S33",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "decoder. When training jointly, we use batch size 4 and learning rate [5e-5, 5e-5, 1e-4, 3e-4] for backbone, ASPP, decoder, and re\ufb01ne- ment module respectively. We train our model on multiple datasets in the follow- ing order. First, we train only the base network Gbase and then the entire model Gbase and Gre\ufb01ne jointly on Video- Matte240K, which makes the model robust to a variety of subjects and poses. Next, we train our model jointly on Pho- toMatte13K to improve the high-resolution details. Finally, we train our model jointly on Distinctions-646. The dataset has only 362 unique training samples, but it is of the high- est quality and contains human-annotated foregrounds that are very helpful for improving the foreground quality",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S34",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "pro- duced by our model. We omit training on the AIM dataset as a possible 4th stage and only use it for testing because it causes a degradation in quality as shown in our ablation study in Section 6. Alpha FG Dataset Method SAD MSE Grad Conn MSE AIM DIM\u2020 37.94 80.67 32935 37861 - FBA\u2020 9.68 6.38 4265 7521 1.94 BGM 16.07 21.00 15371 14123 47.98 BGMa 19.28 29.31 19877 18083 42.84 Ours 12.86 12.01 8426 11116 5.31 Distinctions DIM\u2020 43.70 86.22 49739 43914 - FBA\u2020 11.03 8.32 6894 9892 12.51 BGM 19.21 25.89 30443 18191 36.13 BGMa 16.02 20.18 24845 14900 43.00 Ours 9.19 7.08 6345 7216 6.10 PhotoMatte85 DIM\u2020 32.26 45.40 44658 30876 - FBA\u2020 7.37 4.79",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S35",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "7323 5206 7.03 BGM 17.32 21.21 27454 15397 14.25 BGMa 14.45 19.24 23314 13091 16.80 Ours 8.65 9.57 8736 6637 13.82 Table 1: Quantitative evaluation on different datasets. \u2020 indicates methods that require a manual trimap. 5. Experimental Evaluation We compare our approach to two trimap-based methods, Deep Image Matting (DIM) [33] and FBA Matting (FBA) [8], and one background-based method, Background Mat- ting (BGM) [28]. The input resolution to DIM was \ufb01xed at 320\u00d7320 by the implementation, while we set the FBA in- put resolution to approximately HD due to memory limits. We additionally train the BGM model on our datasets and denote it as BGMa (BGM adapted). Our evaluation uses c = 4, k = 20,000 for photos,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S36",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "c = 4, k = 5,000 for HD videos, and c = 8, k = 20,000 for 4K videos, where cis the downsampling factor for the base network and kis the number of patches that get re\ufb01ned. 5.1. Evaluation on composition datasets We construct test benchmarks by separately composit- ing test samples from AIM, Distinctions, and PhotoMatte85 datasets onto 5 background images per sample. We ap- ply minor background misalignment, color adjustment, and noise to simulate \ufb02awed background capture. We gener- ate trimaps from ground-truth alpha using thresholding and morphological operations. We evaluate matte outputs us- ing metrics from [26]: MSE (mean squared error) for alpha and foreground, SAD (sum of absolute difference), Grad (spatial-gradient metric), and Conn (connectivity) for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S37",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "alpha only. All MSE values are scaled by 10 3 and all metrics are only computed over the unknown region of trimaps gener- ated as described above. Foreground MSE is additionally only measured where the grouth-truth alpha \u03b1\u2217>0. Table 1 shows that our approach outperforms the ex- isting background-based BGM method across all datasets. 5 Input Ours BGM BGM a Trimap FBA Trimap FBA auto",
      "page_hint": null,
      "token_count": 65,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S38",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Figure 5: Qualitative comparison on real images. We produce superior results at high-resolution with minimal user input. While FBA is competitive, it fails in a fully automatic application where the segmentation-based trimap is faulty. Our approach is slightly worse than the state-of-the-art trimap-based FBA method, which requires carefully anno- tated manual trimaps and is much slower than our approach, as shown later in the performance comparison. 5.2. Evaluation on captured data Although quantitative evaluation on the above- mentioned datasets serves the purpose of quantifying the performance of different algorithms, it is important to evaluate how these methods perform on unconstrained real data. To evaluate on real data, we capture a number of photos and videos containing subjects in varying poses",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S39",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "and surroundings. The videos are captured on a tripod with consumer smartphones (Samsung S10+ and iPhone X) and a professional camera (Sony \u03b17s II), in both HD and 4K resolution. The photos are captured in 4000 \u00d76000 resolution. We also use some HD videos presented in the BGM paper that are made public to compare with our method. For fair comparison in the real-time scenario, where manual trimaps cannot be crafted, we construct trimaps by morphing segmentation result from DeepLabV3+, as sug- gested in [28]. We show results on both trimaps, denoting FBA using this fully automatic trimap as FBAauto. Figure 5 shows our method produces much sharper and more detailed results around hair and edges compared to other methods.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S40",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Since our re\ufb01nement operates at the native resolution, the quality is far superior relative to BGM, which resizes the images and only processes them at 512 \u00d7512 resolution. FBA, with manual trimap, pro- duces excellent results around hair details, however can- not be evaluated at resolutions above around HD on stan- dard GPUs. When FBA is applied on automatic trimaps generated with segmentation, it often shows large artifacts, mainly due to faulty segmentation. We extract 34 frames from both the test videos shared by the BGM paper and our captured videos and photos to create a user study. 40 participants were presented with an interactive interface showing each input image as well as the mattes produced by BGM and our",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S41",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "approach, in random or- der. They were encouraged to zoom in on details and asked to rate one of the mattes as \u201dmuch better\u201d, \u201dslightly better\u201d, or \u201dsimilar\u201d. The results, shown in Table 2, demonstrate signi\ufb01cant qualitative improvement over BGM. 59% of the time participants perceive our algorithm to be better, com- 6 pared to 23% for BGM. For sharp samples in 4K and larger, our method is preferred 75% of the time to BGM\u2019s 15%. Much worse Worse Similar Better Much better All 6% 17% 18% 32% 27% 4K+ 5% 10% 10% 34% 41% Table 2: User study results: Ours vs BGM 5.3. Performance comparison Table 3 and 4 show that our method is smaller and much faster than",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S42",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "BGM. Our method contains only 55.7% of the parameters compared to BGM. Our method can achieve HD 60fps and 4K 30fps at batch size 1 on an Nvidia RTX 2080 TI GPU, considered to be real-time for many appli- cations. It is a signi\ufb01cant speed-up compared to BGM which can only handle 512 \u00d7512 resolution at 7.8fps. The performance can be further improved by switching to Mo- bileNetV2 backbone, which achieves 4K 45fps and HD 100fps. More performance results, such as adjusting the re- \ufb01nement selection parameter kand using a larger batch size, are included in the ablation studies and in the appendix.",
      "page_hint": null,
      "token_count": 103,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S43",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "FBA HD 3.3 54.3 FBAauto HD 2.9 137.6 BGM 512 2 7.8 473.8 Ours ResNet-50* HD 60.0 34.3 ResNet-101 HD 42.5 44.0 MobileNetV2 HD 100.6 9.9 Ours ResNet-50* 4K 33.2 41.5 ResNet-101 4K 29.8 51.2 MobileNetV2 4K 45.4 17.0 Table 3: Speed measured on Nvidia RTX 2080 TI as PyTorch model pass-through without data transferring at FP32 precision and with batch size 1. GMac does not account for interpolation and cropping operations. For the ease of measurement, BGM and FBAauto use adapted PyTorch DeepLabV3+ implementation with ResNet101 backbone as segmentation.",
      "page_hint": null,
      "token_count": 90,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S44",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "FBA 34,693,031 138.80 MB FBAauto 89,398,348 347.48 MB BGM 72,231,209 275.53 MB Ours ResNet-50* 40,247,703 153.53 MB ResNet-101 59,239,831 225.98 MB MobileNetV2 5,006,839 19.10 MB Table 4: Model size comparison. BGM and FBA auto use DeepLabV3+ with Xception backbone for segmentation. Natural capture BGM Ours Green screen capture DaVinci Ours Figure 6: We produce better results than a chroma-keying soft- ware, when an amateur green-screen setup is used. 5.4. Practical use Zoom implementation We build a Zoom plugin that in- tercepts the webcam input, collects one no-person (back- ground) shot, then performs real-time video matting and compositing, streaming the result back into the Zoom call. We test with a 720p webcam in Linux. The upgrade elicits praise in real meetings,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S45",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "demonstrating its practicality in a real-world setting. Comparison to green-screen Chroma keying with a green screen is the most popular method for creating high- quality mattes. However, it requires even lighting across the screen and background-subject separation to avoid cast shadows. In Figure 6, we compare our method against chroma-keying under the same lighting with an amateur green-screen setup. We \ufb01nd that in the unevenly lit setting, our method outperforms approaches designed for the green screen. 6. Ablation Studies Role of our datasets We train on multiple datasets, each of which brings unique characteristics that help our net- work produce high-quality results at high-resolution. Table 5 shows the metrics of our method by adding or remov- ing a dataset from",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S46",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "our training pipeline. We \ufb01nd adding the AIM dataset as a possible 4th stage worsens the metrics even on the AIM test set itself. We believe it is because sam- ples in the AIM dataset are lower in resolution and quality compared to Distinctions and the small number of samples may have caused over\ufb01tting. Removal of VideoMatte240K, PhotoMatte13K, and Distinctions datasets from the train- ing pipeline all result in worse metrics, proving that those datasets are helpful in improving the model\u2019s quality. Role of the base network We experiment with replac- ing ResNet-50 with ResNet-101 and MobileNetV2 as our encoder backbone in the base network. The metrics in Ta- ble 6 show that ResNet-101 has slight improvements over ResNet-50 on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S47",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "some metrics while doing worse on others. This indicates that ResNet-50 is often suf\ufb01cient for obtain- ing the best quality. MobileNetV2 on the other hand is worse than ResNet-50 on all metrics, but it is signi\ufb01cantly faster and smaller than ResNet-50 as shown in Tables 3 and 4, and still obtains better metrics than BGM. 7 Alpha FG",
      "page_hint": null,
      "token_count": 58,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S48",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "Ours* 12.86 12.01 8426 11116 5.31 + AIM 14.19 14.70 9629 12648 6.34 - PhotoMatte13K 14.05 14.10 10102 12749 6.53 - VideoMatte240K 15.17 17.31 11907 13827 7.04 - Distinctions 15.95 19.51 11911 14909 14.36 BGM 16.07 21.00 15371 14123 42.84 Table 5: Effect of removing or appending a dataset in the training pipeline, evaluated on the AIM test set. Base Re\ufb01ne Alpha FG Backbone Kernel SAD MSE Grad Conn MSE BGMa 16.02 20.18 24845 14900 43.00 MobileNetV2 3 \u00d73 10.53 9.62 7904 8808 8.19 ResNet-50* 3 \u00d73 9.19 7.08 6345 7216 6.10 ResNet-101 3 \u00d73 9.30 6.82 6191 7128 7.68 ResNet-50 1 \u00d71 9.36 8.06 7319 7395 6.92 Table 6: Comparison of backbones and re\ufb01nement kernels on the Distinctions test",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S49",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "set Role of the re\ufb01nement network Our re\ufb01nement net- work improves detail sharpness over the coarse results in Figure 7, and is effective even in 4K resolution. Figure 8 shows the effects of increasing and decreasing the re\ufb01ne- ment area. Most improvement can be achieved by re\ufb01ning over only 5% to 10% of the image resolution. Table 7 shows that re\ufb01ning only the selected patches provides signi\ufb01cant speedup compared to re\ufb01ning the full image. Input 480 \u00d7270 HD 4K Figure 7: Effect of re\ufb01nement, from coarse to HD and 4K. % image refined scaled loss 0 10 20 30 0.1 0.5 1 5 10 MSE* SAD Grad* Conn* FG-MSE* Figure 8: Metrics on the Distinctions test set over percentage of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S50",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "image area re\ufb01ned. Grad and Conn are scaled by 10\u22123. k 2,500 5,000* 7,500 Full FPS 62.9 60.0 55.7 42.8 Table 7: Performance with different k values. Measured on our method with ResNet-50 backbone at HD. Figure 9: Failure cases. Our method fails when the subject casts a substantial shadow on, or matches color with, the background (top) and when the background is highly textured (bottom). Patch-based re\ufb01nement vs. Point-based re\ufb01nement Our re\ufb01nement module uses a stack of 3 \u00d73 convolution kernels, creating a 13 \u00d713 receptive \ufb01eld for every output pixel. An alternative is to re\ufb01ne only on points using 1 \u00d71 convolution kernels, which would result in a 2\u00d72 receptive \ufb01eld with our method. Table 6 shows that",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S51",
      "paper_id": "arxiv:2012.07810v1",
      "section": "method",
      "text": "the 3 \u00d73 kernel can achieve better metrics than point-based kernels, due to a larger receptive \ufb01eld.",
      "page_hint": null,
      "token_count": 17,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S52",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "by applying homography alignment to the background on every frame, but it is limited to small motion. Other com- mon limitations are indicated in Figure 9. We recommend using our method with a simple-textured background, \ufb01xed exposure/focus/WB setting, and a tripod for the best result. 7. Conclusion We have proposed a real-time, high-resolution back- ground replacement technique that operates at 4K 30fps and HD 60fps. Our method only requires an input image and an pre-captured background image, which is easy to obtain in many applications. Our proposed architecture ef\ufb01ciently re- \ufb01nes only the error-prone regions at high-resolution, which reduces redundant computation and makes real-time high- resolution matting possible. We introduce two new large- scale matting datasets that help our method",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S53",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "generalize to real-life scenarios. Our experiment shows our method sets new state-of-the-art performance on background matting. We demonstrate the practicality of our method by stream- ing our results to Zoom and achieve a much more realistic virtual conference call. Ethics Our primary goal is to enable creative applica- tions and give users more privacy options through back- ground replacement in video calls. However, we recognize that image editing can also be used for negative purposes, which can be mitigated through watermarking and other se- curity techniques in commercial applications of this work. 8 References [1] Ya \u02d8giz Aksoy, Tae-Hyun Oh, Sylvain Paris, Marc Pollefeys, and Wojciech Matusik. Semantic soft segmentation. ACM Transactions on Graphics (TOG), 37(4):72, 2018. 2 [2] Yagiz",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S54",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Design- ing effective inter-pixel information \ufb02ow for natural image matting. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 29\u201337, 2017. 1, 2 [3] Shaofan Cai, Xiaoshuai Zhang, Haoqiang Fan, Haibin Huang, Jiangyu Liu, Jiaming Liu, Jiaying Liu, Jue Wang, and Jian Sun. Disentangled image matting. International Conference on Computer Vision (ICCV), 2019. 2 [4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587 , 2017. 2, 3 [5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 2, 3 [6] Qifeng Chen,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S55",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Dingzeyu Li, and Chi-Keung Tang. Knn mat- ting. IEEE transactions on pattern analysis and machine intelligence, 35(9):2175\u20132188, 2013. 1, 2 [7] Yung-Yu Chuang, Brian Curless, David H Salesin, and Richard Szeliski. A bayesian approach to digital matting. In CVPR (2), pages 264\u2013271, 2001. 1, 2 [8] Marco Forte and Franc \u00b8ois Piti\u00b4e. F,b, alpha matting. arXiv preprint arXiv:2003.07711, 2020. 5 [9] Eduardo SL Gastal and Manuel M Oliveira. Shared sampling for real-time alpha matting. In Computer Graphics Forum, volume 29, pages 575\u2013584. Wiley Online Library, 2010. 1, 2 [10] Minglun Gong and Yee-Hong Yang. Near-real-time image matting with known background. In 2009 Canadian Con- ference on Computer and Robot Vision, pages 81\u201387. IEEE, 2009. 2 [11] Kaiming He, Georgia Gkioxari,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S56",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Piotr Doll \u00b4ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 2 [12] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 770\u2013778, 2016. 3 [13] Qiqi Hou and Feng Liu. Context-aware image matting for simultaneous foreground and alpha estimation.International Conference on Computer Vision (ICCV), 2019. 2 [14] S. Ioffe and Christian Szegedy. Batch normalization: Accel- erating deep network training by reducing internal covariate shift. ArXiv, abs/1502.03167, 2015. 4 [15] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir- shick. Pointrend: Image segmentation as rendering. In Pro- ceedings of the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S57",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9799\u20139808, 2020. 2 [16] Anat Levin, Dani Lischinski, and Yair Weiss. A closed-form solution to natural image matting. IEEE transactions on pattern analysis and machine intelligence , 30(2):228\u2013242, 2007. 1, 2 [17] Anat Levin, Alex Rav-Acha, and Dani Lischinski. Spectral matting. IEEE transactions on pattern analysis and machine intelligence, 30(10):1699\u20131712, 2008. 1, 2 [18] Yaoyi Li, Qingyao Xu, and Hongtao Lu. Hierarchical opacity propagation for image matting. arXiv preprint arXiv:2004.03249, 2020. 2 [19] Xiaodan Liang, Ke Gong, Xiaohui Shen, and Liang Lin. Look into person: Joint body parsing & pose estimation net- work and a new benchmark. IEEE transactions on pattern analysis and machine intelligence, 41(4):871\u2013885, 2018. 2 [20] Jinlin Liu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S58",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, and Xian-sheng Hua. Boosting se- mantic human matting with coarse annotations. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8563\u20138572, 2020. 2 [21] Hao Lu, Yutong Dai, Chunhua Shen, and Songcen Xu. In- dices matter: Learning to index for deep image matting. In- ternational Conference on Computer Vision (ICCV) , 2019. 2 [22] V . Nair and Geoffrey E. Hinton. Recti\ufb01ed linear units im- prove restricted boltzmann machines. In ICML, 2010. 4 [23] Adam Paszke, S. Gross, Francisco Massa, A. Lerer, J. Brad- bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas K\u00a8opf, E. Yang, Zach De- Vito, Martin",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S59",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Raison, Alykhan Tejani, Sasank Chilamkurthy, B. Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py- torch: An imperative style, high-performance deep learning library. ArXiv, abs/1912.01703, 2019. 11 [24] Richard J Qian and M Ibrahim Sezan. Video back- ground replacement without a blue screen. In Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), volume 4, pages 143\u2013146. IEEE, 1999. 2 [25] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hi- erarchical structure aggregation for image matting. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13676\u201313685, 2020. 1, 2 [26] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptu- ally motivated",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S60",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "online benchmark for image matting. In 2009 IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1826\u20131833. IEEE, 2009. 5, 12 [27] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4510\u20134520, 2018. 3 [28] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Back- ground matting: The world is your green screen. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2291\u20132300, 2020. 1, 2, 5, 6 [29] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and Ji- aya Jia. Deep automatic portrait matting. In European Con-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S61",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "ference on Computer Vision, pages 92\u2013107. Springer, 2016. 2 9 [30] Jian Sun, Jiaya Jia, Chi-Keung Tang, and Heung-Yeung Shum. Poisson matting. In ACM Transactions on Graph- ics (ToG), volume 23, pages 315\u2013321. ACM, 2004. 1, 2 [31] Jingwei Tang, Yagiz Aksoy, Cengiz Oztireli, Markus Gross, and Tunc Ozan Aydin. Learning-based sampling for natural image matting. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 2 [32] Jue Wang, Michael F Cohen, et al. Image and video matting: a survey. Foundations and Trends\u00ae in Computer Graphics and Vision, 3(2):97\u2013175, 2008. 2 [33] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S62",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": ", pages 2970\u2013 2979, 2017. 1, 2, 5 [34] Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi Cai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min Hu. Pose2seg: Detection free human instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 889\u2013898, 2019. 2 [35] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qix- ing Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for digital matting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7469\u2013 7478, 2019. 2 [36] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, and Ming Tang. Fast deep matting for portrait anima- tion on mobile phone. In Proceedings of the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S63",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "25th ACM inter- national conference on Multimedia , pages 297\u2013305. ACM, 2017. 2 10 A. Overview We provide additional details in this appendix. In Sec. B, we describe the details of our network architecture and implementation. In Sec. C, we clarify our use of keywords for crawling background images. In Sec. D, we explain how we train our model and show details of our data aug- mentations. In Sec. E, we show additional metrics about our method\u2019s performance. In Sec. F, we show all the qual- itative results used in our user study along with the average score per sample. B. Network B.1. Architecture Backbone Both ResNet and MobileNetV2 are adopted from the original implementation with minor modi\ufb01cations. We change",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S64",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "the \ufb01rst convolution layer to accept 6 channels for both the input and the background images. We follow DeepLabV3\u2019s approach and change the last downsampling block with dilated convolutions to maintain an output stride of 16. We do not use the multi-grid dilation technique pro- posed in DeepLabV3 for simplicity. ASPP We follow the original implementation of ASPP module proposed in DeepLabV3. Our experiment suggests that setting dilation rates to (3, 6, 9) produces the better results. Decoder CBR128 - CBR64 - CBR48 - C37 \u201dCBRk\u201d denotes k 3\u00d73 convolution \ufb01lters with same padding without bias followed by Batch Normalization and ReLU. \u201dCk\u201d denotes k 3\u00d73 convolution \ufb01lters with same padding and bias. Before every convolution, decoder uses bilinear upsampling",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S65",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "with a scale factor of 2 and concate- nates with the corresponding skip connection from the backbone. The 37-channel output consists of 1 channel of alpha \u03b1c, 3 channels of foreground residual FR c , 1 channel of error mapEc, and 32 channels of hidden featuresHc. We clamp \u03b1c and Ec to 0 and 1. We apply ReLU on Hc. Re\ufb01ner First stage: C*BR24 - C*BR16 Second stage: C*BR12 - C*4 \u201dC*BRk\u201d and \u201dC*k\u201d follow the same de\ufb01nition above except that the convolution does not use padding. Re\ufb01ner \ufb01rst resamples coarse outputs \u03b1c, FR c , Hc, and input images I, Bto 1 2 resolution and concatenates them as [n\u00d742\u00d7h 2 \u00d7w 2 ] features. Based on the error prediction",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S66",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "Ec, we crop out topkmost error-prone patches[nk\u00d742\u00d78\u00d78]. After applying the \ufb01rst stage, the patch dimension becomes [nk\u00d716 \u00d74 \u00d74]. We upsample the patches with nearest upsampling and concatenate them with patches at the corre- sponding location from I and Bto form [nk\u00d722 \u00d78 \u00d78] features. After the second stage, the patch dimension be- comes [nk\u00d74 \u00d74 \u00d74]. The 4 channels are alpha and fore- ground residual. Finally, we bilinearly upsample the coarse \u03b1c and FR c to full resolution and replace the re\ufb01ned patches to their corresponding location to form the \ufb01nal output \u03b1 and FR. B.2. Implementation We implement our network in PyTorch [23]. The patch extraction and replacement can be achieved via the native vectorized operations for maximum",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S67",
      "paper_id": "arxiv:2012.07810v1",
      "section": "limitations",
      "text": "performance. We \ufb01nd that PyTorch\u2019s nearest upsampling operation is much faster on small-resolution patches than bilinear upsampling, so we use it when upsampling the patches. C. Dataset VideoMatte240K The dataset contains 484 video clips, which consists a total of 240,709 frames. The average frames per clip is 497.3 and the median is 458.5. The longest clip has 1500 frames while the shortest clip has 124 frames. Figure 10 shows more examples from Video- Matte240K dataset. Figure 10: More examples from VideoMatte240K dataset.",
      "page_hint": null,
      "token_count": 82,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S68",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "ground images are: airport interior attic bar interior bathroom beach city church interior classroom interior empty city forest garage interior gym interior house outdoor interior kitchen lab interior landscape lecture hall mall interior night club interior office rainy woods rooftop stadium interior theater interior train station warehouse interior workplace interior 11 D. Training Table 8 records the training order, epochs, and hours of our \ufb01nal model on different datasets. We use 1\u00d7RTX 2080 TI when training only the base network and 2 \u00d7RTX 2080 TI when training the network jointly. Dataset Network Epochs Hours VideoMatte240K Gbase 8 24 VideoMatte240K Gbase + Gre\ufb01ne 1 12 PhotoMatte13K Gbase + Gre\ufb01ne 25 8 Distinctions Gbase + Gre\ufb01ne 30 8 Table 8: Training epochs",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S69",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "and hours on different datasets. Time measured on model with ResNet-50 backbone. Additionally, we use mixed precision training for faster computation and less memory consumption. When using multiple GPUs, we apply data parallelism to split the mini- batch across multiple GPUs and switch to use PyTorch\u2019s Synchronized Batch Normalization to track batch statistics across GPUs. D.1. Training augmentation For every alpha and foreground training sample, we ro- tate to composite with backgrounds in a \u201dzip\u201d fashion to form a single epoch. For example, if there are 60 train- ing samples and 100 background images, a single epoch is 100 images, where the 60 samples \ufb01rst pair with the \ufb01rst",
      "page_hint": null,
      "token_count": 109,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S70",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "the rest of the 40 background images again. The rotation stops when one set of images runs out. Because the datasets we use are very different in sizes, this strategy is used to generalize the concept of an epoch. We apply random rotation (\u00b15deg), scale (0.3\u223c1), trans- lation (\u00b110%), shearing ( \u00b15deg), brightness (0.85 \u223c1.15), contrast (0.85\u223c1.15), saturation (0.85\u223c1.15), hue (\u00b10.05), gaussian noise ( \u03c32 \u22640.03), box blurring, and sharpen- ing independently to foreground and background on ev- ery sample. We then composite the input image using I = \u03b1F + (1\u2212\u03b1)B. We additionally apply random rotation (\u00b11deg), transla- tion (\u00b11%), brightness (0.82\u223c1.18), contrast (0.82\u223c1.18), saturation (0.82\u223c1.18), and hue ( \u00b10.1) only on the back- ground 30% of the time.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S71",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "This small misalignment between input I and background Bincreases model\u2019s robustness on real-life captures. We also \ufb01nd creating arti\ufb01cial shadows increases model\u2019s robustness because subjects in real-life often cast shadows on the environment. Shadows are created on I by darkening some areas of the image behind the subject fol- lowing the subject\u2019s contour 30% of the time. Examples of composited images are shown in Figure 11. The bottom row shows examples of shadow augmentation. Figure 11: Training samples with augmentations. Bottom row are samples with shadow augmentation. Actual samples have different resolutions and aspect ratios. D.2. Testing augmentation For AIM and Distinctions, which have 11 human test samples each, we pair every sample with 5 random back- grounds from the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S72",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "background test set. For PhotoMatte85, which has 85 test samples, we pair every sample with only",
      "page_hint": null,
      "token_count": 16,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S73",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "[26] to evaluate the resulting sets of 55, 55, and 85 images. We apply a random subpixel translation ( \u00b10.3 pixels), random gamma (0.85 \u223c1.15), and gaussian noise ( \u00b5 = \u00b10.02, 0.08 \u2264\u03c32 \u22640.15) to background Bonly, to simu- late misalignment. The trimaps used as input for trimap-based methods and for de\ufb01ning the error metric regions are obtained by thresh- olding the grouth-truth alpha between 0.06 and 0.96, then applying 10 iterations of dilation followed by 10 iterations of erosion using a 3\u00d73 circular kernel. E. Performance Table 9 shows the performance of our method on two Nvidia RTX 2000 series GPUs: the \ufb02agship RTX 2080 TI and the entry-level RTX 2060 Super. The entry-level GPU yields lower FPS",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S74",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "but is still within an acceptable range for many real-time applications. Additionally, Table 10 shows that switching to a larger batch size and a lower precision can increase the FPS signi\ufb01cantly. F. Additional Results In Figures 13, 14, 15, we show all 34 examples in the user study, along with their average rating and results by different methods. Figure 12 shows the web UI for our user- study. 12 Figure 12: The web UI for our user study. Users are shown the original image and two result images from Ours and BGM methods. Users are given the instruction to rate whether one algorithm is \u201dmuch better\u201d, \u201dslightly better\u201d, or both as \u201dsimilar\u201d. GPU Backbone Reso FPS RTX 2080 TI ResNet-50",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S75",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "HD 60.0 4K 33.2 MobileNetV2 HD 100.6 4K 45.4 RTX 2060 Super ResNet-50 HD 42.8 4K 23.3 MobileNetV2 HD 75.6 4K 31.3 Table 9: Performance on different GPUs. Measured with batch size 1 and FP32 precision. Backbone Reso Batch Precision FPS MobileNetV2 HD 1 FP32 100.6 8 FP32 138.4 8 FP16 200.0 4K 8 FP16 64.2 Table 10: Performance using different batch sizes and precisions. Measured on RTX 2080 TI. 13 +2.0 -3.3 +1.8 +0.1 +3.3 +9.0 +7.5 +7.1 -2.6 -1.3 -0.8 2.4 Input Ours BGM BGM a Trimap FBA Trimap FBA auto Score",
      "page_hint": null,
      "token_count": 94,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S76",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Figure 13: Additional qualitative comparison (1/3). Average user ratings between Ours and BGM are included. A score of -10 denotes BGM is \u201dmuch better\u201d, -5 denotes BGM is \u201dslightly better\u201d, 0 denotes \u201dsimilar\u201d, +5 denotes Ours is \u201dslightly better\u201d, +10 denotes Ours is \u201dmuch better\u201d. Our method receives an average 3.1 score. 14 -5.5 +2.9 +2.3 +3.8 +3.6 -3.6 -2.4 +8.8 +6.0 +5.6 +6.3 Input Ours BGM BGM a Trimap FBA Trimap FBA auto Score",
      "page_hint": null,
      "token_count": 75,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S77",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Figure 14: Additional qualitative comparisons (2/3) 15 +7.4 +8.8 +7.4 +7.0 -3.5 +6.8 -3.3 +5.8 +5.1 +0.1 +3.8 Input Ours BGM BGM a Trimap FBA Trimap FBA auto Score",
      "page_hint": null,
      "token_count": 29,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2012_07810v1:S78",
      "paper_id": "arxiv:2012.07810v1",
      "section": "background",
      "text": "Figure 15: Additional qualitative comparisons (3/3) 16",
      "page_hint": null,
      "token_count": 7,
      "paper_year": 2020,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9468092076447584,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 16,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3861,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5620,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4595,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4697,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4919,
        "empty": false
      },
      {
        "page": 6,
        "chars": 2702,
        "empty": false
      },
      {
        "page": 7,
        "chars": 4180,
        "empty": false
      },
      {
        "page": 8,
        "chars": 3676,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5876,
        "empty": false
      },
      {
        "page": 10,
        "chars": 1495,
        "empty": false
      },
      {
        "page": 11,
        "chars": 3751,
        "empty": false
      },
      {
        "page": 12,
        "chars": 3961,
        "empty": false
      },
      {
        "page": 13,
        "chars": 688,
        "empty": false
      },
      {
        "page": 14,
        "chars": 506,
        "empty": false
      },
      {
        "page": 15,
        "chars": 228,
        "empty": false
      },
      {
        "page": 16,
        "chars": 228,
        "empty": false
      }
    ],
    "quality_score": 0.9468,
    "quality_band": "good"
  }
}