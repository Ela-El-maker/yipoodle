{
  "paper": {
    "paper_id": "arxiv:2311.12651v3",
    "title": "Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots",
    "authors": [
      "Youqi Liao",
      "Shuhao Kang",
      "Jianping Li",
      "Yang Liu",
      "Yun Liu",
      "Zhen Dong",
      "Bisheng Yang",
      "Xieyuanli Chen"
    ],
    "year": 2023,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Precise and rapid delineation of sharp boundaries and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The encoder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fusion of semantic and boundary information by learning channel-wise relationships, allowing for precise weight assignment of each channel. Furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. Compared to existing methods, the proposed Mobile-Seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. Experiments on the Cityscapes dataset have shown that Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA) baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while maintaining an online inference speed of 23.9 frames-per-second (FPS) with 1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on CamVid and PASCAL Context datasets confirm our method's generalizability. Code and additional results are publicly available at https://whu-usi3dv.github.io/Mobile-Seed/.",
    "pdf_path": "data/automation/papers/arxiv_2311.12651v3.pdf",
    "url": "https://arxiv.org/pdf/2311.12651v3",
    "doi": null,
    "arxiv_id": "2311.12651v3",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:50:38.452839+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2311_12651v3:S1",
      "paper_id": "arxiv:2311.12651v3",
      "section": "body",
      "text": "IEEE ROBOTICS AND AUTOMATION LETTERS. 1 Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong, Bisheng Yang, Xieyuanli Chen",
      "page_hint": null,
      "token_count": 32,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S2",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "and robust semantics is essential for numerous downstream robotic tasks, such as robot grasping and manipulation, real-time semantic mapping, and online sensor calibration performed on edge computing units. Although boundary detection and semantic segmentation are complementary tasks, most studies focus on lightweight models for semantic segmentation but overlook the critical role of boundary detection. In this work, we introduce Mobile-Seed, a lightweight, dual-task framework tailored for simultaneous semantic segmentation and boundary detection. Our framework features a two-stream encoder, an active fusion decoder (AFD) and a dual-task regularization approach. The en- coder is divided into two pathways: one captures category-aware semantic information, while the other discerns boundaries from multi-scale features. The AFD module dynamically adapts the fu- sion of semantic and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S3",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "boundary information by learning channel- wise relationships, allowing for precise weight assignment of each channel. Furthermore, we introduce a regularization loss to mitigate the conflicts in dual-task learning and deep diversity supervision. Compared to existing methods, the proposed Mobile- Seed offers a lightweight framework to simultaneously improve semantic segmentation performance and accurately locate object boundaries. Experiments on the Cityscapes dataset have shown that Mobile-Seed achieves notable improvement over the state- of-the-art (SOTA) baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while maintaining an online inference speed of 23.9 frames-per-second (FPS) with 1024\u00d72048 resolution input on an RTX 2080 Ti GPU. Additional experiments on CamVid and PASCAL Context datasets confirm our method\u2019s generalizability. Code and additional",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S4",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "results are publicly available at: https://whu-usi3dv.github.io/Mobile-Seed/. Index Terms\u2014Deep learning for visual perception, visual learning, deep learning methods. I. I NTRODUCTION Semantic segmentation and boundary detection are fun- damental tasks for simultaneous localization and mapping (SLAM) [3], autonomous driving [4], behavior prediction [5] and sensors calibration [6]. Semantic segmentation predicts the categorical labels for each pixel, and the boundary detection task identifies pixels lying on the boundary area where at least one neighborhood pixel belongs to a different class. Manuscript received: Nov. 21, 2023; Revised: Jan. 15, 2024; Accepted: Feb. 20, 2024. This paper was recommended for publication by Editor Cesar Cadena Lerma upon evaluation of the Associate Editor and Reviewers\u2019 comments. Digital Object Identifier (DOI): see top of this page.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S5",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "This study was supported by the National Natural Science Foundation Project (No. 42201477, No. 42130105) (Corresponding author: Jianping Li) Y . Liao, Z. Dong and B. Yang are with Wuhan University, China. S. Kang is with the Technical University of Munich, Germany. J. Li is with Wuhan University, China and Nanyang Technological University, Singapore. Yang Liu is with the King\u2019s College London, UK. Yun Liu is with the Institute of Infocomm Research (I2R), A*STAR, and X. Chen is with the National University of Defense Technology, China. PSPNet Dlabv3+ Seg-B1 Seg-B0 AFF-T AFF-B 10M5M2M Mobile-Seed 66 68 70 72 74 76 78 80 82 0 10 20 30 mIoU(%) Inference Speed (Frames/Second) 68.0 76.2 72.2 78.4 66 68 70 72 74",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S6",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "76 78 80 mF-score mIoU value(%) AFF-T Mobile-Seed (b) Performance (proposed) Fig. 1: (a) Motivation map: Mobile-Seed performs pixel-wise seg- mentation and object boundary detection simultaneously, and then fuses semantic and boundary features for accurate prediction. The boundary detection and semantic segmentation predictions could be transferred for downstream tasks, e.g., robot manipulation, semantic mapping and sensor calibration. (b) Our Mobile-Seed achieves higher performance on both semantic segmentation and boundary detection tasks while keeping real-time efficiency. The resolution of input is 1024\u00d72048 when testing inference speed. \u201cAFF\u201d and \u201cSeg\u201d mean the AFFormer [1] and SegFormer [2], respectively. As the boundary always surrounds the object\u2019s body [7], robust prediction of the body label guides the object boundary detection, while improving the boundary",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S7",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "location is crucial for semantic segmentation accuracy. In other words, seman- tic segmentation and boundary detection are complementary tasks. Moreover, simultaneously extracting segmentation and boundary information in compact robotics is important for semantic SLAM [8], [9], in which the boundary is a strong constraint for solving the relative pose and location, and segmentation is crucial for dynamic object removal. However, on the one hand, most lightweight approaches [1], [2], [10] attempt to solve the semantic segmentation task but overlook the boundary accuracy. On the other hand, existing dual-task learning approaches [7], [11], [12] design novel architectures for performance improvement but neglect the computational burden. Overall, simultaneously capturing the segmentation and boundary has not received enough attention, but this arXiv:2311.12651v3 [cs.CV]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S8",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "11 Mar 2024 2 IEEE ROBOTICS AND AUTOMATION LETTERS. (a) (b) (c) (d) Fig. 2: Example diagram of color image (a), semantic mask (b), semantic boundary mask (c) and binary boundary mask (d). Semantic boundary masks are generated as [16], [17], and binary boundary masks are generated as [18]. is precisely in urgent need of the robotics society. In this paper, we investigate how to design a lightweight framework for jointly learning the semantic and boundary mask in a complementary manner, as shown in Fig. 1. To deploy semantic segmentation for real-world online robotic and autonomous driving applications, powerful yet lightweight vision transformers (ViT) [13] have been de- veloped. For example, the hierarchical attention [2], stride attention [10], and window",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S9",
      "paper_id": "arxiv:2311.12651v3",
      "section": "abstract",
      "text": "attention [14] are proposed to capture the long-range context with low computation cost and outperform convolution neural network (CNN) based",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S10",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "insufficient to accurately locate object boundaries. The main reasons are: i) as the Transformer lacks inductive bias [13], it is not good at capturing fine-grained details in a local window; ii) most methods adopt very simple decoder designs, which lack the ability to capture and recover details. Some recent approaches [1], [15] even remove the decoder for efficiency, called \u201chead free\u201d, which exacerbates boundary blurring. For the boundary detection task, most existing approaches [16], [17], [18] overlooked the computational efficiency. In the field of dual-task learning for semantic segmentation and boundary detection, several approaches [11], [12], [19] pointed out that jointly learning the boundary detection and seman- tic segmentation tasks with reasonable designs benefits both tasks, but none of them",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S11",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "discussed how to implement with a lightweight design for mobile robots. It should be retained that the boundary detection task here is significantly different from the edge detection task [20]. Fig. 2 shows the semantic mask, the semantic boundary mask, and the binary boundary mask of a color image. Boundary detection aims to find semantically discontinuous areas instead of dramatic intensity, illumination, or texture changes in edge detection task. To address the limitations mentioned above, we present a lightweight framework for simultaneous semantic segmen- tation and boundary detection. The workflow is shown in Fig. 3. Our objective is to utilize the semantic stream to offer fundamental knowledge for the boundary stream while sup- plementing the semantic segmentation task with fine-grained",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S12",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "details captured by the boundary branch. Additionally, we introduce the active fusion decoder (AFD) to learn the fusion weights from inputs and fuse the semantic and boundary features in a dynamic way. Furthermore, we incorporate the dual-task regularization losses to alleviate conflicts arising from deep diverse supervision (DDS) [18]. Experiments on multiple public datasets demonstrate our Mobile-Seed out- performs existing methods by a large margin, especially in predicting crisper boundaries and segmenting small and thin objects. Overall, the main contributions of this paper include: 1) We propose a lightweight joint semantic segmentation and boundary detection framework for mobile robots. This framework can concurrently learn both the boundary mask and semantic mask. 2) We present the AFD for learning the channel-wise",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S13",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "re- lationship between semantic features and boundary fea- tures. Compared to the fixed weight methods (fusion weights independent of the input), our AFD is more flexible in assigning proper weights for semantic features and boundary features. 3) We introduce the dual-task regularization loss to effec- tively mitigate conflicts arising from DDS, allowing the tasks of semantic segmentation and boundary detection to contribute to each other. II. R ELATED WORK A. Lightweight Semantic Segmentaiton Since the pioneering approaches fully convolution network (FCN) [21] ushered in a new era, a significant amount of works [22], [23] have been dedicated to addressing seman- tic segmentation tasks. To reduce the computational burden caused by dense convolution operations on feature maps, the MobileNet [24] proposed",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S14",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "the depthwise separable convolu- tion and ShuffleNet [25] proposed the channel shuffle to maintain accuracy. Fast-SCNN [26] proposed the \u201clearning to downsample\u201d module to produce shared low-level funda- mental features. During the transformer era, SegFormer [2] was the first transformer-based lightweight design for mobile devices. Activated by Swin-Transformer [27], MobileViT [14] proposed the hybrid CNN and Transformer blocks for local and global processing. TopFormer [10] designs the token pyramid module for scale-aware features. Experiments show that Topformer achieves a better trade-off between accuracy and efficiency than previous approaches. AFFormer [1] pro- posed the channel-wise attention module and SeaFormer [15] proposed the axial-attention module. Coincidentally, both of them utilized the \u201chead free\u201d decoder design: a simple clas- sification head with several",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S15",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "convolution layers, which means predicting at a low resolution without progressive upsampling and refinement. PP-mobileSeg [28] inherited the stride-Former frame [15] and proposed the aggregated attention module (AAM) and valid interpolation module (VIM) to enhance the semantic features. Unlike the above approaches which design single-branch models for semantic segmentation, we introduce a dual-branch framework to simultaneously learn semantic segmentation and boundary detection. B. Boundary Detection CASENet [16] is the first multi-label learning framework to identify semantic boundaries. Based on the ResNet-101 [29], LIAO et al.: MOBILE-SEED: JOINT SEMANTIC SEGMENTATION AND BOUNDARY DETECTION FOR MOBILE ROBOTS 3 AFD Concat Semantic Stream Boundary Stream Fused Semantic Mask Boundary Mask 1/2 1/4 Shared Stem Mobile ViT Mobile ViT Mobile ViT Mobile ViT 3x3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S16",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Conv & Up Semantic Branch Boundary Branch Active Fusion DecoderAFD Semantic Mask Fig. 3: Workflow of Mobile-Seed, where the semantic stream S and boundary stream B extract semantic and boundary features respectively. AFD estimates the relative weights for each channel of semantic features Fs and boundary features Fb. An auxiliary classification head is applied to the semantic stream for direct supervision during training. Semantic prediction s, fused semantic prediction sf , and boundary prediction b are supervised separately and accordingly. Regularization loss Lreg mitigates the divergences caused by dual-task learning. CASENet utilizes the bottom layers for details and the top layers for category-aware features. STEAL [30] detects seman- tic boundaries and corrects noise labels iteratively for crisper prediction. DFF [17]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S17",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "proposed to learn dynamic weights for different input images and locations. RPCNet [7] proposed to jointly learn the semantic and semantic boundary with iterative pyramid context modules. DDS [18] proposed the information converter consisting of several ResNet blocks [29] to mitigate the conflicts caused by deep diverse supervision. However, integrating the information converter into the network will significantly increase the computational burden, especially for high-resolution images. GSCNN [11], DecoupleNet [12] and BASeg [19] are the most similar approaches to our work. They take binary bound- ary detection as a supplement for semantic segmentation, which is performed in an auxiliary manner like the auxiliary loss function. However, our approach has significant differ- ences compared to them: (i) Mobile-Seed is a joint",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S18",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "boundary detection and semantic segmentation framework instead of a boundary-auxiliary semantic segmentation; (ii) we focus on designing a lightweight model with the least computational burden in contrast to previous cumbersome models. III. M OBILE -SEED OVERVIEW In this section, we present the lightweight Mobile-Seed for joint semantic segmentation and boundary detection learning. As illustrated in Fig. 3, the Mobile-Seed contains a two-stream encoder for semantic segmentation and boundary detection, and then an active fusion decoder (AFD) for features fusion. Each branch\u2019s output is supervised with the corresponding ground truth. Moreover, regularization loss is introduced to direct dual-task learning in a complementary way. A. Architecture Overview Since the goal is to learn the semantic and boundary information simultaneously, we propose a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S19",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "two-stream encoder to capture the corresponding features from the input image. Firstly, a simple shared stem module consisting of two Mo- bileNetV2 blocks [31] is utilized to embed the original image I \u2208 R3\u00d7H\u00d7W into high-dimension feature space, where H and W mean the height and width of image I respectively. The semantic stream S takes the second embed feature map as input and generates semantic-rich features. We emphasize that the semantic stream could be any lightweight semantic seg- mentation backbone, e.g., [1], [2], [10], [15], [24], [25], [32]. In this paper, we select one of the most recent SOTA methods, AFFormer-T [1] (\u2018T\u2019 means the tiny model of AFFormer) as our semantic stream backbone. A simple classification head is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S20",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "used to generate the auxiliary semantic map s during training. The boundary stream B takes the first embedded feature map and intermediate feature maps of the semantic stream as input, and feeds into a 3 \u00d7 3 convolution layer, group normalization layer and ReLU layer to differentiate the semantic features to boundary features. Let m denote the stage number and i \u2208 {1, 2, ..., m} denote the running index, the i-th stage\u2019s representation of the semantic stream is denoted as Fi s. For the i-th location, the information conversion process in the boundary stream is denoted as: Fi b = \u03c3(C3\u00d73(Fi s)), (1) where Fi b means boundary feature of the i-th stage, C3\u00d73 means normalized 3 \u00d7 3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S21",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "convolution layer, and \u03c3 means activation operation. Then, the multi-scale boundary features are upsampled with bilinear interpolation and concatenated together. Finally, a simple classification head is applied for predicting boundary map b \u2208 RH\u00d7W , as shown in Fig. 4. B. Active fusion Decoder After obtaining high-dimensional semantic and boundary features, the ensuing problem is how to efficiently fuse features from different domains. As the semantic stream is supervised to learn category-aware semantics and the boundary stream is supervised to learn category-agnostic boundaries, there is a significant domain divergence in two types of features. Most previous approaches use addition [10], [21] or concatena- tion [12] to fuse features from multiple scales or streams, and some others introduce atrous spatial pyramid",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S22",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "pooling 4 IEEE ROBOTICS AND AUTOMATION LETTERS. Mobile-SED Boundary Prediction Ground TruthImage Fig. 4: Examples of boundary maps from the boundary stream. The first column shows the input, the second shows the boundary predictions, and the last column shows the ground-truth boundaries. Fig. 5: Illustration of the proposed AFD. (ASPP) [11] or pyramid context module (PCM) [7] for well- mixed in spatial dimension. The above methods could be classified as fixed weights methods, where the fusion weights in the channel dimension are image-independent. However, the importance of each channel in semantic features and boundary features may vary for different images. Therefore, the fusion weights should be conditioned on the input. There are dynamic fusion methods [17], [19] that can adapt",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S23",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "the weights for semantic edge detection and semantic segmentation tasks. However, calculating fusion weights in both spatial and chan- nel dimensions is still too cumbersome for the lightweight framework. To tackle this issue, we propose the active fusion decoder (AFD) module to learn the fusion weights for semantic stream and boundary stream, as shown in Fig. 5. With the semantic feature map Fs \u2208 RC\u00d7H\u00d7W from semantic stream and boundary feature map Fb \u2208 RC\u00d7H\u00d7W from boundary stream, we first calculate the semantic channel-wise attention vector Fatt s \u2208 RC\u00d71\u00d71 and boundary channel-wise attention vector Fatt b \u2208 RC\u00d71\u00d71 with global average pooling (GAP): Fatt s = fs(GAP(Fs)), Fatt b = fb(GAP(Fb)), (2) where Fatt s , Fatt b \u2208",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S24",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "RC\u00d71\u00d71, and f(\u00b7) means multi-layer perception (MLP) modules. We stack attention vectors of semantic and boundary features in channel dimension: Fatt f = (Fatt s \u2225Fatt b ), (3) where \u2225 means channel-wise concatenation operation. To met- ric the affinity among each channel of semantic and boundary features, we split the fusing attention vector Fatt f \u2208 R2C into H groups and generate the channel-wise affinity matrix A \u2208 R2C/H\u00d72C/H . We first calculate the query vector q, key vector k and value vector v from Fatt f by linear projection, and then each component of affinity matrix Ai,j is computed as: Ai,j = eqikj \u22a4 / HX i=1 eqi , (4) where qi is the i-th head of query,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S25",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "and kj is the j-th head of key. The i-th head of active fusion weight w is calculated with: wi = Avi, (5) where vi means the i-th head of value vector v. Then we divide the weight vector w into ws for semantic stream and wb for boundary stream by inverse operation of Eq. 3. The fused features Ff is calculated through residual connection: Ff = (1 +ws)Fs + (1 +wb)Fb. (6) A 1 \u00d7 1 convolution layer is followed as classification head to compact the Ff into the final semantic prediction map sf \u2208 RN\u00d7H\u00d7W , N means the category number. Overall, the AFD estimates the channel-wise relationship within and among semantic features and boundary features for learning",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S26",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "optimal fusion weights. C. Loss Functions In the Mobile-Seed framework, we jointly train the semantic and boundary stream in an end-to-end way and use the AFD to fuse the dual-task features for final prediction. Therefore, available supervisions include semantic label \u02c6s \u2208 RN\u00d7H\u00d7W , semantic boundary label \u02c6bs \u2208 RN\u00d7H\u00d7W and binary boundary label \u02c6b \u2208 RH\u00d7W (as shown in Fig. 2 (b), (c) and (d)). The cross-entropy (CE) loss and binary cross-entropy (BCE) loss function are used to supervise the semantic and boundary predictions: Lcls = Lce(s, \u02c6s) + Lce(sf , \u02c6s) + Lbce(b, \u02c6b). (7) Dual-task regularization. With the top supervision of semantic label \u02c6s, the top layers are acquired to learn abstracted semantic representation, enabling it to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S27",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "cover diverse object shapes, lighting conditions and textures. In contrast, bottom supervision of the boundary label \u02c6b leads the bottom layers to distinct boundaries or non-boundaries, rather than category- aware semantics. Since the bottom layers provide basic rep- resentations for both semantic segmentation and boundary detection, the bottom layers receive two distinctively different supervisions under back-propagation. Liu et al. [18] pointed out that applying deep diverse supervision (DDS) directly may lead to conflicts and performance degradation, while a single convolution layer in the boundary stream is too weak to allevi- ate the supervision conflicts. Our ablation studies in Sec. IV-C also support this finding. To address these conflicts, authors of the DDS introduced buffer blocks to isolate the backbone and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S28",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "side layers. Unlike that, we design bi-directional consistency loss Lreg consisting of the semantic-to-boundary consistency loss Ls2b and the boundary-to-semantic consistency loss Lb2s to soften the conflict and free computation burden during LIAO et al.: MOBILE-SEED: JOINT SEMANTIC SEGMENTATION AND BOUNDARY DETECTION FOR MOBILE ROBOTS 5 inference. The semantic-to-boundary consistency loss Ls2b is designed to align pseudo semantic boundary bps generated from semantic prediction sf with the semantic boundary label \u02c6bs. We introduce a filtering template T \u2208 RN\u00d7(2r+1)\u00d7(2r+1) to look into neighbors of each pixel and seek for the maximum difference in each category, where r is the search radius. For ease of description, we set r = 2 here and each channel of the filtering template T is:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S29",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Ti = \uf8ee \uf8ef\uf8ef\uf8ef\uf8ef\uf8f0 0 0 1 0 0 0 1 1 1 0 1 1 \u22121 1 1 0 1 1 1 0 0 0 1 0 0 \uf8f9 \uf8fa\uf8fa\uf8fa\uf8fa\uf8fb , i\u2208 1, 2, ..., N. (8) We slide the template on the fused semantic prediction map sf \u2208 RN\u00d7H\u00d7W and select the max absolute value in the filtering window as the pseudo semantic boundary prediction: bps = max T (\u2225T \u229b sf \u2225). (9) Intuitively, the template T mimics the generation process of the semantic boundary label \u02c6bs by checking whether neigh- boring pixels have different labels or not. Mean absolute loss is used to supervise the pseudo semantic boundaries: Ls2b = \u2225bps \u2212 \u02c6bs\u2225. (10) On the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S30",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "other hand, boundary prediction provides important prior knowledge to ensure semantic consistency between body and boundary. We combine weighted cross-entropy loss with boundary prior to formulating the boundary-to-semantic con- sistency loss Lb2s : Lb2s = X k,p 1b,\u02c6b \u0000\u02c6s(k, p) log(sf (k, p) \u0001 , (11) where k and p walk over the categories and pixels. 1b,\u02c6b = {1 : b > \u03f5 \u222a \u02c6b = 1} marks ground-truth (GT) pixels and high confidence pixels on the boundary prediction map b. \u03f5 is the confidence threshold and we use 0.8 in the experiments. With the bi-direction consistency losses Ls2b and Lb2s, the regularization function can be formulated as : Lreg = Ls2b + Lb2s. (12) The total loss function is:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S31",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "L = \u03bb1Lcls + \u03bb2Lreg, (13) where \u03bb1, \u03bb2 are hyper-parameters to control weights of clas- sification loss and dual-task regularization. IV. E XPERIMENTAL EVALUATION In this section, we conduct experiments on three publicly available datasets: Cityscapes [33], CamVid [34] and PASCAL Context [35] to show the capability of our method in various environments. Sec. IV-A introduces the implementation details and evaluation metrics. In Sec. IV-B, we compare our method with SOTA methods on the Cityscapes dataset and extensively validate on CamVid and PASCAL Context to demonstrate the generalization and robustness. In Sec. IV-C, ablation studies on the AFD and regularization loss demonstrate the effectiveness of our design. Overall, the results prove that our approach could (i) jointly learn the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S32",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "semantic segmentation and boundary detection tasks; (ii) improve the semantic segmentation per- formance while maintaining online operation; (iii) accurately detect object boundaries in complex scenes. A. Experimental Setup Implementation details. We build our model based on the MMsegmentation toolbox. All experiments were performed on an NVIDIA RTX 4090 GPU. We select the AFFormer-T [1] as our semantic stream and pre-train on ImageNet-1k [36], while the boundary stream learns from scratch. Most training details follow previous approaches [1], [10]. The hyperparameters for controling loss weight are set to \u03bb1 = \u03bb2 = 1. We use the AdamW optimizer [37] for all datasets to update model parameters. The data augmentation methods include random resize, random scaling, random horizontal flipping and color jittering.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S33",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "The training iterations, batch size and input image size for Cityscapes, CamVid and PASCAL Context datasets are set to [160K, 8, 1024 \u00d71024], [20K, 16, 520 \u00d7520], [80K, 16, 480 \u00d7480] respectively. For more training details, please refer to our open-source code. Evaluation metrics. We report three quantitative measures to evaluate the performance of our method. (i) We evaluate the semantic segmentation results with the widely used Inter- section over Union (IoU) metric. (ii) To evaluate our purpose that the Mobile-Seed extracts high-quality semantic boundary, we use the F-score as previous approaches [11], [12] on the Cityscapes val dataset. This boundary metric computes the F1 score between dilated semantic boundary prediction and ground truth with a threshold to control the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S34",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "bias degree. We set thresholds 0.00088, 0.001875, 0.00375, and 0.005, which correspond to 3, 5, 9, and 12 pixels respectively. (iii) The boundary IoU (BIoU) [38] is introduced to further evaluate both the semantic boundary and binary boundary performance on various datasets. Compared with the F-score, the BIoU is more sensitive to small object errors. For efficiency analysis, we report the FLOPs, params number and FPS evaluated on an RTX 2080 Ti GPU with batch size of 1. For a fair comparison, inferences are conducted on the origin image resolution instead of multi-scale inference. B. Quantatitave and Qualitative Results The comparison of the semantic segmentation results with SOTA methods on the Cityscapes val dataset is shown in Tab. I. As",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S35",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "can be seen, the Mobile-Seed owns fewer pa- rameters, lower computation costs, and higher mIoU per- formance than AFFormer-B, validating that our two-stream design achieves a better balance of accuracy and efficiency. Tab. II shows the category-wise comparison in terms of IoU with our strong baseline method AFFormer-T. Our method sig- nificantly outperforms the baseline method in most categories (18/19), improving the mIoU score from 76.2 to 78.4 ( 2.2% improvement) over the strong baseline. Moreover, our method could still keep near real-time (23.9 FPS) inference speed. The qualitative results are shown in Fig. 6, with additional results available on the project page. 6 IEEE ROBOTICS AND AUTOMATION LETTERS. (d) AFFormer-T (e) Mobile-Seed (b) Grount Truth (c) TopFormer-B (a) Image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S36",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Fig. 6: Qualitative comparisons for semantic segmentation. The \u201cunlabeled\u201d area is rendered as black in ground truth. To demonstrate that Mobile-Seed achieves a more precise boundary location, we evaluate the semantic boundary accu- racy with the F-score metric reported in Tab. III. It shows that our method outperforms the baseline method by a large mar- gin, especially in the strictest condition with the 3px threshold (about 4.2% improvement in the F-score metric). The results of semantic boundary validate that jointly learning semantic segmentation and boundary detection boosts the segmentation performance in boundary areas. The qualitative results of the semantic boundary in Fig. 7 show that our method predicts sharper and more continuous boundaries. Lastly, we report the semantic boundary and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S37",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "binary bound- ary performance with the BIoU metric. Fig. 8 (a) shows the Mobile-Seed achieves higher BIoU value under several thresholds. As the baseline method is a semantic segmentation framework and has no boundary stream, we retrained it with the binary boundary \u02c6b supervision (called AFF-T-B in the following). The BIoU scores of Mobile-Seed and AFF-T-B shown in Fig. 8 (b) demonstrate that our framework extracts crisper and more accurate boundaries than independently learning object boundaries. We additionally visualize the activation maps of each stage from the boundary stream in Fig. 9, illustrating that the lower stages are interested in sharp intensity change (Fig. 9 (b), (c)) and higher stages (Fig. 9 (d), (e)) focus on semantic inconsistency. Intuitively, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S38",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "bottom layers capture low-level details and the top layers obtain high-level semantics, and in the end, the boundary stream head adaptively combines multi- level features for boundary prediction.\u00b7 Extensive validations. Furthermore, we evaluate our",
      "page_hint": null,
      "token_count": 34,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S39",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "retrain the baseline AFFormer-T and report the segmentation and boundary result in terms of IoU and BIoU respectively. Quantitative results in Tab. IV show that our method signif- icantly improves semantic segmentation accuracy in various datasets, demonstrating the generalization ability. TABLE I: Semantic segmentation results on Cityscapes val dataset. LRFormer-T\u22c6: code of LRFormer is not available.",
      "page_hint": null,
      "token_count": 56,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S40",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "FCN [21] 9.8M 317G 61.5 11.2 PSPNet [22] 13.7M 423G 70.2 9.5 DeepLabV3+ [23] 15.4M 555G 75.2 8.2 SegFormer-B0 [2] 3.8M 125G 76.2 11.7 TopFormer-B [10] 5.1M 11.2G 75.2 55.6 PIDNet-S [32] 7.6M 47.6G 78.7 15.3 LRFormer-T\u22c6 [39] 13.0M 122.0G 80.7 - AFFormer-T [1] 2.2M 23.6G 76.2 27.8 AFFormer-B [1] 3.0M 33.5G 77.8 21.2 Mobile-Seed(Ours) 2.4M 31.6G 78.4 23.9 C. Ablation Studies and Insights Effectiveness of dual-task learning framework. We con- duct an ablation study to demonstrate thaxt our dual-task learn- ing framework is better than learning semantic segmentation task individually, as shown in Tab. V. This ablation experiment employs the single semantic stream S as the baseline [A] and test boundary stream B, boundary loss function Lb and dual-task",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S41",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "regularization loss Lreg, respectively. [B] shows that adding \u201dmulti-scale\u201d features from the boundary stream boosts the semantic segmentation performance. We do not refer to the \u201cmulti-scale\u201d features as \u201dboundary\u201d features because the boundary supervision is removed. [C] shows that explicitly supervising the boundary stream with the Lb leads to performance degradation, as the mIoU drops about 0.7%. This circumstance proves our suppose that applying distinctive supervision to different modules may harm the framework. [D] shows that our dual-task regularization loss Lreg could mitigate the learning divergence and promote the semantic segmentation and boundary detection tasks learning in a complementary way. Comparison of feature fusion methods. We conduct ablation studies to prove the effectiveness of our AFD. We take the semantic",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S42",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "stream S, boundary stream B and total loss L as baseline and check out the influence of feature fusion methods. We compare our AFD with fixed weight fusion methods, as addition and concatenation in previous approaches. Tab. VI shows that our AFD is more lightweight than concatenation and achieves better performance compared to both addition and concatenation. The results support our assumption that the fusion weights should be conditioned on the input, where our AFD dynamically assigns proper weights to each channel of semantic and boundary features and outperforms addition and concatenation. V. C ONCLUSION In this paper, we present a novel lightweight framework Mobile-Seed for joint semantic segmentation and boundary detection. Our method consists of a two-stream encoder and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S43",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "an active fusion decoder (AFD), where the encoder extracts semantic and boundary features respectively, and the AFD assigns dynamic fusion weights for two kinds of features. Moreover, regularization loss is introduced to alleviate the divergence in dual-task learning. We have implemented and evaluated our approach on various datasets and provided comparisons to other existing techniques. The experimental",
      "page_hint": null,
      "token_count": 57,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S44",
      "paper_id": "arxiv:2311.12651v3",
      "section": "results",
      "text": "LIAO et al.: MOBILE-SEED: JOINT SEMANTIC SEGMENTATION AND BOUNDARY DETECTION FOR MOBILE ROBOTS 7 Image Mobile-Seed Ground Truth AFFormer-T TopFormer-B Fig. 7: Qualitative results of the semantic boundary. TABLE II: Comparison class-aware semantic segmentation results to the baseline method. AFF-T is short of AFFormer-T. mIoU road s.walk build wall fence pole t-light t-sign veg terrain sky person rider car truck bus train motor bike mean AFF-T 98.2 85.3 92.5 54.7 57.6 63.9 70.1 78.5 92.7 66.3 94.8 81.1 60.0 94.7 70.2 80.0 69.5 61.7 75.9 76.2 Ours 98.3 85.9 92.8 61.8 58.7 66.7 71.6 79.6 92.9 65.9 95.1 82.0 61.4 94.9 78.9 85.8 77.9 63.0 77.5 78.4 TABLE III: Quantatitave results of semantic boundary on the Cityscapes val dataset. AFF-T",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S45",
      "paper_id": "arxiv:2311.12651v3",
      "section": "results",
      "text": "is short of AFFormer-T. Thrs Method road s.walk build wall fence pole t-light t-sign veg terrain sky person rider car truck bus train motor bike mean 3px AFF-T 81.3 61.0 66.1 47.8 47.4 66.1 65.5 66.9 68.2 53.6 78.8 57.6 65.2 74.7 77.7 85.1 91.3 77.5 61.2 68.0 Ours 84.2 66.8 72.0 57.0 53.4 73.6 68.5 70.1 73.8 59.5 82.2 61.1 66.5 79.3 81.0 88.2 95.2 76.8 62.6 72.2 5px AFF-T 86.9 70.1 75.2 50.5 50.1 72.9 71.8 74.9 78.6 57.6 85.5 65.7 69.9 82.7 78.8 86.3 91.6 78.8 68.7 73.5 Ours 88.6 74.2 80.0 59.4 56.2 78.6 74.0 76.4 82.8 63.0 87.9 68.2 71.2 85.7 81.8 89.3 95.6 78.4 69.8 76.9 9px AFF-T 90.7 76.8 82.5 53.2 53.0 77.1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S46",
      "paper_id": "arxiv:2311.12651v3",
      "section": "results",
      "text": "76.4 79.8 86.5 61.3 89.1 71.7 74.2 87.9 79.8 87.6 91.9 80.0 75.8 77.6 Ours 91.5 79.4 86.1 61.8 59.0 82.0 77.4 80.3 89.0 66.3 90.9 73.5 75.4 89.9 82.6 90.3 95.9 79.8 75.9 80.4 12px AFF-T 91.9 79.0 85.1 54.3 54.3 78.8 77.6 81.4 89.1 62.6 90.3 73.8 75.8 89.7 80.4 88.1 92.0 80.6 78.4 79.1 Ours 92.4 81.4 88.3 62.9 60.3 83.3 78.4 81.6 91.1 67.6 91.8 75.3 77.0 91.3 83.0 90.6 96.0 80.4 78.1 81.6 Fig. 8: (a) Semantic boundary results and (b) Binary boundary results on the Cityscapes val dataset. AFF-T-B means AFFormer-T for the binary boundary detection task. TABLE IV: Comparison with baseline method on CamVid and PAS- CAL Context datasets. PASCAL 59 and PASCAL60",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S47",
      "paper_id": "arxiv:2311.12651v3",
      "section": "results",
      "text": "mean PASCAL Context dataset with 59 and 60 categories, respectively. The threshold of BIoU is set to 3px.",
      "page_hint": null,
      "token_count": 18,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S48",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "mIoU BIoU mIoU BIoU mIoU BIoU AFFormer-T 71.6 41.2 45.7 20.7 41.4 14.9 Mobile-Seed 73.4 45.2 47.2 22.1 43.0 16.2 methods and support all claims made in this paper. We believe that the Mobile-Seed can be deployed on lightweight robotics platforms and serves for semantic SLAM, robot manipulation and other downstream tasks. (a) (b) (e)(d) (c) (f) Fig. 9: Visulization of multi-scale activation maps in boundary stream. (a) input image. (b) stage I. (c) stage II. (d) stage III. (e) stage IV . (f) final prediction. TABLE V: Ablation study on our dual-task learning framework. S means semantic stream, and B means boundary stream. Lb means boundary loss and Lreg means dual-task regularization loss. The threshold of BIoU metric is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S49",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "set to 3px. S B Lb Lreg mIoU BIoU [A] \u2713 76.2 41.3 [B] \u2713 \u2713 77.7 42.1 [C] \u2713 \u2713 \u2713 76.9 41.6 [D] \u2713 \u2713 \u2713 \u2713 78.4 43.3 REFERENCES [1] D. Bo, W. Pichao, and F. Wang, \u201cAfformer: Head-free lightweight semantic segmentation with linear transformer,\u201d in Proc. of the Conf. on Advancements of Artificial Intelligence (AAAI) , 2023. [2] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, \u201cSegformer: Simple and efficient design for semantic segmentation with 8 IEEE ROBOTICS AND AUTOMATION LETTERS. TABLE VI: Ablation study on feature fusion methods. \u2018ADD\u2019 means features addition, \u2018CAT\u2019 means features concatenation and \u2018AFD\u2019 means active fusion decoder proposed in our method. ADD CAT AFD",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S50",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "mIoU FLOPs FPS \u2713 77.7 0.96G 24.3 \u2713 78.0 1.91G 23.6 \u2713 78.4 0.96G 23.9 transformers,\u201d Proc. of the Advances in Neural Information Processing Systems (NIPS), vol. 34, pp. 12 077\u201312 090, 2021. [3] X. Chen, T. L \u00a8abe, A. Milioto, T. R \u00a8ohling, O. Vysotska, A. Haag, J. Behley, and C. Stachniss, \u201cOverlapNet: Loop Closing for LiDAR- based SLAM,\u201d in Proc. of Robotics: Science and Systems (RSS) , 2020. [4] C. Sautier, G. Puy, S. Gidaris, A. Boulch, A. Bursuc, and R. Marlet, \u201cImage-to-lidar self-supervised distillation for autonomous driving data,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 9891\u20139901. [5] J. Gao, C. Sun, H. Zhao, Y . Shen, D. Anguelov, C.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S51",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Li, and C. Schmid, \u201cVectornet: Encoding hd maps and agent dynamics from vectorized representation,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 11 525\u201311 533. [6] Y . Liao, J. Li, S. Kang, Q. Li, G. Zhu, S. Yuan, Z. Dong, and B. Yang, \u201cSe-calib: Semantic edges based lidar-camera boresight online calibration in urban scenes,\u201d IEEE Trans. on Geoscience and Remote Sensing, 2023. [7] M. Zhen, J. Wang, L. Zhou, S. Li, T. Shen, J. Shang, T. Fang, and L. Quan, \u201cJoint semantic segmentation and boundary detection using iterative pyramid contexts,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 13 666\u2013 13 675. [8]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S52",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "J. Zhang and S. Singh, \u201cLoam: Lidar odometry and mapping in real- time.\u201d in Proc. of Robotics: Science and Systems (RSS) , 2014, pp. 1\u20139. [9] X. Chen, A. Milioto, E. Palazzolo, P. Giguere, J. Behley, and C. Stach- niss, \u201cSuma++: Efficient lidar-based semantic slam,\u201d in Proc. of the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS) , 2019, pp. 4530\u20134537. [10] W. Zhang, Z. Huang, G. Luo, T. Chen, X. Wang, W. Liu, G. Yu, and C. Shen, \u201cTopformer: Token pyramid transformer for mobile semantic segmentation,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 12 083\u201312 093. [11] T. Takikawa, D. Acuna, V . Jampani, and S. Fidler, \u201cGated-scnn: Gated shape",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S53",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "cnns for semantic segmentation,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 5229\u20135238. [12] X. Li, X. Li, L. Zhang, G. Cheng, J. Shi, Z. Lin, S. Tan, and Y . Tong, \u201cImproving semantic segmentation via decoupled body and edge supervision,\u201d in Proc. of the Europ. Conf. on Computer Vision (ECCV), 2020, pp. 435\u2013452. [13] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d Proc. of the Int. Conf. on Learning Representations (ICLR) , 2021. [14] S. Mehta and M. Rastegari, \u201cMobilevit: Light-weight, general-purpose,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S54",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "and mobile-friendly vision transformer,\u201d in Proc. of the Int. Conf. on Learning Representations (ICLR) , 2022. [15] Q. Wan, Z. Huang, J. Lu, G. Yu, and L. Zhang, \u201cSeaformer: Squeeze- enhanced axial transformer for mobile semantic segmentation,\u201d in Proc. of the Int. Conf. on Learning Representations (ICLR) , 2023. [16] Z. Yu, C. Feng, M.-Y . Liu, and S. Ramalingam, \u201cCasenet: Deep category-aware semantic edge detection,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2017, pp. 5964\u20135973. [17] Y . Hu, Y . Chen, X. Li, and J. Feng, \u201cDynamic feature fusion for semantic edge detection,\u201d in Proc. of the Intl. Conf. on Artificial Intelligence (IJCAI), 2019. [18] Y . Liu, M.-M. Cheng, D.-P.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S55",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Fan, L. Zhang, J.-W. Bian, and D. Tao, \u201cSemantic edge detection with diverse deep supervision,\u201d Intl. Journal of Computer Vision (IJCV) , vol. 130, no. 1, pp. 179\u2013198, 2022. [19] X. Xiao, Y . Zhao, F. Zhang, B. Luo, L. Yu, B. Chen, and C. Yang, \u201cBaseg: Boundary aware semantic segmentation for autonomous driv- ing,\u201d Neural Networks, vol. 157, pp. 460\u2013470, 2023. [20] Y . Liu, M.-M. Cheng, X. Hu, K. Wang, and X. Bai, \u201cRicher convolu- tional features for edge detection,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2017, pp. 3000\u2013 3009. [21] J. Long, E. Shelhamer, and T. Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in Proc. of the IEEE/CVF Conf.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S56",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "on Computer Vision and Pattern Recognition (CVPR) , 2015, pp. 3431\u20133440. [22] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \u201cPyramid scene parsing network,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 2881\u20132890. [23] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, \u201cEncoder- decoder with atrous separable convolution for semantic image segmen- tation,\u201d in Proc. of the Europ. Conf. on Computer Vision (ECCV) , 2018, pp. 801\u2013818. [24] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, \u201cMobilenets: Efficient convo- lutional neural networks for mobile vision applications,\u201d arXiv preprint arXiv:1704.04861, 2017. [25] X. Zhang, X. Zhou, M.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S57",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "Lin, and J. Sun, \u201cShufflenet: An extremely efficient convolutional neural network for mobile devices,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6848\u20136856. [26] R. P. Poudel, S. Liwicki, and R. Cipolla, \u201cFast-scnn: Fast semantic segmentation network,\u201d in Proc. of British Machine Vision Conference (BMVC), 2019. [27] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, \u201cSwin transformer: Hierarchical vision transformer using shifted windows,\u201d in Proc. of the IEEE/CVF Intl. Conf. on Computer Vision (ICCV), 2021, pp. 10 012\u201310 022. [28] S. Tang, T. Sun, J. Peng, G. Chen, Y . Hao, M. Lin, Z. Xiao, J. You, and Y . Liu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S58",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "\u201cPp-mobileseg: Explore the fast and accurate semantic seg- mentation model on mobile devices,\u201d arXiv preprint arXiv:2304.05152, 2023. [29] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770\u2013778. [30] D. Acuna, A. Kar, and S. Fidler, \u201cDevil is in the edges: Learning semantic boundaries from noisy annotations,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2019, pp. 11 075\u201311 083. [31] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, \u201cMobilenetv2: Inverted residuals and linear bottlenecks,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2018,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S59",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "pp. 4510\u20134520. [32] J. Xu, Z. Xiong, and S. P. Bhattacharyya, \u201cPidnet: A real-time semantic segmentation network inspired by pid controllers,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2023, pp. 19 529\u201319 539. [33] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- nenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2016, pp. 3213\u20133223. [34] G. J. Brostow, J. Fauqueur, and R. Cipolla, \u201cSemantic object classes in video: A high-definition ground truth database,\u201d Pattern Recognition Letters, vol. 30, no. 2, pp. 88\u201397, 2009. [35] R. Mottaghi, X. Chen,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S60",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille, \u201cThe role of context for object detection and semantic segmentation in the wild,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014, pp. 891\u2013898. [36] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \u201cImagenet: A large-scale hierarchical image database,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , 2009, pp. 248\u2013255. [37] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in Proc. of the Int. Conf. on Learning Representations (ICLR) , 2017. [38] B. Cheng, R. Girshick, P. Doll \u00b4ar, A. C. Berg, and A. Kirillov, \u201cBound- ary iou: Improving",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2311_12651v3:S61",
      "paper_id": "arxiv:2311.12651v3",
      "section": "method",
      "text": "object-centric image segmentation evaluation,\u201d in Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recog- nition (CVPR), 2021, pp. 15 334\u201315 342. [39] Y .-H. Wu, S.-C. Zhang, Y . Liu, L. Zhang, X. Zhan, D. Zhou, J. Feng, M.-M. Cheng, and L. Zhen, \u201cLow-resolution self-attention for semantic segmentation,\u201d arXiv preprint arXiv:2310.05026 , 2023.",
      "page_hint": null,
      "token_count": 55,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9493346983868929,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 8,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5415,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5743,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5029,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4665,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5645,
        "empty": false
      },
      {
        "page": 6,
        "chars": 5295,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3351,
        "empty": false
      },
      {
        "page": 8,
        "chars": 8551,
        "empty": false
      }
    ],
    "quality_score": 0.9493,
    "quality_band": "good"
  }
}