{
  "paper": {
    "paper_id": "arxiv:2206.08662v3",
    "title": "PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile Devices",
    "authors": [
      "Xiang Yang",
      "Zikang Xu",
      "Qi Qi",
      "Jingyu Wang",
      "Haifeng Sun",
      "Jianxin Liao",
      "Song Guo"
    ],
    "year": 2022,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Distributing the inference of convolutional neural network (CNN) to multiple mobile devices has been studied in recent years to achieve real-time inference without losing accuracy. However, how to map CNN to devices remains a challenge. On the one hand, scheduling the workload of state-of-the-art CNNs with multiple devices is NP-Hard because the structures of CNNs are directed acyclic graphs (DAG) rather than simple chains. On the other hand, distributing the inference workload suffers from expensive communication and unbalanced computation due to the wireless environment and heterogeneous devices. This paper presents PICO, a pipeline cooperation framework to accelerate the inference of versatile CNNs on diverse mobile devices. At its core, PICO features: (1) a generic graph partition algorithm that considers the characteristics of any given CNN and orchestrates it into a list of model pieces with suitable granularity, and (2) a many-to-many mapping algorithm that produces the best pipeline configuration for heterogeneous devices. In our experiment with 2 ~ 8 Raspberry-Pi devices, the throughput can be improved by 1.8 ~ 6.8x under different CPU frequencies.",
    "pdf_path": "data/automation/papers/arxiv_2206.08662v3.pdf",
    "url": "https://arxiv.org/pdf/2206.08662v3",
    "doi": null,
    "arxiv_id": "2206.08662v3",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.077821+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2206_08662v3:S1",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "1 PICO: Pipeline Inference Framework for Versatile CNNs on Diverse Mobile Devices Xiang Y ang, Zikang Xu, Qi Qi, Jingyu Wang, Haifeng Sun, Jianxin Liao, and Song Guo,Fellow, IEEE Abstract\u2014Distributing the inference of convolutional neural network (CNN) to multiple mobile devices has been studied in recent years to achieve real-time inference without losing accuracy. However, how to map CNN to devices remains a challenge. On the one hand, scheduling the workload of state-of-the-art CNNs with multiple devices is NP-Hard because the structures of CNNs are directed acyclic graphs (DAG) rather than simple chains. On the other hand, distributing the inference workload suffers from expensive communication and unbalanced computation due to the wireless environment and heterogeneous devices. This paper presents PICO,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S2",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "a pipeline cooperation framework to accelerate the inference of versatile CNNs on diverse mobile devices. At its core, PICO features: (1) a generic graph partition algorithm that considers the characteristics of any given CNN and orchestrates it into a list of model pieces with suitable granularity, and (2) a many-to-many mapping algorithm that produces the best pipeline configuration for heterogeneous devices. In our experiment with 2 \u223c 8 Raspberry-Pi devices, the throughput can be improved by 1.8 \u223c 6.8\u00d7 under different CPU frequencies. Index Terms\u2014Mobile Computing, Pipeline Inference, Model Deployment. \u2726 1 I NTRODUCTION R ECENT years witness an explosive growth of mobile devices. The huge number of mobile devices provides a large volume of data (images, videos, etc.). Meanwhile,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S3",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "ver- satile convolution neural networks (CNN) with pre-trained parameters become powerful tools to make intelligent de- cisions using these raw data ( CNN inference ). Embedding CNN with mobile devices enables many intelligent appli- cations to become reality, such as smart home, intelligent factory, and even automatic driving [1], [2]. One obstacle to the embedding is the resource-limited mobile devices. Compared with datacenter, the computing capability of mobile devices is not enough to perform CNN inference alone. But on the contrary, the current wireless network is not prepared for transmitting the massive vol- ume of raw data collected by these mobile devices. For example, an autopilot camera could capture more than 700 MB video record every second [3], and uploading",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S4",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "all the video data to the datacenter will bring significant network latency. Moreover, uploading data from user devices to the cloud always brings concern about privacy [3]. Benefitted from the spatial independence of convolution operation, the input and output ( feature) of convolutional (conv) layers can be split into several small tiles and executed on different devices. As a consequence, cooperative CNN inference on multiple mobile devices gains the attention of researchers recently [4], [5], [6]. During inference procedure, the data source (camera, sensors, etc.) captures raw data and splits it into tiles. These tiles are distributed to multiple \u2022 Xiang Yang, Zikang Xu, Qi Qi, Jingyu Wang, Haifeng Sun and Jianxin Liao are with the State Key Laboratory of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S5",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "Networking and Switching Technology, Beijing University of Posts and Telecommuni- cations. E-mail: {yangxiang, xuzikang, qiqi8266, wangjingyu, hfsun, liaojx}@bupt.edu.cn. \u2022 Song Guo is an IEEE Fellow (Computer Society) and an ACM Distin- guished Member with the Department of Computing at The Hong Kong Polytechnic University. E-mail: cssongguo@comp.polyu.edu.hk \u2022 Qi Qi and Jingyu Wang are the corresponding authors. nearby mobile devices through a wireless network and executed independently using one or several layers. Then the data source is responsible to gather all the output tiles and stitch them to obtain the result. The procedure will be iterated multiple times until all layers are executed. Cooperative inference also protects user privacy since all the data stay in local. Moreover, the closer to the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S6",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "data source, the lower network latency it suffers. Compared with other strategies such as model compression and parameter pruning [7], [8], [9], cooperative inference neither losses the inference accuracy nor requires re-train the model. However, despite all these benefits, there still leave some challenges that are not completely solved in previous works. Although the input feature can be parallel executed, (1) the parallelism introduces redundant calculation due to the property of CNN. The scalar in the output feature of one conv layer is calculated through a dot product with the conv kernel and a subregion of input feature. For most cases, the kernel size is bigger than 1 \u00d7 1, so that the input tiles of partitioned input feature will",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S7",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "overlap with each other to guarantee the scalars at the edge of output tiles are correct. Moreover, the overlapped part will increase recursively when devices execute multiple layers during one iteration in the inference procedure, but the communication is expen- sive for mobile devices. As a consequence, the executed lay- ers need to be carefully chosen. However, (2) the structures of many CNNs are directed acyclic graphs (DAG) rather than chains. ResNet34 [10] uses skip-connection technology that allows a layer to directly connects to another deeper layer. The structure of InceptionV3 [11] contains multiple branches to capture more information from the input fea- ture. These complex structures lead to a huge number of possible choices. Previous works mainly focused on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S8",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "the chain structure [4], [5], [6], which is much easier than DAG. Compared with datacenter, (3) the computing resources of mobile devices are diverse, the heterogeneous environment also hinders the optimization for cooperative inference. arXiv:2206.08662v3 [cs.DC] 25 Mar 2024 2 Stage 1 Stage 2 Stage 3 4 3 2 14 3 2 14 3 2 1 Processing CompletedWaiting 555Inference Tasks CNN Segments Heterogeneous Environment Fig. 1: A diagrammatic sketch of pipeline inference. In this paper, we explore previous works about paral- lelizing the CNN inference and propose a pipeline coop- eration (PICO) framework for accelerating the inference on diverse mobile devices. Fig. 1 plots a diagrammatic sketch of our framework. PICO divides the entire CNN graph and mobile devices into",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S9",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "3 stages. These stages compose an efficient inference pipeline. Since each stage owns a small segment of original CNN and a subset of mobile devices, both communication overhead and the redundant calcu- lation can be significantly reduced. There are two import metrics for pipeline: latency and period. The first term is the sum of inference latencies of all stages and the last term is the longest latency among stages. Obviously decreasing the period tends to increase the latency. Our optimization goal is to minimize the redundant calculation and period (maximize throughput) meanwhile to keep the latency of the pipeline under a certain value. We first formulate the pipeline inference, then we ana- lyze the complexity of the optimization problem and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S10",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "find that it is NP-Hard to directly obtain the optimal result. Based on our analysis, PICO uses a two-step optimization to maximize the throughput. In the first step, we orchestrate the CNN graph into a sequence of pieces. These pieces have the minimum redundant calculation inside and compose the original CNN graph in a chain structure. Then we choose the best partition for these pieces and devices to construct the inference pipeline. The algorithms used in the above procedures are based on dynamic programming. In our experiment we use 2 \u223c 8 Raspberry-Pi devices to evaluate PICO framework. The throughput can be improved by 1.8 \u223c 6.8\u00d7 under different CPU frequencies and number of devices. In a nutshell, we make",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S11",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "the following contributions: \u2022 We present a pipeline cooperation (PICO) framework to accelerate CNN inference with diverse mobile devices. \u2022 We propose an algorithm to split the complex CNN graph structure into more fine-grained pieces. \u2022 We propose an algorithm to decide the optimal stage settings for inference pipeline which maximize the throughput. \u2022 We apply our technique on a cluster consisting of Raspberry-Pi-based hardware and evaluate image recognition and object detection CNN models. The rest of this paper is organized as follows: Section 2 provides background information of CNN and different parallelization strategies in mobile devices. Section 3 for- mulates the inference process and gives a cost model for further optimization. Section 4 and 5 describe our approach to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S12",
      "paper_id": "arxiv:2206.08662v3",
      "section": "body",
      "text": "find near-optimal parallelization. Section 6 presents the",
      "page_hint": null,
      "token_count": 7,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S13",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "and Section 8 concludes. 2 B ACKGROUND AND MOTIVATION In this section, we briefly introduce the CNN inference and the current parallel schemes. Then we propose our pipeline cooperation scheme. 2.1 Procedure of CNN Inference The convolution layer (conv) is the key module during CNN inference, Each conv layer owns a set of kernels. To produces the output feature, conv layers use their kernels to slide over the input feature received from the previous layer. Every movement of the kernel will produce a scalar in the output feature by a dot product between the weights of kernels and a small subregion of the input. The pooling layer (pool) performs a down-sampling operation. It is used to progressively reduce the number",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S14",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "of parameters, memory footprint and amount of computation in the network. Conv operation is the biggest bottleneck. Fig. 2 plots the computation and communication percentage by layer for two classic CNNs (VGG16 [12], YOLOv2 [13]). From the figure we can find conv layers dominate the consumption of computing resources. The conv operations occupy 99.19% of the computation in VGG16 and 99.59% in YOLOv2. How to efficiently execute conv operations is the key to accelerat- ing CNN inference. Another finding is the variation. Since 3 (a) VGG16 (b) YOLOv2 Fig. 2: The communication and computation percentages of each layer. different conv layers have different configurations (kernel size, padding, in and out channels), the communication or computation percentage also varies. 2.2 Parallelizing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S15",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "CNNs With Mobile Devices Benefitted from the spatial independence of conv opera- tions, the inference can be parallel executed by splitting the input feature into multiple tiles and distributing them to different mobile devices, as shown in Fig. 4. We refer this technology as feature partition . However, the partition of input feature overlaps with each other due to the prop- erty of conv operations. In Fig. 4, an input feature is split into four tiles and distributed to four devices. Assume the corresponding conv layer has a 3 \u00d7 3 kernel size, to obtain the correct value in P1, the calculation with 3 \u00d7 3 kernel has to use more proportion (the edges of the yellow and pink region)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S16",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "of the input feature. This property leads to a redundant calculation and increases the difficulty of the design of parallel algorithm. We next introduce the two parallelization schemes used in this paper. [4] is the first work that uses feature partition for cooperative CNN inference. For each layer, the basic idea is to split the input feature into tiles and distribute them to all devices, then gather them to obtain the output of this layer. We refer such a scheme as layer-wise parallelization. In a WLAN network, it can cause substantial network la- tency. The gain of layer-wise parallelization is significantly defeated by communication overhead. To reduce the com- munication among devices, fused-layer parallelization was introduced in [5] and [6].",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S17",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "This scheme fuses multiple layers instead of distributing the computation of every layer indi- vidually. Thus, mobile devices can execute the calculation of multiple layers without communication. But since the input will go through multiple layers, to obtain the correct value of output feature, the overlapped part of the input increases recursively. In addition, all mobile devices need a full copy of original CNN for the two schemes, which increases the memory footprint. 2.3 The Structures Of CNNs The structures of CNNs can be divided into three categories. We plot Fig. 3 to give an illustration. Note the norm layer and activation layer are ignored since they do not change the input and output shape and has less proportion of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S18",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "computation. The earlier model such as VGG16 [12] and YOLOv2 [13] are built with the (1) chain structures. This structure is simple: neural layers inside the model are connected one by one, and the output of the previous layer is the input of the next layer. We plot the model structure of VGG16 in Fig. 3a for further explanation. Later, the (2) block structure becomes popular in CNNs. Block structure enables CNNs to capture multiple features of input data to improve its performance using carefully designed blocks [11] and prevent the vanishing gradient problem when training deeper model [10]. It uses blocks to replace the layers in chain structure. All the blocks are still connected one by one, but inside",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S19",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "the block, neural layers can be represented as an acyclic directed graph (DAG). Fig. 3b plots the 8th and 9th blocks in InceptionV3 [11]. Each block has multiple branches and contains several conv and pool layers, and these blocks are connected with the Contact operations that stacks the output of every sink layer of previous block in channel dimension and feeds the result to the next block. To avoid manual design of the model structure, neural architecture search (NAS) is proposed. Compared with the previous two structures, the output structure of NAS is usually a complete graph, which can not be divided into sequence of blocks. We refer the structure as (3) graph structure. We plot a partition of NasNetMobile",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S20",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "[14] in Fig. 3c, which has two source layers and three sink layers. The complex structure of CNN models is a big challenge for optimizing parallel strategy. 2.4 Motivation And Pipeline Inference 2.4.1 Motivation In the above discussion, How to tackle the complex graph structure of CNN model and how to reduce the redundant calculation are keys to accelerating the CNN inference. Tackle the complex structure: Cooperative inference needs to distribute the CNN model into multiple devices, but the structure of these model is complex and prevent more fine-grained optimization. Lots of previous works focus [4], [5], [6] on cooperative inference, they only consider chain structure. There lacks a solution for the more complex block and graph structures. Reduce the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S21",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "redundant calculation: Communication is expensive in mobile environment. For layer-wise scheme, frequent communication among mobile devices causes in- efficient performance. The redundant calculation also lim- its the cooperation of mobile devices for CNN inference. For fused-layer scheme, the redundant calculation quickly grows as the number of fused layers or devices increases. 4 3x3 Conv 3x3 Conv 2x2 Pool 3x3 Conv 3x3 Conv 3x3 Conv 3x3 Conv 2x2 Pool 3x3 Conv 3x3 Conv 2x2 Pool 2x2 Pool (a) Chain Structure (VGG16) 1x1 Pool 1x1 Conv 1x7 Conv 7x1 Conv 1x7 Conv 7x1 Conv 1x1 Conv 1x7 Conv 7x1 Conv 1x1 Conv Concat Concat 1x1 Conv 1x7 Conv 7x1 Conv 3x3 Conv 1x1 Conv 3x3 Conv 2x2 Pool Concat (b) Block Structure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S22",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "(InceptionV3) 1x1 Conv 7x7 Conv 7x7 Conv 7x7 Conv 1x1 Conv 7x7 Conv 1x1 Conv 5x5 Conv 1x1 Conv 5x5 Conv 1x1 Conv 5x5 Conv 1x1 Conv 5x5 Conv 1x1 Conv 2x2 Pool 1x1 Conv 3x1 Conv 1x1 Conv 2x2 Pool Add Add Concat (c) Graph Structure (NasNetMobile) Fig. 3: CNNs with different structures: The chain structure is the simplest one which just put the neural layers into a sequence. Block structure replaces the element in chain structure from neural layer to block, each block can be seen as a directed acyclic graph (DAG). Graph structure can not be partitioned into blocks, the entire structure is a huge DAG. Fig. 4: Feature map partition strategy introduces redundant calculation. 1 x11",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S23",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "x22 x22 x33 x3024681012141618Computation overhead per device (GFLOPs) 5xConv+2xPool 10xConv+3xPool 13xConv+5xPool (a) Per device overhead 1 x11 x22 x22 x33 x31.0x1.5x2.0x2.5x3.0x3.5x4.0x4.5x5.0xTotal computation overhead 5xConv+2xPool 10xConv+3xPool 13xConv+5xPool (b) Total computation overhead Fig. 5: Computation overhead with different partition set- tings. To give a detailed explanation, we evaluate the required floating-point operations (FLOPs) for VGG16 with different numbers of fused layers and mobile devices. Fig. 5a presents the FLOPs per device meanwhile Fig. 5b shows the sum of FLOPs of all devices. We can find that fused-layer strategy performs well at the start, but when the numbers of fused layers or devices increase, the redundant calculation quickly grows. 2.4.2 Pipeline Inference From the above discussion, the acceleration of CNN infer- ence faces",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S24",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "challenges when the number of devices or fused layers grows. For layer-wise scheme, the devices are idle in most time due to frequent communication and expensive network latency. On the contrary, devices can keep running with the fused-layer scheme, but it is whistling to the wind since the most computation is redundant. Here we introduce the pipeline scheme for parallelizing the CNN inference. This scheme divides both layers and mobile devices into several groups, as shown in Fig. 1. We refer such a group as stage in our description. The inference inside the stage uses fused- layer scheme and the entire CNN inference is performed stage by stage. If we set the number of stages to 1, The fused-layer scheme",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S25",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "is a special case of the pipeline scheme. To maximize the inference throughput, the inference latency of every stage should be optimized as close as possible. Using pipeline for inference has several advantages. (1) First, the communication overhead can be reduced since the calculated features only need to be synchronized among a subset of devices. (2) Second, the proportion of redundant calculation also decreases due to smaller numbers of layers and devices. (3) Third, each device owns a segment of CNN instead of the entire model, which reduces the memory footprint. The concept of pipeline is widely adopted in task scheduling [15], [16] which maps multiple processors to an application composed of several tasks. However, pipeline meets difficulties when applied",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S26",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "to CNN inference. The structure of CNN is a directed acyclic graph (DAG) rather than a chain, the mapping has to consider the data flow of DAG. Generally, the number of layers in CNN is more than the number of devices, thus the mapping is many-to-many, and different mapping strategy also changes redundant calculation. Moreover, the heterogeneous environment is also a big challenge. 5 TABLE 1: Notation definitions Notation Description G : (V, E) CNN with graph structure. D A heterogeneous cluster with D devices. li Layer i in model G. wi, hi The width and height of the output frame of li. ki, pi, si, ci Kernel size, padding, stride, and channel of li. dk A device in cluster",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S27",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "D. Fk i Input feature frame of layer li for device dk. Fk Set of all input feature for layers assigned to dk. Fk in Input feature for source layers assigned to dk. Fk out Output feature for sink layers assigned to dk. b(dh, dk) Bandwidth between device dh and device dk. D A subset of devices. M : (V, E) Model partition deployed on dk \u2208 D. S : (M, D) A stage that belongs to the inference pipeline. ME Ending piece of CNN G. \u03c6(Fk) Input frame size of Fk. \u03b8(M; Fk) Required computing resources of M. \u03d1(dk) Computing capacity of device dk. tcomm(dk, Fk) Communication time of device dk. tcomp(df ,dk,Fk) Computation time of device dk. T(S)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S28",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "Time overhead for executing stage S. Tcomm(S) Communication time of stage S. Tcomp(S) Computation time of stage S. Tlim Inference latency limit for optimization. S Pipeline configuration containing all stages S. S\u22c6 Optimal stage configuration. T (G, D, S) Latency of the pipeline under configuration S. P(G, D, S) Period of the pipeline under configuration S. 3 S YSTEM MODEL In this section, we define our optimization problem for pipeline inference. 3.1 Problem Define Generally speaking, our goal is to divide both CNN model with graph structure and mobile devices with heteroge- neous computing resources into several stages properly, so that these stages could compose an inference pipeline that maximizes the throughput. 3.1.1 CNN With Graph Structure We use an",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S29",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "acyclic directed graph (DAG) G :< V, E > to represent a given CNN model. The vertex set V contains all the neural layers and connector (e.g., Add and Contact in Fig. 3) li \u2208 V, and the elements (li, lj) in the edge set E denotes the data flow of CNN model G. In particular, (li, lj) \u2208 Emeans the output of layer li is the input of layer lj. Since the CNN model will be executed as an inference pipeline with multiple stages, the G also needs to be split into multiple parts. We refer these parts as segments. A segment M :< V, E > is a subset of original DAG G, where V \u2286V and E",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S30",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "\u2286E. Note the segment is not a regular smaller graph, since the edge set E contains some vertices that are not included by V. Take Fig. 1 as an example, these segments on the top also contain edges that are connected with previous or next segments. Here we give some definitions of segment to simply our following modeling: Definition 1. A subset M :< V, E > is a segment of original graph G :< V, E > if for all e : (u, v) \u2208 E, once u or v belongs to V, e also belongs to E. Definition 2. For a segment M :< V, E > and an edge (u, v) \u2208 E, if u /\u2208 V,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S31",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "then v is a source vertex of M. Definition 3. For a segment M :< V, E > and an edge (u, v) \u2208 E, if v /\u2208 V, then u is a sink vertex of M. 3.1.2 Optimization Goal Given a heterogeneous cluster D, where dk \u2208 D is a comput- ing device in the cluster. We assume the computing capacity \u03d1(dk) of device dk are known. In our practice, the \u03d1(dk) denotes floating point operations per second (FLOPS). We also assume the bandwidth between all mobile devices is the same and is known as b. This assumption covers most cases when these devices are under the same WLAN environment such as home and factory [6], [15]. For pipeline",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S32",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "scheme, D \u2286D is a subset of heterogeneous devices. Each device dk \u2208 Downs a copy of model segment M but is assigned to produce different region Fk of the output feature map of all the sink vertex in M. We use F to present the set of all Fk in D. A stage S can be represented as a tuple (M, D, F). Let S denote the set of stages composed by all the stages S we defined above, the optimization objective is to find such a S\u22c6 that satisfies: S\u22c6 = arg min T (G,D,S)\u2264Tlim P(G, D, S) (1) where T (G, D, S) denotes the pipeline latency under spe- cific stage configuration S and P(G, D, S)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S33",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "is the period of pipeline. Tlim is a hyperparameter that indicates the maximum inference latency we can accept. 3.2 Cost Model Here we represent our cost model to guide the optimization. First, we quantify the essential input feature size for every device in a stage. Then, we formulate the inference latency of every stage. Finally, we get the inference period and latency of the entire pipeline using previous results. 3.2.1 The Input Feature Size For Devices Every device dk owns a segment M :< V, E > and needs to produce correct output features Fk. Once the M and Fk is given, we need to calculate the necessary input feature size for every layer li \u2208 M. The calculation had",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S34",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "been discussed in [5], but it only considered models with chain structure. We will extend it into a more complex graph structure here with a top-down algorithm. To calculate the input feature size of layer li, we need to find all the edges (li, lj) start from li. We can assume the input feature sizes of all lj is already calculated. Since the input of lj is just the output of li, the necessary output feature size of li can be denoted as: wi = max {wi\u2192j}, hi = max {hi\u2192j}. (2) Here we use wi and hi to denote the necessary width and height of the output feature size of li, meanwhile, wi\u2192j and hi\u2192j is the input size",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S35",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "of layer lj. Assume layer li has kw i \u00d7 kh i kernel size and si stride size, once the output feature size is determined, the height hi and width wi of input feature can be calculated using the following equation: w\u2217\u2192i = (wi \u2212 1)si + kw i , h\u2217\u2192i = (hi \u2212 1)si + kh i (3) 6 where w\u2217\u2192i and h\u2217\u2192i is the input feature size for li. Note this formula suits for both conv and pool layers. Since the output feature size of all sink vertices of M is given (corresponding to Fk), we can iteratively calculate all the output and input feature size of all layers in M with a top-down algorithm. The input feature",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S36",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "size of all the source vertices of M is the input feature size needed by device dk. 3.2.2 Inference Cost Of Devices We use f(li; Fk i ) to denote the required floating operations (FLOPs) of conv layer li when generating an output feature map Fk i with size ci \u00d7 wi \u00d7 hi. Assume layer li is a conv layer with c\u2032 i \u00d7 kw i \u00d7 kh i kernel size, ci output channel and si stride size. Since each floating scalar in the output feature is calculated by sliding the kernel over the input feature, f(li; Fk i ) can be given by: f(li; Fk i ) = kw i kh i c\u2032 iwihici. (4) Here we ignore",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S37",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "the pool layers since they require far fewer FLOPs than conv layers (In Fig. 2). Note the wi and hi in Eq. (3) denote the region of correct output feature. However, the Fk i is the actual output feature size, which contains not only the correct output but also some redundant parts at the margin of Fk i . Assume the size of Fk i is known and (li, lj) is an edge in M, the output Fk j of layer lj can be calculated by: wj = wi + 2pw j \u2212 kw j sw j + 1, hj = hi + 2ph j \u2212 kh j sh j + 1 (5) where pw j and ph j is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S38",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "the padding size of conv layer lj. Since we have the input feature size for all source vertices in M, to calculate the Fk i for all layer li \u2208 M, we use a bottom-up algorithm similar with above, omit here. Assume a device dk is responsible to produce Fk with model segment M, we can give the required FLOPs opera- tion \u03b8(M; Fk) with: \u03b8(M; Fk) = X lj\u2208M f(lj; Fk j ). (6) Empirical studies by [17] have demonstrated that for specific layers and device, the computation time is proportional to the size of the input or output features, Therefore, the inference time tcomp(dk, Fk) for device dk can be estimated by the following equation: tcomp(dk, Fk)) =",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S39",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "\u03b1k \u03b8(M; Fk) \u03d1(dk) (7) where \u03d1(dk) is the computing capacity (FLOPS) of device dk. \u03b1k is a coefficient computed by a regression model. 3.2.3 The Period and Latency Of Pipeline As each device executes inference in parallel within stage, the computation time for stage S :< M, D > is determined by the maximum inference time among devices in D: Tcomp(S) = max dk\u2208D tcomp(dk, Fk). (8) Since each device dk \u2208 D will generate part of the calculation of stage S, there exists a device df which is TABLE 2: Optimization Complexity Device Model Chain Block Graph Homogeneous P NP * NP Heterogeneous NP NP NP * [6] solves the optimization by considering the entire block as a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S40",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "special layer. However, this operation introduces lots of unnecessary calculations during inference. responsible to distribute stage input and gather stage out- put from other devices. For a device dk \u2208 S, the feature transferring time tcomm(df , dk, M) can be given by: tcomm(df , dk, F) = \u03c6(Fk in) + \u03c6(Fk out) b(df , dk) (9) where \u03c6(F) is the feature size on a given input feature sizes F. Here we use Fk in and Fk out to denote the input and output feature sizes of M owned by dk. Sum the communication cost for each device dk in stage S, we define Tcomm(S) = X dk\u2208D dk\u0338=df tcomm(df , dk, Fk) (10) as the communication cost of stage",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S41",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "S. The cost function for each stage in pipeline inference is then defined as the total time of the frame transfer and layer computation: T(S) = Tcomp(S) + Tcomm(S) (11) Note the time for feature map partition and stitch is not discussed here. In practice, it is far less than the layer computation time Tcomm(S) and could be ignored. Next, we define the optimization objective as: P(G, D, S) = max S\u2208S T(S), T (G, D, S) = X S\u2208S T(S) (12) where P(G, D, S), T (G, D, S) estimate the maximum execu- tion time of stages in and inference latency in pipeline. 3.3 Analysis The goal of our optimization algorithm is finding the best stage set S\u22c6 that",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S42",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "minimizes the maximum period P(G, D, S) of pipeline with heterogeneous clusters. Such an optimiza- tion faces the following challenges: \u2022 The overhead of computation and communication of each layer in model varies and would be affected by the assigned feature map size Fk i . \u2022 The computing capacity \u03d1(dk) of every device in the heterogeneous cluster varies. \u2022 For a specific stage S, the number of devices |D| and the model segment M in stage also need to be configured. \u2022 The structure of CNN model can be complex and hard to be partitioned. In fact, we show the optimal solution can not be found in polynomial time unless P = NP . Theorem 1. Given a CNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S43",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "model G with chain structure, the problem of minimizing maximum stage execution time 7 P(G, D, S) with heterogeneous mobile devices under a constriction that T (G, D, S) \u2264 Tlim is NP-hard. Proof 3.1. Considering a scheduling problem defined as follows: Given L identical tasks that are needed to be executed one by one. All tasks can be paralleled to sev- eral processors without additional overhead. The goal is to assign these tasks to D heterogeneous devices and maximize the throughput. This problem is proven to be NP-hard by [16]. We can construct a CNN model with chain structure whose layers are identical and the kernel size of each layer is 1 \u00d7 1. This kernel size guar- antees",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S44",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "there is no overlapped partition when parallels the inference. If there exists a polynomial solution for this CNN model, obviously it can also be applied to the above task assignment problem. Thus, the optimization of P(G, D, S) is NP-hard. Here complete the proof. Theorem 2. Given a CNN model G with graph structure, the problem of minimizing maximum stage execution time P(G, D, S) with homogeneous mobile devices under a constriction that T (G, D, S) \u2264 Tlim is NP-hard. Proof 3.2. We begin with introducing the problem of most balanced st-edge cuts (MBSTC). Given a graphG, found- ing an edge cut [G, \u00afG], which minimize max \b |G|, | \u00afG|",
      "page_hint": null,
      "token_count": 112,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S45",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "is NP-Hard [18]. Obviously, MBSTC is a special case of our optimization when the number of stages is 2. Thus, the problem is NP-Hard. Given a heterogeneous edge environment, Theorem 1 shows find the optimal solution for CNNs with chain struc- ture is NP-Hard. Theorem 2 shows that for CNNs with graph structure, even homogeneous edge environment is NP-Hard. We summarize the result in Table 2, almost every sit- uation is NP-Hard for optimization except chain structure model with homogeneous devices. 4 O RCHESTRATE THE MODEL STRUCTURE In this section, we introduce our strategy to orchestrate the complex block and graph structures. 4.1 Insight Ideally, we hope to directly divide CNN model G and mobile devices D into several stages",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S46",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "S. However, we can find there is no polynomial solution for G with block and graph structures from Table 2, neither with homogeneous nor heterogeneous environment. The only feasible situation to find the optimal strategy S is that the structure of model is a chain. For block structure, a simple trade-off is to consider every block as a special layer. So that it could be optimized in polynomial time. However, this scheme introduces lots of redundant calculation inside blocks and can not be applied on these models with graph structures. Fig. 6 shows an extreme case of such a scheme. Considering a block with only two conv layers (la, lb). The kernel size of la is 1\u00d77 but the kernel",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S47",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "size of lb is 7 \u00d7 1. For layer la, according to Eq. (3) there is no redundant calculation on its width dimension since kw a = 1 (assuming the stride size is 1). Similarly, there is no redundant calculation on its length dimension for layer 1x7Conv7x1ConvInputOutput1x7Conv7x1ConvInputOutput (a) 1x7Conv7x1ConvInputOutput1x7Conv7x1ConvInputOutput (b) Fig. 6: A extreme case for a block with two layers. (a): consider the entire block as a special layer. (b): partition the block into more fine-grained pieces. BEGHDFCA BEGHDFCA BEGHDFCA BEGHDFCA (a) BEGHDFCA BEGHDFCA BEGHDFCA BEGHDFCA (b) BEGHDFCA BEGHDFCA BEGHDFCA BEGHDFCA (c) BEGHDFCA BEGHDFCA BEGHDFCA BEGHDFCA (d) Fig. 7: The illustration of ending pieces. (a): The origi- nal graph G. (b): {E, G, H} is an ending piece of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S48",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "G. (c): {E, F, H} is not an ending piece of G. (d): A partition of G using ending piece iteratively. lb. However, if we regard the block as a special whole layer, it will have redundant calculation on both width and length, as shown in Fig. 6a. But the block can be divided into two sequential pieces, one piece contains the input vertex and layer la, the other piece contains layer lb and output vertex as shown in Fig. 6b. After this operation, there is no more redundant calculations inside the two pieces. Here comes the insight. Given a CNN model G, the goal is to transform it into a sequence of pieces. Since there may be some redundant",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S49",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "calculation inside pieces, we need to minimize the redundancy inside every piece. After this operation, each piece can be regarded as a layer of original G. Since these pieces construct a chain structure, the operation gives change for further optimization. 4.2 Partition Graph Into Pieces We give a graph partition algorithm based on dynamic programming. Here we will reveal the existence of the optimal sub- structure property of the problem of partitioning graph into pieces, which is necessary for dynamic programming. We first define the concept of ending piece of graph G. Note since the piece of G is just a smaller segment defined in the previous section, we still use the notationM to denote these pieces. Definition 4. An",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S50",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "ending piece ME is a special piece of G which for any edge (u, v) \u2208 G, if u \u2208 ME, then v \u2208 ME. Fig. 7 gives an illustration of ending pieces. We plot a small graph G with 8 vertices in Fig. 7a and two different pieces in Fig. 7b and Fig. 7c. The green region {E, G, H} in Fig. 7b is an ending piece of original graph G. But the red region {E, F, H} in Fig. 7c is not an ending piece since the edge E is a member of this piece but G is outside the red region. Graph G can be partitioned into pieces using 8 Algorithm 1 Partition graph into pieces Require:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S51",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "F: map indexed with the hash h(G) and return F(G). Require: R: map indexed with the hash h(G) and return M. 1: function PARTITION (G, M\u2032 E) 2: compute the hash h(G) 3: if F contains h(G) then return F[h(G)] 4: min \u2190 \u221e 5: get M \u2282h(G) which directly is connected with M\u2032 E 6: for ME \u2190 DFS (G \u2212 M) do 7: ME \u2190 ME \u222a M 8: calculate the redundancy C(ME) 9: cur \u2190 max(partition(G \u2212 ME, ME), C(ME)) 10: if min > curthen 11: F[h(G)] =cur 12: R[h(G)] =ME 13: min \u2190 cur return F[h(G)] 14: function OBTAIN (G) 15: if G == \u03d5 then return 16: M \u2190R[h(G)] 17: print the piece M 18:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S52",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "obtain(G \u2212 M) the concept of ending piece recursively. Given a graph G, the sketch of the procedure is to find an ending piece ME and add it to the partition result (a sequence of pieces), then consider G \u2212 ME as a new graph and repeat the previous procedure recursively. Fig. 7d shows a partition result of graph G. Note partition G by ending piece can not guarantee that these pieces obtained are with a chain structure. If we assign vertex B in Fig. 7d from the middle piece {B, C, F} to the first piece {A, D}, the partition result can also be obtained by the above procedure. However, such a result do not satisfy our goal since",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S53",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "the first piece connects with two pieces at the same time. To prevent this, we add a constraint that all the vertices that are directly connect to the ending piece must belong to the ending piece in the next iteration. With this constraint, once{E, G, H} is determined as ending piece, {B, C, F} must belong to the same piece in the final result, which guarantees the obtained pieces a chain structure. 4.3 The Algorithm Since our goal to minimize the redundant calculation of every piece, we need to quantify the redundant calculation cost C(M) for a given piece M. Assume I is the original input feature sizes of sources nodes in M, and I\u2032 is the feature size with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S54",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "redundant parts that are calculated with Eq. (3). The value of C(M) can be easily quantified by the difference of required FLOPs for the two input. Here we give the state transfer equation for partitioning the graph structure: F(G) = min ME\u2282G {max {F(G \u2212 ME), C(ME)}}. (13) If G is partitioned into multiple piecesM, the function F(G) return the minimum FLOPs difference C(M) among all partitioned pieces in G. Algorithm 1 gives the pseudocode. Line 2-4 checks whether F(G) is already calculated, if true, the following computation can be skipped. Otherwise, we use a variable min to store the minimum value located in Eq. (13). Line 5-7 adds a constraint to make sure the partitioned pieces follow a chain",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S55",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "structure. Since the partition function uses recursion, the parameter M\u2032 E stores the partitioned piece in its previous calculation. We use a DFS function to produce all the possible ME. Line 8-13 is the core part of our proposed dynamic pro- gramming. It iterates all possible ME, and uses recursion to solve the optimization problem. Here we use a variable cur to store the best partition strategy for currentME. If the current strategy is better than the one we have recorded, we update the record variable min and map F and R. Line 14-18 is the obtain function that receives the CNN G and shows the best partition strategy using the map R that is calculated in the partition function.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S56",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "Note the DFS function can produce tons of availableME for a given G. Since iterating all of them leads to unfeasible complexity for optimization, we use a simple pruning strat- egy here. From the above discussion, it is clear that the more sequential layers we fuse, the more redundancy we get. In fact, we observe that the redundancy is intolerable when the diameter of ME exceeds a specific number. Definition 5. The diameter of piece M is the greatest dis- tance of any vertex pair in M. With this observation, we limit the diameter of produced ME in DFS function under a constant integer d. In practice, we set the value of d to 5. 5 P IPELINE COOPERATION FOR",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S57",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "CNN I NFERENCE In this section, we present a pipeline cooperation (PICO) scheme aimed at efficiently executing CNN inference. PICO uses a heuristic algorithm based on dynamic programming to optimize the inference pipeline. We also implement an adaptive framework that automatically chooses suitable parallel scheme under dynamic workload. 5.1 Heuristic For chain structure, although the polynomial algorithm for P(G, D, S) does not exist unless P = NP , the optimal solution can be found in polynomial time if these mobile de- vices are homogeneous, which leads to a heuristic two-step algorithm. We first find the optimal S\u22c6 for a homogeneous cluster, then adapt the S\u22c6 to a heterogeneous cluster using a greedy algorithm. Since the CNN G is partitioned",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S58",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "into L pieces, consider- ing a specific stage S :< M, D, F > that starts from i-th piece and ends at j-th piece. We can use the notation Si\u2192j to represent it, so to the two notations Mi\u2192j and Di\u2192j. 5.1.1 Dynamic Programming Based on the given cluster D, we construct a new cluster D\u2032, which has the same number of devices of D, but the computing capacity of each device is equivalent to the average of D. \u03d1(d\u2032 k) = P dk\u2208D \u03d1(dk) |D| \u2200d\u2032 k \u2208 D\u2032, |D\u2032| = |D| (14) 9 Algorithm 2 Dynamic programming for pipeline inference Require: P, L: 3D arrays used to record the period and latency. Require: S, R: 3D array used",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S59",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "to trace the computed stage and sub-pipeline. 1: function DP(i, j, p, Tlim) 2: if P[i][j][p] exists then return P[i][j][p], L[i][j][p] 3: calculate Ts[i][j][p] using (11) 4: P[i][j][p] \u2190 Ts[i][j][p] 5: T[i][j][p] \u2190 Ts[i][j][p] 6: S[i][j][p] \u2190 (i, j, p) 7: if m = 1or j = i + 1then return P[i][j][p], T[i][j][p] 8: for s := i \u2192 j \u2212 1 do 9: for m := 1\u2192 p \u2212 1 do 10: calculate Ts[s + 1][j][m] using (11) 11: Tlim \u2190 Tlim \u2212 Ts[s + 1][j][m] 12: if Tlim < 0 then 13: continue 14: P[i][s][p\u2212m], T[i][s][p\u2212m] \u2190 DP(i, s, p\u2212m, Tlim) 15: if Tlim < T[i][j][p \u2212 m] then 16: continue 17: period \u2190 max(P[i][s][p \u2212 m], Ts[s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S60",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "+ 1][j][m]) 18: if period < P[i][j][p]) then 19: P[i][j][p] \u2190 period 20: T[i][j][p] \u2190 T[i][s][p \u2212 m] +Ts[s + 1][j][m] 21: R[i][j][p] \u2190 (i, s, p\u2212 m) 22: S[i][j][p] \u2190 (s + 1, j, m) return P[i][j][p], L[i][j][p] 23: function BUILD STRATEGY ((i, j, p), S) 24: if R[i][j][p] then 25: BuildStrategy(R[i][j][p], S) 26: calculate Si\u2192j using S[i][j][p] 27: S \u2190 Si\u2192j \u222a S For any device dk belongs to this stage, the output feature size Fk is equivalently partitioned. Thus, Fk can be determined by the size of stage. We denote p = |Di\u2192j| for convenience. The expression of stage can now be simplified as a three- element tuple (i, j, p). For the optimal pipeline S\u22c6, it",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S61",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "can now be broken into an optimal sub-pipeline consisting of pieces form 1 through s with p\u2212m mobile devices followed by a single stage with pieces s + 1through j replicated over m workers. Then using the optimal sub-problem property, we can solve the optimization problem through dynamic programming: P[i][j][p]= min i\u2264s<j min 1\u2264m<p max ( P[i][s][p \u2212 m] T s[s + 1][j][m] (15) where P[i][s][p\u2212m] is the time taken by the slowest stage of the optimal sub-pipeline between piece i and s with p \u2212 m edge devices, T s[s + 1][j][m] is the time taken for a stage with model segment Ms+1\u2192j with m devices. Obviously P[1][L][D] is equivalent to P(G, D\u2032, S) in the homogeneous case. During",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S62",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "optimization, we prune these solutions that exceed the inference limitation Tlim. Algorithm 2 shows the pseudocode of our optimization algorithm which uses dynamic programming with mem- orization to find out the optimal parallelization strategy. Function DP computes the minimum period and records the optimal pipeline configuration in two 3D arrays R and S. The optimal parallelization strategy is built up through Algorithm 3 Adjust stage configuration S for heterogeneous devices Require: S\u2032: the optimal stage set for homogeneous cluster. 1: function ADJUST STAGE 2: Initialize an empty S 3: Sort devices in D by computing capabilities \u03d1(dk) 4: for dk \u2208 D do 5: Find the stage S\u2032 i\u2192j \u2208 S\u2032 with minimum \u0398\u2032 i\u2192j |D\u2032 i\u2192j| 6: Get Si\u2192j",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S63",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "from S or create Si\u2192j with empty Di\u2192j 7: Di\u2192j \u2190 dk \u222a Di\u2192j 8: Remove one device from D\u2032 i\u2192j 9: if |D\u2032 i\u2192j| = 0then 10: Adjust feature partition Fk for every dk \u2208 Di\u2192j. 11: S \u2190 Si\u2192j \u222a S 12: Remove S\u2032 i\u2192j from S\u2032 return S function BuildStrategy by recursively iterating the calculated R and S, and adding the corresponding stage configuration Si\u2192j to S. 5.1.2 Adapt to the heterogeneity We use a greedy algorithm to adapt the calculated S\u2032 in Algorithm 2 to the heterogeneous environment. For every stage S\u2032 i\u2192j \u2208 S\u2032, we keep the model segment Mi\u2192j un- changed and choose a proper set of edge devices as Di\u2192j from heterogeneous cluster",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S64",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "D. Let \u0398i\u2192j and \u0398\u2032 i\u2192j denotes the required computing resources of stage Si\u2192j and S\u2032 i\u2192j: \u0398i\u2192j = X dk\u2208Di\u2192j \u03b8(Mi\u2192j; Fk), (16) We want \u0398i\u2192j to be as close to \u0398\u2032 i\u2192j as possible. We initialize the stage set S with the same number of stages, each stage only the same number of workers and the same model fragment Si\u2192j. To achieve our goal, we sort the mobile devices by the computing capabilities \u03d1(dk) in reverse order and then iterate each device. In every itera- tion, we find the stage S\u2032 i\u2192j \u2208 S\u2032 with maximum average computing requirement \u0398\u2032 i\u2192j |D\u2032 i\u2192j|. The current device dk will be added to device set Di\u2192j. Once Di\u2192j owns the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S65",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "same number of device in D\u2032 i\u2192j, we adjust the output feature size Fk for every device dk \u2208 Di\u2192j with a Divide And Conquer algorithm. After this operation, we accomplish the presentation of stage Si\u2192j and add it to S. After all the iterations, we have a set of stages S for the heterogeneous cluster. The complete algorithm is shown in Algorithm 3. 5.1.3 Correctness PICO can not guarantee the final configuration for the inference pipeline is optimal since the problem is NP-Hard. But we could show that both Algorithm 1 and 2 find the optimal solutions for the sub-problem they focus on. Theorem 3. Given a CNN model G, F(G) in Eq. (13) returns the maximum amount of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S66",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "redundant FLOPs among those pieces in the optimal arrangement of G. Theorem 4. Assuming i and j is the start and end index of pieces that need to be deployed, P[i][j][p] in Eq. (15) returns the minimal period of all possible pipeline configurations for p mobile devices. The detailed proof can be found in the appendices. 10 5.2 Optimization Complexity PICO aims to find a many-to-many mapping for various CNNs and heterogeneous mobile devices, which has been shown as NP-Hard in Section 3.3. Here we analyze the complexity of these algorithms proposed by PICO. 5.2.1 Analysis PICO contains three novel algorithms, we analyze them one by one. Algorithm 1 arranges G into sequential pieces. We first define the width of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S67",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "CNN to formulate the complexity of Algorithm 1: Definition 6 (Width w of CNN). Given a CNN graph G, w is the width of G if there are at most w neural layers in G such that there is no path connecting any two of them. Since for every M generated in Line 6, Algorithm 1, the upper bound for every path in M is d since we limit the length of every path in M (see Section 4.3), the time complexity of Algorithm 1 can be given by Theorem 5. Theorem 5 (Complexity of Algorithm 1). The time complex- ity of PICO is O(wd(nd w )w), where d is the upper bound for every path in M, n is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S68",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "the number of vertices in G, and w is the width of G. Proof 5.1. We only give a proof sketch here for ease of reading. The detailed proof is in the appendices. Line 8 in Algorithm 1 dominates the computation and the complexity for computing C(ME) for every ME is O(wd), since there are up towd vertices in ME. We need to count all the possible pairs (G, ME) during execution to analyze the complexity of the entire algorithm. Any directed acyclic graph G is also equivalent to a partially ordered set. By applying Dilworth\u2019s Theorem on G, we can decompose G into w disjoint chains {Vi}. Thus, any possible pair (G, ME) can be decomposed into a combination",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S69",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "between ME and these chains. Since ME is an ending piece of G, ME \u2229 Vi must be a suffix of Vi. Therefore, the upper bound for the possible pair (G, ME) is \u03a0i\u2208{1,\u00b7\u00b7\u00b7,w}Vid, where Vi is the length of chain Vi. And the maximum of \u03a0i\u2208{1\u00b7\u00b7\u00b7,w}Vid achieves when Vi = Vj = n/w. Thus, the number of all possible pairs (G, ME) is less than (nd w )w, and the complexity of Algorithm 1 is O(wd(nd w )w). Algorithm 2 generates an inference pipeline for D ho- mogeneous mobile devices. The sub-problem of Algorithm 2 is to find the inference cost for a given stage, and the com- putational complexity is O(nD). Assuming the Algorithm 1 partitions the CNN",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S70",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "into L pieces, the total number of sub- problem of Algorithm 2 is O(L2D), leading to a total time complexity of O(nL2D2). Algorithm 3 is a simple greedy algorithm. The sorting operation in Line 3 has O(D log(D)) complexity (e.g., quick sort). The for-loop in Line 4 repeats for D times and the line 6 has O(log(S)) complexity for choosing Si\u2192j from a tree set S, where S is the size of S. The complexity for Algorithm 3 is O(D(log(D) + log(S))) and could be relaxed to O(D log(D)) since S \u2264 D. From the above discussion, we can deduce the complex- ity of PICO is O(wd(nd w )w + nL2D2). The complexity is listed in Table 3 for summary.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S71",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "Note Algorithm 1 only needs Fig. 8: The workflow of stages in an inference pipeline. to run one time for every CNN and is not affected if the mobile environment changes. TABLE 3: Computational complexity of PICO framework. Algorithm 1 Algorithm 2 Algorithm 3 PICO O(wd( nd w )w) O(nL2D2) O(D log(D)) O(wd( nd w )w + nL2D2) 5.2.2 PICO in Practice The computational cost of PICO in practice can be decom- posed into two parts: the one-time cost O(wd(nd w )w) caused by Algorithm 1 and the ongoing cost O(nL2D2) caused by Algorithm 2 and 3. The one-time cost caused by Algorithm 1 does not involve the specific edge environment or mobile device cluster, it can be executed on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S72",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "a powerful PC and the results can be directly used by mobile devices without additional processing. The ongoing cost caused by Algorithm 2 and 3 is lightweight and takes less than 1 second in the resource- limited Raspberry-Pi. Thus, the Algorithm 2 and 3 can be triggered if the mobile environment changes and immedi- ately adapt to the new environment. 5.3 Implementation The workflow of stages: We summarize the workflow of stages in Fig. 8. Each stage owns its configuration Si\u2192j which is given by the previous optimization. The main thread of stage takes the feature map from the input queue, then splits it into small tiles with different sizes according to F and distributes them to those devices Di\u2192j.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S73",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "Once the computation finishes, the outputs of those devices are gathered and stitched together. There are two other threads responsible to put the receiving feature map into the input queue and send the output to the next stage. Feature split and stitch: Most popular DL frameworks such as TensorFlow, PyTorch does not provide an efficient way to split feature map with overlapped parts, and using these high level provided by those frameworks to imple- ment these operations brings intolerable latency. We accom- plish the frame split and stitch operations by directly oper- ate the frame tensor data point in the memory space through C++. In practice, after optimization, the time consumption of feature split and stitch can be ignored. 11",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S74",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "Fig. 9: The testbed in our experiment is composed of 8 Raspberry-Pi 4Bs, 2 Nvidia TX2 NXs, a Monsoon High Voltage Power Monitor (HVPM) and a Wi-Fi access point. Represent CNN into graph: We implement an automatic GraphConvertor module to convert a given CNN model file into a DAG. The module will record the input and output layers for every tensor during profiling. To achieve this, We modify the source file of PyTorch and add a new hook function register prev forward hook as suggested in [19]. 6 E XPERIMENT We give the details of our evaluation bed for experiment and analyze the obtained result. 6.1 Environment Setup Here we give the details of our evaluation setup. Hardware: The mobile",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S75",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "cluster for evaluating the PICO framework uses one Wi-Fi access point with 50Mbps band- width and 8 ARM based Raspberry-Pi 4Bs. Each Raspberry- Pi 4B has a Quad Core ARM Cortex-A73, which has 1.5 GHz max CPU frequency. It has 2 GB LPDDR2 SDRAM, and dual-band 2.4 GHz/5 GHz wireless for communication. To represent a realistic low-end mobile device cluster, we set these Raspberry-Pi 4B running with one CPU core during inference. Fig. 9 shows the test bed we used, the laptop is used to monitor and control this cluster. We limit the CPU frequency for each Raspberry-Pi using Linux cGroup to simulate the heterogeneous mobile cluster in the real world. The heterogeneous cluster has two Nvidia TX2 NX devices,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S76",
      "paper_id": "arxiv:2206.08662v3",
      "section": "results",
      "text": "which have a 2.2 GHz CPU frequency, and the six Raspberry-Pis that have three different CPU frequency settings: 1.5 GHz, 1.2 GHz, and 0.8 GHz, respectively. Software: We implement PICO and other compared",
      "page_hint": null,
      "token_count": 33,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S77",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "backend. Due to differing output feature sizes on each device, we only used asynchronous P2P algorithms such as isend() and irecv() in Gloo. As for the network bandwidth, we use a method similar to MPEG-DASH [21], using the tool ping to send data of two different sizes and measure the re- sponse times. The rate is then determined by calculating the ratio between the difference in data size and the difference in response times. Input Output Conv Pooling Fc SoftMax InceptionV3 Residual Block Inception Block ResNet34 Inception Block Residual Block Fig. 10: The model structure of ResNet34 and InceptionV3. Models overview: VGG16 [12] is a classic CNN classi- fication model. It contains 13 conv layers, 5 pooling layers and 3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S78",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "fc layers. You only look once version 2 (YOLOv2) [13] is a lightweight CNN used for real-time object detection system. It has deeper architecture compared with VGG- 16. There are 23 conv and 5 pooling layers in YOLOv2, nearly twice as VGG16. Both VGG16 and YOLOv2 follow the chain structure. ResNet34 [10] and InceptionV3 [11] are two classic CNNs that use a block structure. ResNet34 uses a skip-connection strategy that allow the feature to skip several layers. Compared with ResNet34, InceptionV3 has more complex structure. The Inception block has multiple branches, and the conv layers also have many unbalanced (e.g., 1 \u00d7 7, 5 \u00d7 1) kernels. Compared method: For VGG16 and YOLOv2, four dif- ferent parallelization strategies are used",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S79",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "in the evaluation: (1) Layer-wise (LW) scheme, which parallelizes the CNNs layer by layer; (2) Early-fused-layer (EFL) scheme, an ex- tension to the implementation of DeepThings [5], which fuses and parallelizes the first few conv layers, then exe- cutes the rest layers in a single device; (3) Optimal Fused- layer (OFL) scheme, which selectively fuses convolution layers at different parts of a model; (4) CoEdge (CE) [22], the state-of-the-art parallelization scheme which extends the layer-wise scheme to heterogeneous environment and reduce the communication overhead by sorting devices into a list and limiting the communication to the neighbors for each device. Moreover, CoEdge uses a dynamic number of devices to process different layers to further reduces the impact of communication overhead",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S80",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "instead of using all mobile devices. (5) Pipeline Cooperation (PICO) scheme, which is proposed in this paper. For ResNet34 and YOLOv2, we compare two different graph partition strategies: (1) Consider each block as a piece, which is used in [6], [17]. (2) Partition the entire graph into multiple pieces with suitable granularity, which is proposed in this paper. 6.2 CNN Graph Partition Here we present some experimental results of our proposed graph partition algorithm. 12 1x1Pool1x1Conv7x1Conv1x7Conv7x1Conv1x7Conv1x1Conv1x7Conv7x1Conv1x1ConvConcatConcat (a) Original Concat7x1Conv1x1Conv1x1Pool1x7Conv1x7Conv1x1Conv7x1Conv7x1Conv1x1Conv1x7ConvConcatPiece 26Piece 25Piece 27Width: 7Length: 7Width: 7Length: 0Width: 7Length: 0 (b) Optimized Fig. 11: An illustration of our graph partition algorithm. (a) Part of InceptionV3 model (InceptionC block). (b) The obtained pieces after optimization. 1 2 3 4 5 6 7 8",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S81",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "12345671 2 3 4 5 6 7 8 1234567T he speedup ratio 1GHz 750MHz 500MHz 1GHz 750MHz 500MHz (a) ResNet34 1 2 3 4 5 6 7 8 12345671 2 3 4 5 6 7 8 1234567T he speedup ratio 1GHz 750MHz 500MHz 1GHz 750MHz 500MHz (b) InceptionV3 Fig. 12: The speedup ratio for ResNet34 and InceptionV3. The left part shows the result by treating the entire block as a whole, and the right part uses graph partition algorithm. 6.2.1 Partitioned Pieces Fig. 11 shows part of the partition result of InceptionV3 model. Fig. 11a plots the InceptionC block, which consists 4 branches and 10 neural layers. We can find if we consider the entire block as a layer [6],",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S82",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "lots of redundant calculations will be introduced since there are many unbalanced conv kernels (e.g., 1 \u00d7 7 and 7 \u00d7 1). We can use Eq. (3) and Eq. (5) to quantify the redundant calculation. If we fuse the entire block into one piece (used in [6]), the devices have to introduce 13 pixel length on both the width and height dimensions. After running the partition algorithm, the block is split into three pieces (Piece 25, Piece 26, and Piece 27) as Model n w wd ( nd w )w Execution Pieces VGG16 19 1 4.7 \u00d7 102 0.10s 19 SqueezeNet 30 2 5.6 \u00d7 104 0.14s 29 ResNet34 38 2 9.0 \u00d7 104 0.28s 21 MobileNetV3 96 3 6.1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S83",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "\u00d7 107 0.79s 31 InceptionV3 99 4 4.6 \u00d7 109 3.01s 38 NASNetL 570 8 1.1 \u00d7 1022 > 5h NaN NASNetL-P 75 \u00d7 8 8 9.3 \u00d7 1014 1.9h 34 TABLE 4: The performance of Algorithm 1 for various CNNs. NASNetL-P denotes the strategy which roughly partitions it into 8 parts. shown in Fig. 11b (The full partition result is attached in the supplemental material). The entire InceptionV3 model is composed of 40 pieces, but due to the size of model, we can not plot the entire model here. The complete partition result is shown in the supplemental materials. These pieces have much smaller redundant calculation. Piece 25 has 7 pixel length redundancy, and Piece 26 and Piece 27",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S84",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "only have redundancy on only one dimension. Compared with Fig. 11a, the redundant calculation during inference can be significantly reduced. In addition, since we break block into pieces, the later optimization can make more fine-grained optimization. 6.2.2 Speedup After Partition We can adapt PICO to those CNNs with non-chain structure by applying our graph partition algorithm at first. Here we compare the speedup ratio for ResNet34 and InceptionV3. Fig. 10 shows the structures of the two model, obviously they are constructed with the block structure. According to the figure, we can find the Inception block in InceptionV3 is more complex than Residual block used in ResNet34. Fig. 12 plots the speedup ratio under different CPU frequencies for ResNet34 and InceptionV3",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S85",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "with different strategy. The figures on the left part fuse the entire block into a whole, and the figures on the right show the results that adopt our graph partition algorithm, When executing CNN infer- ence with 8 devices, PICO can achieve 6.8\u00d7 speedup for ResNet34 and 6.5\u00d7 for InceptionV3 after partitioning the CNN model. The speedup effect is more obvious with low CPU frequency since the limitation of computing resource is relieved when the number of mobile devices increases. As for the strategy of fusing the entire block, it achieves up to 5\u00d7 speedup for ResNet34, but only 4\u00d7 for InceptionV3 when the CPU frequency is low. We think it is caused by the difference in the number of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S86",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "layers in Residual and Inception blocks. Since the Inception block contains more layers than Residual block, the optimal model partition is more likely to exist within blocks. 6.2.3 Optimization Complexity Algorithm 1 has O(wd(nd w )w) complexity to optimize the given CNN, as we analyzed in Section 5.2. Here we run Algorithm 1 on many popular CNNs on a PC equipped with Intel Core i9-10940X to give a comprehensive evaluation of its performance. The number of layers n, the width w and the upper bound wd(nd w )w for every tested CNN and the execution time are listed in Table 4. Note n only counts conv and pool layers, since other layers such as BN and ReLU do 13 0",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S87",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "1 2 3 4 5 6 7 8 9 46810121416182022240 1 2 3 4 5 6 7 8 9 46810121416182022242628300 1 2 3 4 5 6 7 8 9 51015202530354045A verage Inference Latency (s)T he number of mobile devices (1GHz)L WF LD FLC EP ICOA verage Inference Latency (s)T he number of mobile devices (750MHz)A verage Inference Latency (s)T he number of mobile devices (500MHz) (a) Inference period under different parallel schemes and CPU frequencies 1 GHz7 50MHz5 00MHz02 4 6 8 1 01 21 4Algorithm Throughput Per MinuteC PU FrequencyL W E FL O FL C E P ICO (b) Throughput with 8 devices Fig. 13: The cluster capacity when executing VGG16. 0 1 2 3 4 5 6",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S88",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "7 8 9 68101214161820222426280 1 2 3 4 5 6 7 8 9 810121416182022242628300 1 2 3 4 5 6 7 8 9 10152025303540A verage Inference Latency (s)T he number of mobile devices (1GHz)L WF LD FLC EP ICOA verage Inference Latency (s)T he number of mobile devices (750MHz)A verage Inference Latency (s)T he number of mobile devices (500MHz) (a) Inference period under different parallel schemes and CPU frequencies 1 GHz7 50MHz5 00MHz0123456789Algorithm Throughput Per MinuteC PU FrequencyL W E FL O FL C E P ICO (b) Throughput with 8 devices Fig. 14: The cluster capacity when executing YOLOv2. 1 2 3 4 5 6 7 8 050100150200250300 LW FL DFL PICOM emory Footprint (MiB)N umber of Mobile Devices",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S89",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "Model Feature Fig. 15: The memory footprints of different algorithms. not change the output feature shape and require negligible computing resources. Additionally, the last column shows the number of pieces after optimization. Many real-world CNNs are deep but narrow, which means they have small width w. Algorithm 1 is efficient and could be executed in less than one or several seconds for these models. We also add the NASNet-A-Large [23] (NASNetL) model to evaluate Algorithm 1 in an extremely complex case. NASNetL is constructed through neural archi- tecture search technology. NASNetL is much broader (w = 8) and contains much more layers ( n = 570 ) compared with the hand-designed models ( w \u2264 4 and n \u2264 100).",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S90",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "It is rare to deploy such a large-scale model on mobile devices. The trade-off that considers a block as a layer [6], [17] has no effect since there is no block in NasNetL. Directly applying PICO to NASNetL takes nearly infinite time to produce the final output considering the time com- plexity (1.1\u00d71022). We successfully adapt PICO to NASNetL using divide-and-conquer. Assume Algorithm 1 divides a model into L pieces, if we fuse the L/2 pieces from the input position into a smaller model and apply Algorithm 1 to it, then the smaller model must be divided into the same L/2 pieces as the original model (the property of dynamic programming). Inspired by this property, We cut a small part",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S91",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "from the beginning of NASNetL, and apply Algorithm 1 on the smaller model to obtain several pieces. Only these pieces away from the cut line will be kept to make sure these pieces from different small model are still sequential. Then we apply the same strategy to the rest model until all the smaller models are partitioned into pieces. The last line in Table 4 shows the performance of the divide-and- conquer strategy. NasNetL is tackled with 8 parts and PICO produces the result in two hours. Since Algorithm 1 only needs to run once for every CNN regardless of specific mobile environment (see Section 5.2), the optimization cost is acceptable. 6.3 Pipeline Performance We evaluate our proposed pipeline cooperation",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S92",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "scheme with 2-8 Raspberry-Pi devices and measure some important metrics. 14 TABLE 5: The utilization, redundancy ratios and memory footnotes of heterogeneous mobile devices. Model Attributes Methods Type Devices Average NX@2.2 NX@2.2 Rpi@1.5 Rpi@1.5 Rpi@1.2 Rpi@1.2 Rpi@0.8 Rpi@0.8 VGG16 Layers: 13 conv + 5 pool Input size: 244 \u00d7 244 CE Utili. 80.87% 82.13% 69.37% 59.97% 57.91% 36.56% 23.11% 17.33% 53.40% Redu. 2.02% 1.93% 1.29% 2.06% 1.30% 1.41% 1.32% 0.77% 1.51% Mem. 195.0 M 183.0 M 162.0 M 158.0 M 147.0 M 149.0 M 134.0 M 137.0 M 158.1 M EFL Utili. 32.43% 39.79% 72.58% 75.08% 94.23% 96.77% 64.09% 64.16% 67.39% Redu. 11.02% 11.60% 19.08% 19.83% 18.58% 19.22% 12.78% 13.42% 15.69% Mem. 142.0 M 147.0 M 169.0 M 179.0 M",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S93",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "173.0 M 183.0 M 151.0 M 165.0 M 163.6 M OFL Utili. 38.90% 40.19% 60.87% 61.79% 85.34% 94.15% 76.54% 80.46% 67.28% Redu. 7.45% 7.67% 11.12% 11.39% 10.33% 10.53% 8.15% 8.31% 9.36% Mem. 149.0 M 149.0 M 158.0 M 159.0 M 154.0 M 155.0 M 151.0 M 152.0 M 153.4 M PICO Utili. 91.21% 93.28% 83.12% 79.40% 47.63% 66.26% 68.17% 90.15% 77.40% Redu. 11.08% 10.97% 5.82% 3.83% 6.93% 5.55% 0.00% 3.83% 6.00 % Mem. 189.0 M 144.0 M 121.0 M 103.0 M 92.0 M 121.0 M 115.0 M 111.0 M 124.5 M YOLOv2 Layers: 23 conv + 5 pool Input size: 448 \u00d7 448 CE Utili. 76.85% 75.46% 65.81% 66.94% 46.32% 46.77% 22.49% 20.21% 52.61% Redu. 0.82% 0.76% 0.87% 0.83% 0.79%",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S94",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "0.71% 0.68% 0.61% 0.75% Mem. 265.0 M 260.0 M 255.0 M 246.0 M 245.0 M 240.0 M 235.0 M 239.0 M 248.1 M EFL Utili. 37.85% 35.64% 67.24% 67.61% 96.01% 95.28% 75.87% 72.81% 68.54% Redu. 27.09% 27.09% 45.08% 45.08% 44.68% 44.68% 29.29% 29.29% 36.54% Mem. 189.0 M 178.0 M 208.0 M 208.0 M 208.0 M 207.0 M 178.0 M 178.0 M 194.3 M EFL Utili. 39.28% 37.03% 69.47% 68.92% 97.02% 95.99% 77.61% 73.94% 69.91% Redu. 25.98% 25.98% 44.51% 44.51% 44.86% 44.86% 28.12% 28.12% 35.86% Mem. 193.0 M 182.0 M 212.0 M 212.0 M 212.0 M 211.0 M 182.0 M 182.0 M 198.3 M PICO Utili. 89.37% 97.91% 89.96% 97.85% 89.44% 99.40% 91.89% 89.03% 93.11% Redu. 6.95% 2.27% 1.25% 9.18% 9.18%",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S95",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "5.89% 6.13% 5.05% 5.73% Mem. 188.0 M 135.0 M 108.0 M 116.0 M 113.0 M 122.0 M 159.0 M 157.0 M 137.3 M 6.3.1 Maximum Throughput Fig. 13 and Fig. 14 plot the cluster capacity when executing VGG16 and YOLOv2 with different parallel schemes. The first three figures plot the inference period with different parallel schemes and CPU frequencies. The last figure plots the accomplished inference task per minute with 8 devices. It represents the throughput of different parallel schemes. PICO has the best performance as expected, since our optimization goal is to reduce the redundant computation and achieve minimum pipeline period. When the number of devices increases, the throughput of different strategies also improve except the executing YOLOv2 using",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S96",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "LW with 1GHz CPU core. YOLOv2 has nearly twice number of layers compared with VGG16, which brings more communication overhead for Layer-wise strategy. Through CE also exe- cutes CNN layer by layer, CE uses a dynamic number of working devices during inference and reduces the traffic volume by only synchronizing overlapped features. There- fore, CE outperforms LW. When the computing resource is rich (1GHz), the gain brought by the increasing number of devices is offset by communication overhead. EFL and OFL fuse multiple layers into one model segment, and do not require communication among devices when they are executing one segment, thus the communication overhead is reduced. Since OFL optimizes the configuration of fused layers, it outperforms EFL which simply",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S97",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "fuses the very early layers. However, when the number of devices is bigger than a certain number (4 for example), the improvement is very tiny due to the additional computation CPU redundancy. 6.3.2 Memory Footprint Memory footprint is another important metric during in- ference. The inference latency will quickly grow when the required memory exceeds the onboard memory of the device, since the device has to use swap memory [17]. We use a python script to sample memory footprint from /proc/pid/status for each inference process. Fig. 15 plots the average memory footprint of different algorithms. Here we ignore the performance of CE, since LW and CE have very similar performance when devices are homogeneous. According to our previous discussion, the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S98",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "memory footprint can be divided into two more fine-grained parts. The Model and Feature denote how much the model parameters and intermediate features take part in the memory footprint. We can find the memory footprint decreases as the number of mobile devices increases. Since LW, FL, DFL only split features, the whole model needs to be replicated on all mo- bile devices. This approach leads to the result that they can only decrease the memory footprint caused by intermediate features. Meanwhile, PICO distributes both models and fea- tures, thus PICO reduces the memory footprint significantly. 6.4 Impact of Heterogeneity Here we evaluate the impact of heterogeneity on differ- ent parallel schemes. We monitor the CPU usage during inference on the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S99",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "heterogeneous mobile cluster and record the average computing resource utilization ratio (Utili.) for different parallel schemes. We also calculate the redundancy ratio (Redu.) and their memory footprint (Mem.) on every device during computation. The result is presented in Table 5. We remove LW scheme due to its bad performance in heterogeneous environment. 6.4.1 Load Balancing The CPU utilization rates of these devices show the work- load among these devices. PICO and CE will adjust the 15 VGG16 YOLOv2 0 10 20 30 40 50 60 70Energy Consumption (J) CE EFL OFL PICO Execution Standby Fig. 16: The average energy consumption for every inference task with heterogeneous devices. feature partition size according to the specific devices. Thus, the workload of PICO",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S100",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "and CE is better than EFL and OFL. We find both PICO and CE impose more percentages of workloads on these devices with higher CPU frequency (1.2 GHz). Take the CE as an example, the CPU utilization rate is up to 82.61% for the fastest devices, but drifts down 22.64% for the slowest devices when running VGG16. The reason is that CE uses a dynamic number of devices to process each layer. When the feature map is wide (e.g., 224 x 224), CE may use all devices to accelerate the execution. When the feature map is small (e.g., 7 x 7), CE may place all the workload on one powerful device to avoid redundant computation and communication. However, the computing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S101",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "resources of these slower devices are wasted. On the contrary, PICO can fully utilize the computing resources, thus having a better performance on load balancing. 6.4.2 Computation Efficiency Because the input feature maps of different devices overlap with each other, the redundant computation can lead to in- efficient performance. CE has the minimum average redun- dant computation, since CE synchronizes the feature map for every layer. But the frequent communication leads to low resource utilization and high inference latency. Fusing layers and executing them together can keep the devices busy, but will increase redundant computation. Especially for the EFL which has 46.54% percent redundancies executing YOLOv2. OFL uses dynamic programming to find a balance between communication and computation, but the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S102",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "redundancy ratio (12.08%) is still higher than PICO (7.64%) as PICO uses a subset of mobile devices instead of the entire cluster. 6.4.3 Energy Consumption We measure the energy consumption for every inference task, the result is shown in Fig. 16. The energy consumption is composed of the inference execution and standby power consumption. EFL consumes the most energy, since EFL has the highest redundant computation compared with other schemes. Moreover, the redundant computation does not accelerate the inference, thus EFL also has high standby power consumption. OFL has a lower energy consump- tion compared with EFL since OFL reduces the redundant computation by synchronizing feature map periodically. CE executes the CNN layer by layer and has the lowest redundancy",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S103",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "among all the schemes. However, the standby power consumption is the majority of energy consumption, TABLE 6: Optimization time with graph-like CNN. Branches, Layers, Devices PICO BFS (Optimal) (2, 4, 4) < 1s 1.58s (2, 8, 6) < 1s 18.23s (3, 12, 4) < 1s 11.96m (3, 12, 6) < 1s 45.24m (3, 12, 8) < 1s > 1s (4, 20, 4) < 1s > 1h (4, 20, 6) < 1s > 1h TABLE 7: Optimization time with heterogeneous devices. Layers, Devices PICO BFS (Optimal) (4, 4) < 1s < 1s (8, 4) < 1s 1.62s (12, 4) < 1s 3.84s (16, 4) < 1s 11.27s (8, 6) < 1s 4.35m (10, 6) < 1s 12.28m (12, 6) < 1s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S104",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "> 1h (8, 8) < 1s > 1h because CE has a long inference latency, especially executing YOLOv2. On the contrary, PICO has the lowest standby power consumption during inference task, since PICO can maximize the throughput during inference. Through PICO has more redundant computation compared with CE, the overall energy consumption is still lower than CE. 6.5 Comparing With Optimal Configuration Because it is NP-Hard to find the best many-to-many map- ping for graph-like CNN and heterogeneous devices, PICO can not guarantee finding the optimal inference pipeline configuration. Thus, we compare PICO with the optimal pipeline to further evaluate the performance. The optimal pipeline is obtained through a broad first search (BFS). We compare the optimization time for producing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S105",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "the pipeline configuration and the resource utilization of every mobile device during runtime.",
      "page_hint": null,
      "token_count": 13,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S106",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "The main problem for the comparison is the possible so- lution space for BFS is over-complex. According to Table 2, finding the best many-to-many mapping for both chain- like CNN, heterogeneous devices and graph-like CNN, homogeneous devices are NP-Hard. But BFS tries to find the best many-to-many mapping for graph-like CNN and heterogeneous devices. We test the BFS with CNNs on 4-8 Raspberry-Pi devices, but all of them fail to produce the final output after several hours on a powerful PC. Therefore, we compare the performance of PICO and BFS from two sides. On the one side, (1) we compare PICO and BFS with graph- like CNN and homogeneous devices. On the other side, (2) we compare PICO and BFS",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S107",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "with chain-like CNN and heterogeneous devices. Table 6 and 7 show the optimization overhead of PICO and BFS. Fig. 17 and 18 give the runtime performance. 6.5.2 Optimization Time For all the situations listed in Table 6 and 7, PICO could accomplish the optimization within 1 second, But BFS re- quires much more time to give the output even on small 16 9 3%1 00%9 1%9 9%9 5%9 8%6 %7 %3 %8 %5 %4 %8 6%9 0%9 2%8 8%9 7%8 4%9 %4 %8 %4 %8 %8 %1 GHz1 GHz1 GHz1 GHz1 GHz1 GHz0%20%40%60%80%100%Resource UsageH omogeneous Mobile Devices (BFS)1 GHz1 GHz1 GHz1 GHz1 GHz1 GHz0%20%40%60%80%100%H omogeneous Mobile Devices (PICO)U tili.R edu. Fig. 17: Runtime performance with graph-like CNN. 9 8%9 9%9",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S108",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "6%9 9%8 5%9 7%8 %9 %2 %8 %3 %3 %9 0%9 5%8 8%8 7%8 0%8 3%1 3%1 2%5 %5 %0 %4 %1.2 GHz1.2 GHz0.8 GHz0.8 GHz0.6 GHz0.6 GHz0%20%40%60%80%100%Resource UsageH eterogeneous Mobile Devices (BFS)1 .2 GHz1.2 GHz0.8 GHz0.8 GHz0.6 GHz0.6 GHz0%20%40%60%80%100%H eterogeneous Mobile Devices (PICO)U tili.R edu. Fig. 18: Runtime performance with heterogeneous devices. scale problems. The optimization time dramatically grows on larger problems and BFS fails to finish the calculation on both sides. Moreover, these problems that BFS fails to solve are much easier (either chain-like CNN or homogeneous devices) than those that PICO has solved in the paper. Thus, BFS is not applicable in practice. Another observation from Table 6 and 7 is that the changing of different parameters",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S109",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "(branches, layers, devices) has different impacts on the optimization time. When the CNN is a graph and devices are homogeneous, increas- ing the number of layers has more impact than devices. Take the Table 6 as an example, row 3 and row 4 show that the optimization time increases from 11.96 minutes to 45.24 minutes when the number of devices increases from 4 to 6 (3.78\u00d7). But row 2 and row 3 show that the optimization time increases from 18.23 seconds to 11.96 minutes when the number of layers increases from 8 to 12 (39.36\u00d7) through the number of homogeneous devices decreases. On the contrary, increasing the number of devices has more impact when the CNN is a chain and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S110",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "devices are heterogeneous, as shown in Table 7. The optimization time increases from 1.62 seconds to 3.84 seconds (2.37\u00d7) when the number of layers increases from4 to 8 (row 1 and row 2), but it increases from1, 62 seconds to 4.35 minutes (161.11\u00d7) when the number of devices increases from 4 to 6 (row 2 and row 5). These two observations reveal the complexity of the many-to-many mapping when the CNN is a graph and devices are heterogeneous from sides. 6.5.3 Runtime Performance We compare the runtime performance of PICO and BFS by plotting the computing resources utilization rate for each device. The result is plotted in Fig. 17 and 18. We also analyze the redundant computation during inference since",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S111",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "high utilization rate does not lead to good performance [6]. Fig. 17 shows the runtime performance for a graph-like CNN and 6 homogeneous devices (1 GHz CPU frequency). The graph-like CNN used in the comparison contains 3 branches and 12 layers and is also used in row 4, Table 6. The optimal configuration found by BFS achieves 95% re- source utilization rate. Meanwhile, the configuration found by PICO has the similar performance (around 90%). The redundant computation of BFS is lower than PICO, but all redundant computation keep at a low level for both BFS and PICO. The performance for chain-like CNN and heteroge- neous devices is plotted in Fig. 18. The CNN contains 10 lay- ers and these devices",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S112",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "have different computing resources, as shown on the x-axis (1.2 GHz, 0.8 GHz and 0.6 GHz). Similar to Fig 17, the optimal configuration (BFS) achieves great performance on these devices (up to 99%) except one (85%). As for PICO, the configuration places more workload of the inference to these devices who own rich computing resources, thus the resource utilization of them is similar to BFS (90% and 95% for the fastest devices). The average performance of the other devices is around 84.5%. Since PICO greatly reduces the computation complexity according to previous analysis, the performance of PICO is acceptable for most real world applications. 7 R ELATED WORK Along with the problem of enabling DNN-based intelligent applications, previous researches can",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S113",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "be divided into two categories. 7.1 Inference Offloading Due to the limited up-link of mobile devices, traditional way of uploading captured data to the cloud server is time-consuming [24], [25]. Researchers focus on offloading the computation of early layers to mobile devices ( Inference offloading). To minimize the inference latency, Neurosurgeon [26] proposed to partition model between cloud server and mobile device according to the network situation. But [26] can only handle models with the chain structure. DADS [27] proposed a novel algorithm to partition DNN with graph structure using a min-cut algorithm. QDMP [28] noticed that directly applying min-cut on the entire graph is time- consuming. Based on the block structure, [28] proposed a divide-and-conquer algorithm to find the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S114",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "min-cut, which achieves a nearly linear complexity in their experiments. Meanwhile, Branchynet [29] propose early exit mechanism by adding exit layers at the midden of DNN. This mech- anism enables mobile device not feature map to cloud server if the local accuracy already reaches a certain value. Considering the situation when server does not have the corresponding model, IoNN [30] an incremental offloading technology that significantly improves the inference perfor- mance. 7.2 Cooperative Inference Recently researchers began to turn their attentions on ex- ecuting inference completely at the edge with multiple mobile devices [4], [5], [6], [17], [22], [31], [32], [33], [34]. MoDNN [4] is the first work in this field. MoDNN equally partitions the out feature map for every",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S115",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "layer and distributes these feature maps to homogeneous devices. In their following-up work MeDNN [31], they use an adap- tive partition method for the heterogeneous devices. Both MoDNN and MeDNN need a master device to gather the entire output of every device for every layer. CoEdge [22] reduces the communication overhead by only sending the 17 overlapped feature map to the neighbors of devices. CoEdge also dynamically adjusts the number of working devices during inference to find the balance between communica- tion and computation. EdgeFlow [33] introduces a forward- ing table to overlap the communication with computation for CNNs with complex structures. The devices can execute one layer and receive the feature map required by other layers at the same",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S116",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "time. All these works [4], [22], [31], [33] require devices to communicate with each other for every layer. However, the wireless environment can lead to considerable communication overhead using these works. Deepthings [5] proposed to fuse the layers in the early stage of CNN to avoid communication during inference. But fusing layer increases overlapped feature maps among devices and harms the inference efficiency. DistrEdge [34] trains a deep reinforcement learning model to distribute the inference workload for heterogeneous devices. AOFL [6] uses a dynamic programming to find a trade-off between communication and computation. Devices need to syn- chronize the feature map after several layers using AOFL. DeepSlicing [17] propose a runtime scheduler to distribute the workload for heterogeneous devices. Both",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S117",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "AOFL and DeepSlicing partition the CNN at the block level. Moreover, all these works [5], [6], [17] are at a loss for what to do when meeting some extremely complex CNN [14]. On the contrary, PICO breaks the block into smaller pieces to avoid additional redundant computation. 8 C ONCLUSION AND FURTHER RESEARCH In this paper, we propose a pipeline cooperation scheme (PICO) for efficiently executing inference with versatile CNN models and diverse mobile devices. This scheme improves the inference efficiency by reducing the redundant calculation. We first analyze the problem of partitioning CNNs and mobile devices into an inference pipeline. Using the analysis result, PICO uses a two-step strategy to build the pipeline. First, we orchestrate the graph structure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S118",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "of the given CNN into a sequence of pieces. Then we divide these pieces and devices into several stages. The input data is fed into the first stage and the inference result is produced at the last stage. These stages compose an inference pipeline. We adjust the partition size of features among devices according to their computing resources. The execution time of each stage is optimized to be as close as possible to gain maximum throughput. In our experiment with 8 Raspberry- Pi devices, the throughput can be improved by 1.8 \u223c 6.8\u00d7 under various settings. PICO has demonstrated strong performance across a range of heterogeneous clusters by adjusting the partitioned feature size for each device to accommodate varying com-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S119",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "putation capabilities. However, this approach is limited in addressing device-level imbalances within a given stage and is unable to address imbalances at the stage-level. This can result in failure if the computation capabilities of the devices are extremely varied. To address these challenges, we are actively pursuing the development of a novel algorithm that can better balance the workload across different stages. This is a critical area of focus for our ongoing research efforts. ACKNOWLEDGMENTS We gratefully acknowledge the support received for this work from several sources. This includes the National Natu- ral Science Foundation of China (Grants 62071067, 62171057, 62201072), the Ministry of Education and China Mobile Joint Fund (MCM20200202), Beijing University of Posts and Telecommunications-China Mobile Research Institute",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S120",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "Joint Innovation Center, and BUPT Excellent Ph.D. Students Foundation (CX2021134). Additionally, we appreciate the funding from the Key-Area Research and Development Program of Guangdong Province (No. 2021B0101400003), Hong Kong RGC Research Impact Fund (No. R5060-19), Areas of Excellence Scheme (AoE/E-601/22-R), General Research Fund (No. 152203/20E, 152244/21E, 152169/22E), and Shenzhen Science and Technology Innovation Commis- sion (JCYJ20200109142008673). REFERENCES [1] G. Kour and R. Saabne, \u201cReal-time segmentation of on-line handwritten arabic script,\u201d in Frontiers in Handwriting Recognition (ICFHR). IEEE, 2014. [2] \u2014\u2014, \u201cFast classification of handwritten on-line arabic characters,\u201d in Soft Computing and Pattern Recognition (SoCPaR) . IEEE, 2014. [3] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, \u201cEdge intelligence: Paving the last mile of artificial intelligence",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S121",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "with edge computing,\u201d Proceedings of the IEEE, 2019. [4] J. Mao, X. Chen, K. W. Nixon, C. Krieger, and Y. Chen, \u201cModnn: Local distributed mobile computing system for deep neural net- work,\u201d in Design, Automation & Test in Europe Conference & Exhibi- tion (DATE). IEEE, 2017. [5] Z. Zhao, K. M. Barijough, and A. Gerstlauer, \u201cDeepthings: Dis- tributed adaptive deep learning inference on resource-constrained iot edge clusters,\u201d IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018. [6] L. Zhou, M. H. Samavatian, A. Bacha, S. Majumdar, and R. Teodor- escu, \u201cAdaptive parallel execution of deep neural networks on heterogeneous edge devices,\u201d in Proceedings of the 4th ACM/IEEE Symposium on Edge Computing, 2019. [7] J. Li, Q. Qi, J.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S122",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "Wang, C. Ge, Y. Li, Z. Yue, and H. Sun, \u201cOicsr: Out-in-channel sparsity regularization for compact deep neural networks,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. [8] Y. He, P . Liu, Z. Wang, Z. Hu, and Y. Yang, \u201cFilter pruning via geometric median for deep convolutional neural networks acceleration,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019. [9] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V . Vasudevan et al. , \u201cSearching for mobilenetv3,\u201d in Proceedings of the IEEE International Conference on Computer Vision, 2019. [10] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S123",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "image recognition,\u201d in IEEE conference on computer vision and pattern recognition (CVPR), 2016. [11] C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi, \u201cInception- v4, inception-resnet and the impact of residual connections on learning,\u201d in AAAI, 2017. [12] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014. [13] J. Redmon and A. Farhadi, \u201cYolo9000: better, faster, stronger,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. [14] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, \u201cLearning transfer- able architectures for scalable image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 8697\u20138710. [15] A. Benoit and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S124",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "Y. Robert, \u201cMapping pipeline skeletons onto hetero- geneous platforms,\u201d Journal of Parallel and Distributed Computing , 2008. [16] \u2014\u2014, \u201cComplexity results for throughput and latency optimiza- tion of replicated and data-parallel workflows,\u201dAlgorithmica, 2010. 18 [17] S. Zhang, S. Zhang, Z. Qian, J. Wu, Y. Jin, and S. Lu, \u201cDeepslicing: collaborative and adaptive cnn inference with low latency,\u201d IEEE Transactions on Parallel and Distributed Systems , vol. 32, no. 9, pp. 2175\u20132187, 2021. [18] P . Bonsma, \u201cMost balanced minimum cuts,\u201d Discrete Applied Math- ematics, vol. 158, no. 4, pp. 261\u2013276, 2010. [19] A. Harlap, D. Narayanan, A. Phanishayee, V . Seshadri, G. R. Ganger, and P . B. Gibbons, \u201cPipedream: Pipeline parallelism for dnn training,\u201d in Proceedings of the 1st",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S125",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "Conference on Systems and Machine Learning (SysML), 2018. [20] \u201cGloo: Collective communications library with various primitives for multi-machine training,\u201d https://github.com/ facebookincubator/gloo. [21] T. Stockhammer, \u201cDynamic adaptive streaming over http\u2013 stan- dards and design principles,\u201d in Proceedings of the second annual ACM conference on Multimedia systems, 2011, pp. 133\u2013144. [22] L. Zeng, X. Chen, Z. Zhou, L. Yang, and J. Zhang, \u201cCoedge: Cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices,\u201d IEEE/ACM Transactions on Net- working, vol. 29, no. 2, pp. 595\u2013608, 2020. [23] B. Zoph, V . Vasudevan, J. Shlens, and Q. V . Le, \u201cLearning transfer- able architectures for scalable image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S126",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "8697\u20138710. [24] J. H. Ko, T. Na, M. F. Amir, and S. Mukhopadhyay, \u201cEdge- host partitioning of deep neural networks with feature space encoding for resource-constrained internet-of-things platforms,\u201d in IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEE, 2018. [25] H. Li, C. Hu, J. Jiang, Z. Wang, Y. Wen, and W. Zhu, \u201cJalad: Joint accuracy-and latency-aware deep structure decoupling for edge-cloud execution,\u201d in International Conference on Parallel and Distributed Systems (ICP ADS), 2018. [26] Y. Kang, J. Hauswald, C. Gao, A. Rovinski, T. Mudge, J. Mars, and L. Tang, \u201cNeurosurgeon: Collaborative intelligence between the cloud and mobile edge,\u201d ACM SIGARCH Computer Architecture News, 2017. [27] C. Hu, W. Bao, D. Wang, and F. Liu, \u201cDynamic adaptive",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S127",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "dnn surgery for inference acceleration on the edge,\u201d in IEEE Interna- tional Conference on Computer Communications (INFOCOM). IEEE. [28] S. Zhang, Y. Li, X. Liu, S. Guo, W. Wang, J. Wang, B. Ding, and D. Wu, \u201cTowards real-time cooperative deep inference over the cloud and edge end devices,\u201d Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020. [29] S. Teerapittayanon, B. McDanel, and H.-T. Kung, \u201cBranchynet: Fast inference via early exiting from deep neural networks,\u201d in International Conference on Pattern Recognition (ICPR) , 2016. [30] H.-J. Jeong, H.-J. Lee, C. H. Shin, and S.-M. Moon, \u201cIonn: Incre- mental offloading of neural network computations from mobile devices to edge servers,\u201d in ACM Symposium on Cloud Computing , 2018.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S128",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "[31] J. Mao, Z. Yang, W. Wen, C. Wu, L. Song, K. W. Nixon, X. Chen, H. Li, and Y. Chen, \u201cMednn: A distributed mobile system with enhanced partition and deployment for large-scale dnns,\u201d in IEEE/ACM International Conference on Computer-Aided Design (IC- CAD), 2017. [32] R. Hadidi, J. Cao, M. S. Ryoo, and H. Kim, \u201cTowards collaborative inferencing of deep neural networks on internet of things devices,\u201d IEEE Internet of Things Journal, 2020. [33] C. Hu and B. Li, \u201cDistributed inference with deep learning models across heterogeneous edge devices,\u201d inIEEE INFOCOM 2022-IEEE Conference on Computer Communications. IEEE, 2022, pp. 330\u2013339. [34] X. Hou, Y. Guan, T. Han, and N. Zhang, \u201cDistredge: Speeding up convolutional neural network inference on distributed edge",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S129",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "de- vices,\u201d in 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2022, pp. 1097\u20131107. Xiang Yang received the B.E. degree in com- puter science and technology from Beijing Uni- versity of Posts and Telecommunications, Bei- jing, China, in 2019. He is currently a PhD can- didate of State Key Laboratory of Networking and Switching Technology at Beijing University of Posts and Telecommunications. His research interests span broad aspects of machine learn- ing, distributed computing, edge/cloud comput- ing and deep learning. Zikang Xu is an undergraduate student ma- joring in Computer Science and Technology at Beijing University of Posts and Telecommunica- tions. He obtained a postgraduate recommen- dation of State Key Laboratory of Networking and Switching Technology at Beijing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S130",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "University of Posts and Telecommunications. His research interests span broad aspects of machine learn- ing, edge/cloud computing and distributed com- puting. Qi Qi obtained her PhD degree from Beijing University of Posts and Telecommunications in 2010. Now, she is an associate professor of State Key Laboratory of Networking and Switch- ing Technology at Beijing University of Posts and Telecommunications. She has published more than 30 papers in international journal, and ob- tained two National Natural Science Founda- tions of China. Her research interests include edge computing, mobile cloud computing, Inter- net of Things, ubiquitous services, deep learn- ing, and deep reinforcement learning. Jingyu Wang obtained his PhD degree from Beijing University of Posts and Telecommuni- cations in 2008. He is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S131",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "currently a professor of State Key Laboratory of Networking and Switch- ing Technology at Beijing University of Posts and Telecommunications. He has published more than 100 papers in international journal, includ- ing IEEE CMag, TVT, ISJ, TSC, TMM, TCC, IoT, TWC, and so on. His research interests span broad aspects of SDN/NFV, edge/cloud comput- ing, IoV/IoT, big data processing and transmis- sion, intelligent networks, and traffic engineering. Haifeng Sun obtained his PhD degree from Beijing University of Posts and Telecommuni- cations in 2017. He is currently a lecture of State Key Laboratory of Networking and Switch- ing Technology at Beijing University of Posts and Telecommunications. His research interests span broad aspects of AI, NLP , big data analysis, object detection,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S132",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "deep learning, deep reinforce- ment learning, SDN, processing. Jianxin Liao obtained his Ph.D degree at Uni- versity of Electronics Science and Technology of China in 1996. He is currently the dean of Net- work Intelligence Research Center and the full professor of State Key laboratory of Networking and Switching Technology in Beijing University of Posts and Telecommunications. He has pub- lished hundreds of research papers and several books. His main research interests include cloud computing, mobile intelligent network, service network intelligent, networking architectures and protocols, and multimedia communication. Song Guo is a Full Professor and Associate Head (Research & Development) in the Depart- ment of Computing at The Hong Kong Polytech- nic University. His research interests are mainly in edge",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2206_08662v3:S133",
      "paper_id": "arxiv:2206.08662v3",
      "section": "method",
      "text": "AI, big data and machine learning, mo- bile computing, and distributed systems. He has served on IEEE Fellow Evaluation Committees for both CS and ComSoc, and been named on editorial board of a number of prestigious in- ternational journals like IEEE TC, IEEE TPDS, IEEE TCC, IEEE TETC, ACM CSUR, etc.",
      "page_hint": null,
      "token_count": 51,
      "paper_year": 2022,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9442602783056858,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 18,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5786,
        "empty": false
      },
      {
        "page": 2,
        "chars": 4132,
        "empty": false
      },
      {
        "page": 3,
        "chars": 5032,
        "empty": false
      },
      {
        "page": 4,
        "chars": 3762,
        "empty": false
      },
      {
        "page": 5,
        "chars": 5600,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4754,
        "empty": false
      },
      {
        "page": 7,
        "chars": 5326,
        "empty": false
      },
      {
        "page": 8,
        "chars": 5190,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5088,
        "empty": false
      },
      {
        "page": 10,
        "chars": 5027,
        "empty": false
      },
      {
        "page": 11,
        "chars": 4412,
        "empty": false
      },
      {
        "page": 12,
        "chars": 4472,
        "empty": false
      },
      {
        "page": 13,
        "chars": 3506,
        "empty": false
      },
      {
        "page": 14,
        "chars": 5465,
        "empty": false
      },
      {
        "page": 15,
        "chars": 5030,
        "empty": false
      },
      {
        "page": 16,
        "chars": 5886,
        "empty": false
      },
      {
        "page": 17,
        "chars": 7016,
        "empty": false
      },
      {
        "page": 18,
        "chars": 7245,
        "empty": false
      }
    ],
    "quality_score": 0.9443,
    "quality_band": "good"
  }
}