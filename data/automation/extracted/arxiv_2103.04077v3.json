{
  "paper": {
    "paper_id": "arxiv:2103.04077v3",
    "title": "Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration",
    "authors": [
      "Xiaofeng Gao",
      "Luyao Yuan",
      "Tianmin Shu",
      "Hongjing Lu",
      "Song-Chun Zhu"
    ],
    "year": 2021,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "Aligning humans' assessment of what a robot can do with its true capability is crucial for establishing a common ground between human and robot partners when they collaborate on a joint task. In this work, we propose an approach to calibrate humans' estimate of a robot's reachable workspace through a small number of demonstrations before collaboration. We develop a novel motion planning method, REMP, which jointly optimizes the physical cost and the expressiveness of robot motion to reveal the robot's reachability to a human observer. Our experiments with human participants demonstrate that a short calibration using REMP can effectively bridge the gap between what a non-expert user thinks a robot can reach and the ground truth. We show that this calibration procedure not only results in better user perception, but also promotes more efficient human-robot collaborations in a subsequent joint task.",
    "pdf_path": "data/automation/papers/arxiv_2103.04077v3.pdf",
    "url": "https://arxiv.org/pdf/2103.04077v3",
    "doi": null,
    "arxiv_id": "2103.04077v3",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:50:38.454059+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2103_04077v3:S1",
      "paper_id": "arxiv:2103.04077v3",
      "section": "body",
      "text": "IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 1 Show Me What You Can Do: Capability Calibration on Reachable Workspace for Human-Robot Collaboration Xiaofeng Gao1, Luyao Yuan1, Tianmin Shu 2, Hongjing Lu 3, Song-Chun Zhu 4",
      "page_hint": null,
      "token_count": 39,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S2",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "do with its true capability is crucial for establishing a common ground between human and robot partners when they collaborate on a joint task. In this work, we propose an approach to calibrate humans\u2019 estimate of a robot\u2019s reachable workspace through a small number of demonstrations before collaboration. We develop a novel motion planning method, reachability- expressive motion planning (REMP), which jointly optimizes the physical cost and the expressiveness of robot motion to reveal the robot\u2019s reachability to a human observer. Our experiments with human participants demonstrate that a short calibration using REMP can effectively align a non-expert user\u2019s estimation of the robot\u2019s reachability with its true capacity. We show that this calibration procedure not only results in better user",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S3",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "perception, but also promotes more ef\ufb01cient human-robot collaborations in a subsequent joint task. * Index Terms\u2014Human-Aware Motion Planning, Human Fac- tors and Human-in-the-Loop, Human-Robot Collaboration I. I NTRODUCTION O NE of the main challenges in Human-Robot Interaction is that the capacity of the robot perceived by the human partner may not be consistent with its actual capacity [1], [2], [3]. Such discrepancy may lead to overuse or misuse of the robot. Particularly, in an ad-hoc teaming setting where humans do not have prior experience with their robot partners, the consequence caused by such discrepancy could be detrimental to the team collaboration [4]. In this work, our key insights to address this challenge are two-fold: i) humans\u2019 perception of the capability",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S4",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "of a robot can be calibrated by observing its behavior, e.g., robot demonstrat- ing its motion trajectories in pursuit of certain goals, and ii) Manuscript received: September 9, 2021; Revised December 26, 2021; Accepted January 12, 2022. This paper was recommended for publication by Editor Gentiane Venture upon evaluation of the Associate Editor and Reviewers\u2019 comments. This work was supported by DARPA XAI N66001-17-2-4029. 1Xiaofeng Gao and Luyao Yuan are with UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA), Los Angeles, CA 90095, USA (e-mail: xfgao@ucla.edu, yuanluyao@ucla.edu) 2Tianmin Shu is with Department of Brain and Cognitive Sciences, Mas- sachusetts Institute of Technology, Cambridge, MA 02139, USA (e-mail: tshu@mit.edu) 3Hongjing Lu is with UCLA Department of Psychology, Los Angeles, CA",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S5",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "90095, USA (e-mail: hongjing@ucla.edu) 4Song-Chun Zhu is with Beijing Institute for General Arti\ufb01cial Intelligence (BIGAI), Beijing 100080, China, with School of Arti\ufb01cial Intelligence and Institute for Arti\ufb01cial Intelligence, Peking University, Beijing 100871, China, and also with Department of Automation, Tsinghua University, Beijing 100871, China (e-mail: sczhu@bigai.ai) Digital Object Identi\ufb01er (DOI): see top of this page. *Code and demos are available at https://xfgao.github.io/calib2022ral/. calibrating the perceived robot capability improves the quality of subsequent human-robot collaboration. We focus on a case study as shown in Figure 1, where a human user and a robot share the same workspace, and they must take turns picking up all objects as fast as possible. As the robot can only reach part of the workspace due",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S6",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "to its mechanical limits, the human partner needs to pick up the objects that the robot can not reach to achieve maximum ef\ufb01ciency in completing this joint task. We introduce capability calibration as shown in Figure 1b, where we allow the robot to show a small number of demonstrations. After watching each demonstration, the human can estimate the robot\u2019s capability accordingly. The goal is to come up with motion plans to pragmatically demonstrate the robot\u2019s capability. To achieve a sample-ef\ufb01cient calibration, we propose reachability-expressive motion planning (REMP), a novel planning algorithm that models perceived robot capability as a human\u2019s belief over a robot\u2019s reachable workspace, and inte- grates the belief update into motion planning by introducing an additional cost in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S7",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "trajectory optimization. As a result, REMP can generate a series of expressive trajectories for different robots to showcase their reachability to users. We conducted a user study in which participants i) \ufb01rst observed several robot demonstrations, then ii) estimated where the robot could reach, and iii) proceeded to work with the same robot in a joint task: picking up all objects in the shared workspace as fast as possi- ble. We \ufb01nd that i) REMP signi\ufb01cantly increases the accuracy of humans\u2019 reachability estimation, ii) the subsequent human- robot collaboration bene\ufb01ts from a successful calibration, iii) users perceive the robot as more predictable and reliable. II. R ELATED WORK Perceived robot capability. Various works have studied how humans perceive a robot\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S8",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "capability, differentiating between social and physical capabilities [5]. In prior work, social capabilities were de\ufb01ned as a robot\u2019s ability to communicate and interact with humans [6], and physical capabilities were de\ufb01ned on a set of tasks a robot can successfully perform [7], such as lifting different objects on the table [8], [9], searching and \ufb01re\ufb01ghting in various weather and \ufb01re conditions [10]. In these works, robots\u2019 capabilities of different tasks are estimated separately based on experience counts of action outcomes \u2013 a higher success rate indicates a stronger capability in a task [11]. Since the capability modeled in these works are highly task dependent, the user\u2019s knowledge of a robot\u2019s capability in one task can not be easily generalized to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S9",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "the arXiv:2103.04077v3 [cs.RO] 27 Jan 2022 2 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 (a) Inaccurate capability estimation can lead to failure in collaboration. (b) In this example, the human is supposed to pick up the white and the yellow cubes and let the robot collect the red and the green ones. Fig. 1: (a) Consider a collaborative table clearing task, where the robot has a limited capability and cannot reach the yellow and white objects. Inaccurate estimation of the robot\u2019s reachable workspace would harm collaboration: users who incorrectly estimate that the robot can reach the yellow object would assign it to the robot, resulting in a worse teaming performance. (b) We propose capability calibration, where",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S10",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "the robot uses its motion to demonstrate its capability before collaboration. knowledge of its capability in other tasks. In contrast, our work focuses on physical capabilities that serve as a basis for a robot to achieve success in a wide range of tasks. In particular, we focus on reachability, which is one of the most fundamental physical capabilities for robots. By understanding a robot\u2019s reachability, users can better assess its overall capability in various tasks where reaching is involved. Given an arbitrary task, the user can decide whether the robot can successfully perform it based on perceived reachability. Robot expressive motions. When deploying robots in real- world settings that are beyond factory environments, functional motions only designed to accomplish tasks",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S11",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "are inadequate for human users to correctly understand the robots and establish effective collaborations [12]. It is equally important to convey the rationality and intent of a robot through its motion [13], [14]. To generate such motions, prior work formulated and optimized the legibility of trajectories via functional gradient descent [15], [16]. Similar ideas were also adopted to study robots\u2019 expression of emotion [17] and style [18]. To express robot (in)capability, prior work used repetitive motions, either generated by simple heuristics [19] or hand-crafted for each task [20]. In contrast, [21] proposed a trajectory optimization- based method that maximizes the similarity between motions and would-be successful executions. Our work takes one further step in this direction: we i) model how",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S12",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "humans update their beliefs of the capability of a robot given the observed robot motions and ii) integrate the belief update process into trajectory optimization to generate new motions that can optimally improve humans\u2019 beliefs. III. C APABILITY CALIBRATION We propose a capability calibration framework (as shown in Figure 1b) to align a human user\u2019s understanding of a robot\u2019s capability with the ground truth, where the user can watch a small number of demonstrations of her robot partner before they work together. In this section, we introduce our approach to generate such demonstrations that can ef\ufb01ciently reveal the robot\u2019s reachability. We show how this calibration could be applied to collaboration in Section IV. A. Calibrating Reachable Workspace In this sub-section,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S13",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "we de\ufb01ne the reachability calibration task. In Section III-B, we describe how human belief would be updated before a new trajectory can be generated. In section III-C, we propose REMP, which enables the robot to generate one trajectory showing its reachable workspace based on a simulated human belief that models what the human has already known about the robot. In Section III-E, we reify our capability calibration framework by combining REMP and task planning. We begin with some notations. The robot\u2019s ground truth reachability is de\ufb01ned as f : Xws \u2192{0,1}, i.e. whether a target position x in the workspace Xws is reachable by the end-effector according to the robot\u2019s kinematic constraints. Meanwhile, we assume the human is maintaining a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S14",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "belief bt h : Xws \u2192[0,1], modeling how likely a target is reachable after observing a robot trajectory \u03bet 1:N \u2208\u039e with length N at time t \u2208{1...T}. After observing all the robot demonstrations, human\u2019s \ufb01nal belief would become bT h = \u03c4(b0 h,Z), with \u03c4 denoting the belief transition from the initial guess b0 h to the \ufb01nal bT h with all seen demonstrations Z = \u03be1:T . In addition, we de\ufb01ne Xrs \u2286Xws as the robot\u2019s reachable workspace. \u03c6ee : Q \u2192Xrs is the forward kinematic function of the end-effector, generating its position given a con\ufb01guration. Using notations above, we can formalize capability calibra- tion as an optimizing problem over a set of trajectories, Z, GAO et al.:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S15",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "CAPABILITY CALIBRATION ON REACHABLE WORKSPACE FOR HUMAN-ROBOT COLLABORATION 3 Fig. 2: Simulated human estimation of robot A\u2019s reachability map, after observing each demonstration generated by Algorithm 2, measured by Intersection of Union (IoU) between the human estimation and the ground truth. Robot A is a 2-link arm with link lengths 0.1. whose cardinality may not necessarily be known in advance. The goal is to reduce the mismatch between the robot\u2019s ground truth capability and the user\u2019s \ufb01nal belief: argmin Z\u2208\u039e\u2217 Cost(Z) (1) s.t. \u2211 x\u2208Xws \u23d0\u23d0\u23d0\u03c4 ( b0 h(x),Z ) \u2212f (x) \u23d0\u23d0\u23d0 < \u03b5, where \u039e\u2217 uses the Kleene star to represent all possible sequences of robot motions, Cost(\u00b7) : \u039e\u2217\u2192R is a function to evaluate the overall cost",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S16",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "for trajectories. The condition means the user has a reachability estimation close enough to the robot\u2019s true capability. One intuitive cost is the total length of all the trajectories in Z, optimizing which is equivalent to minimizing the cardinality of Z when the trajectories all have similar length. In this paper, we maintain the homogeneity of trajectories by further regulating the start con\ufb01gurations of all trajectories to be in a set of con\ufb01gurations S \u2282Q0 and target positions in a set of positions G \u2282Xrs. Due to the size of motion space, an exact solution to eq. (1) is intractable. Thus, we adopt an incremental update: we keep generating new trajectories \u03bet until the user\u2019s belief is suf\ufb01ciently aligned with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S17",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "the robot\u2019s capability. Every time we want to expand Z, we \ufb01rst select a pair of starting con\ufb01guration and target position (qt ,xr) \u2208S\u00d7G and then generate a motion using it. We term the former as the task planning problem and the latter as motion planning. B. Human Belief Model Our objective is to make people without any knowledge about robotics easily understand the true capacities of a robot. Thus, our human belief model attempts to capture what a novice user may think about a robot\u2019s reachability after watching its trajectories. We assume human updates its belief on an interested point x in the workspace after observing a new robot trajectory \u03be. Intuitively, if a point is close to the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S18",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "visited positions in an observed trajectory, the human observer would consider it more likely to be reachable. We model the belief update process as an iterative Bayesian inference beginning from a uniform prior: \u03c4(bt ,\u03bet )(x) =bt+1 h (x) \u221d bt h(x)p(\u03bet |x) (2) and p(\u03bet |x) = e\u2212\u03b3d(\u03c6(\u03bet ),x), where d(\u03c6(\u03be),x) captures the distance between the trajectory \u03be and the interested position Algorithm 1: REMP 1 Given a target position xr and a starting con\ufb01guration qt , human belief bt ; 2 Generate trajectory \u03bet based on bt h, qt , Equation (4) ; 3 Update human belief bt+1 h using \u03bet , Equation (2) ; 4 return bt+1 h , \u03bet x. The hyperparameter \u03b3 de\ufb01nes how much",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S19",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "the human ex- trapolates the observed trajectory to the points nearby: a large \u03b3 means that such extrapolation mainly happens to the point which is very close to the trajectory. In particular, we use the end-effector position \u03c6ee as the feature, and compute the squared euclidean distance between the interested position and the closest end-effector position in the trajectory: d(\u03c6(\u03be),x) =min i ||\u03c6ee(\u03bei)\u2212x||2. (3) The design of our distance function is motivated by the fact that, given a trajectory, it is straightforward for users to focus on the robot\u2019s end-effector which is central to the task, while trying to estimate its reachable workspace. C. REMP: Reachability-Expressive Motion Planning Expressing robot reachability is more than randomly mov- ing the end-effector to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S20",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "somewhere in its reachable workspace. Our insight is that it is essential to understand what the human already knows or does not know about the robot, so that every demonstration can communicate as much information to the human as possible. We believe this can be formulated as an optimization problem: \ufb01nding a new trajectory that would minimize the misalignment between the ground truth reachability and human\u2019s updated estimation. We capture the misalignment using a cost function c(\u03be,bt h, f ) and formulate the optimization problem as the following: \u03bet = argmin \u03be c(\u03be,bt h, f )+ 1 \u03bb N \u2211 i=1 ||\u03bei+1 \u2212\u03bei||2, subject to \u03c6ee(\u03ben) =xr,collision-free(\u03be). (4) The \ufb01rst term is an expressiveness cost and the second term is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S21",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "a smoothness cost commonly seen in trajectory optimization. The trajectory at the t-th step is generated by minimizing the sum of the two costs, subjecting to a constraint that requires the end effector to reach a target position xr at the end of the trajectory. Assuming each point in the trajectory contributes to the cost independently, we design the cost function based on a value 4 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 vi(\u03bei,bt h, f ), which represents the degree of alignment between human\u2019s estimation and the robot\u2019s ground truth reachable workspace: cb(\u03be,bt h, f ) =\u03b1 N \u2211 i=1 vi(\u03bei,bt h, f ) = \u03b1 N \u2211 i=1 e\u03b2 ( bt h(\u03c6ee(\u03bei))\u2212f (\u03c6ee(\u03bei)) )",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S22",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "(5) A small value vi suggests that the human observer is underestimating the robot\u2019s capability at \u03bei. In that case, we want to facilitate calibration by encouraging the robot to move to \u03bei. On the other hand, we would see a large vi if the human is over-estimating the capability. In that case, it is bene\ufb01cial for the robot to avoid reaching points near \u03bei. The hyperparameter \u03b1 and \u03b2 control how aggressive the trajectory would be in expressing the capability. We call this cost function cccb, which captures human updated belief. Note that the intuition is if the observer previously underestimates the reachability of a point x, bt h(x) \u2212f (x) will be negative and give low cost for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S23",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "trajectories covering x. Hence, trajectories passing through underestimated points are more likely to be chosen. Trajectories including overestimated points, on the contrary, have larger costs and are less likely to be selected. Static human model. Our key intuition is the human would update its belief of the robot\u2019s reachability after observing each trajectory. To test it, we also design a \ufb01xed cost function as baseline, assuming an underlying uniform belief model \u2200x, bstatic (x) =b0. The corresponding cost function under the assumption of a static human model is cccs. Note that this baseline generates functional motions that solely aim to \ufb01nish the physical task of reaching the target. We envision that in reality, users may also learn from these physical",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S24",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "motions the robot\u2019s capability by interacting with the robot on some tasks, but such learning is not as ef\ufb01cient as the learning in a dedicated calibration phase. D. Generating Reachability-Expressive Trajectories Implementation. We implemented our framework using TrajOpt [22] on two kinds of simulated robots in OpenRA VE [23], including a manipulator with 2 links and a PR2 robot. For the 2-link arm, we manipulated its joint limits and link lengths to allow it to have a variety of two-dimensional reachable workspaces. These serve as testing cases for our framework, as we want to study how well the framework copes with reachable workspaces of different sizes and shapes. For the PR2 robot, we didn\u2019t do such manipulations since the goal",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S25",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "here is to see how practical it is to apply the framework to real robot manipulators. Without loss of generality, we focus on the right arm of the PR2 robot. In practice, we use grid search to \ufb01nd hyperparameters that generate trajectories to maximize the accuracy of reachability estimation in simulation, as described in Section IV-C. Qualitative behaviors. Figure 2, Figure 3a and Figure 3b show the trajectories generated by the cost functions cb and cs for the robots by running REMP iteratively using the updated belief, following Algorithm 2. As both cb and cs assume a uniform belief on the robot\u2019s reachable workspace at the beginning, the \ufb01rst trajectories generated by these Algorithm 2: REMP-T 1 Given a list",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S26",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "of target position and starting con\ufb01guration pairs (x,q)1:T and human belief bh; 2 for t = 1,..., T do 3 bh,\u03bet = REMP(xt ,qt ,bh) 4 return bh,\u03be1:T Algorithm 3: Calibration on Reachable Workspace 1 Given a set of target positions G, starting con\ufb01gurations S, initial human belief b0; 2 for t = 1,2,..., do 3 \u2200x, b0 h(x) \u2190b0; 4 Let \u03c3(\u03b6) =\u2211x\u2208Xws \u23d0\u23d0 f (x)\u2212REMP-T(\u03b6,b0 h)[bh] \u23d0\u23d0; 5 \u03b6\u2217 t \u2190argmin\u03b6\u2208(G\u00d7S)t \u03c3(\u03b6) ; 6 \u03c3t \u2190\u03c3(\u03b6\u2217 t ); 7 if \u03c3t \u2212\u03c3t\u22121 < \u03b5 then 8 return REMP-T(\u03b6\u2217 t ,b0 h) cost functions are almost identical. Starting from the second trajectories, we \ufb01nd that the ones generated using cb can cover a large part of the robot\u2019s reachable",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S27",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "workspace. On the contrary, trajectories generated by cs are more sensitive to the physical cost. Overall, It is clear that REMP accommodates human belief at each time step and tries to traverse uncovered regions to better express the robot\u2019s reachability. E. Planning for Start and Target Pairs We have shown how REMP can generate an expressive trajectory given a starting con\ufb01guration and a target position. For a better capability calibration, we also want to optimize the number of trajectories as well as the sequence of starting con\ufb01gurations and target positions. As outlined in algorithm 3, this could be achieved by Task and Motion Planning (TAMP) [24], where the plans of start and target pairs come from task planning and the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S28",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "trajectories for a given pair comes from REMP. In algorithm 3, we solve (1) in an incremental manner. Namely, we keep increasing the cardinality of Z until user\u2019s belief is aligned with the robot\u2019s actual capability. For each size of Z, we \ufb01nd the best sequence of starting con\ufb01gurations from S and target positions from G (line 5 of algorithm 3). To avoid trajectories that are too short or uninformative, we set S as the set of con\ufb01gurations near the neutral con\ufb01guration of the robot and G as the set of positions far away from the neutral end effector positions: S = {q,\u2200q |q \u2212qneutral |< a1} (6) G = {x,\u2200x min q\u2208S |x \u2212\u03c6ee(q)|> a2} (7) Finding Z incrementally,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S29",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "despite giving the exact optimum, can be time consuming. In practice, rather than demonstrating to humans constantly until converge, we can pre-de\ufb01ne T to a reasonable number and \ufb01nd the optimal set of trajectories. In algorithm 4, we assume \ufb01xed number of trajectories. We start from a uniform prior for the human belief, and update the belief w.r.t. Eq. (2). Figure 4a depicts an example to optimize 4 trajectories that start from different con\ufb01gurations GAO et al.: CAPABILITY CALIBRATION ON REACHABLE WORKSPACE FOR HUMAN-ROBOT COLLABORATION 5 (a) Robot B is a 2-link arm with link lengths 0 .13 and 0 .07. (b) Robot C is a PR2 robot. In this work, we consider the reachable workspace of its right arm.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S30",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "Fig. 3: Visualization of the robot reachable workspace and the trajectories generated by cost function cb (belief ) and cs (static). (a) and (b) show the results for Robot B and Robot C respectively. It can be seen that the belief trajectories cover broader regions of the reachable workspace and new trajectories tend to visit areas that haven\u2019t been covered by their predecessors. The red dots, corresponding to Figure 5, represent the points we use to query the users in our experiments. Algorithm 4: Calibration with Fixed T 1 Given a set of target positions G = {x1,..., xN}, number of trajectories T, starting con\ufb01guration set S, initial human belief b0; 2 \u2200x, b0 h(x) \u2190b0, \u03b4 \u2190\u221e; 3 for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S31",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "\u03ba \u2208T \u2212combination(G \u00d7S) do 4 bh \u2190b0 h; 5 for t \u21901 to T do 6 bh, \u02c6\u03bet \u2190REMP(\u03bat (G),\u03bat (S),bh); 7 \u03c3 = \u2211x\u2208Xws |bh(x)\u2212f (x)|; 8 if \u03c3 < \u03b4 then 9 \u03b4 \u2190\u03c3; 10 \u03be1:T \u2190\u02c6\u03be1:T ; 11 return \u03be1:T and reach different targets. Note that algorithm 4 plans by enumerating all possible combinations, but any stochastic planning approaches can be used to further accommodate resource constraints and task scalability. IV. A PPLYING REMP TO HUMAN -ROBOT COLLABORATION In this section, we discuss how to apply REMP to human- robot collaboration after the calibration. A. Collaborative Table Clearing We design a human-robot collaboration task in a table clearing scenario, where some objects are scattered on a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S32",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "table and a robot can assist the human with the object collection. The human and the robot take turns picking up the objects. In each step, the human collects \ufb01rst and the robot collects one of the remaining objects. The human can reach all of the objects, while the robot can only reach a subset of them. To \ufb01nish the task as quickly as possible, the human and the robot need to split the work wisely, so that, in each round, the robot has some objects to pick up. The reward is calculated by the number of objects picked up and the time penalty. B. Human and Robot Policy After observing robot expressive demonstrations and up- dating the belief with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S33",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "Eq. (2), the human is assumed to act in an approximately rational way with respect to the current estimation of the robot capability, bt h. We use a Boltzmann noisily-rational human decision model [25], assuming the human is more likely to help the robot with its unreachable objects based on the human\u2019s current reachability estimation. Since we want to emphasize the effect of the calibration, we use a simple uniform robot policy in the simulation, i.e., it would randomly pick up objects it can reach, and do nothing if no objects are reachable. C. Simulation Results Using the behavior model described in the previous sections, we simulated with 3 robots A, B and C with different con- \ufb01gurations and reachability:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S34",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "(i) A is a 2-link arm where each link is of equal length, (ii) B is a 2-link arm where the length of its \ufb01rst link (0.13) is larger than the length of the second (0.07), (iii) C is a PR2 robot. The belief and static methods in the legend correspond to the de\ufb01nition in Section III-B. In addition, we implemented a traversal baseline, where the robot moves its end-effector to traverse the workspace to demonstrate its reachability. From the starting position, the 6 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 (a) Trajectories generated by task and mo- tion planning and the simulated reachabil- ity estimation given observed trajectories. (b1) Simulated reachability estimation accuracy, measured by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S35",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "Intersection of Union between the human reachability estimation and the ground truth. Higher value indicates better estimation. (b2) Simulated human-robot collaboration per- formance measured by averaged rewards ac- quired by the group. Every point on the curves for traversal is the mean of 100 trajectories. Fig. 4: (a) Combining REMP with task planning, we can optimize the starting and target positions for better calibration. (b) Simulation results of reachability estimation and collaboration performance. Starting and target positions are chosen greedily. (a) 36 points for 2-link arms. (b) 25 points for PR2. Fig. 5: To evaluate users\u2019 estimation of the robot\u2019s reachable workspace, we sample query points in the workspace and ask users to select points that they think the robot\u2019s",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S36",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "end effector can reach. These points correspond to the red dots in Figure 3a and Figure 3b. end effector moves to unreached waypoints in its reachable workspace one by one. The number of waypoints we sampled corresponds to the number of trajectories in belief and static. Figure 4b1 and Figure 4b2 shows the quantitative results of capability calibration and human-robot collaboration. The result suggests that as the robot shows more demonstrations, the human has a better understanding of its capability and collaborates with it more effectively for all baselines. Looking at the sample ef\ufb01ciency, we notice that without modeling human belief changes, the improvement is quite slow and limited: a large number of demonstrations need to be observed before calibration",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S37",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "is achieved. On the contrary, trajectories generated by our proposed REMP algorithm keep providing new information to the user. As a result, the user\u2019s estimation accuracy increases much faster for belief compared to the baselines. There is \ufb02uctuation when many trajectories are shown, due to the limited memory of our human model. V. U SER STUDY As we have shown the effectiveness of REMP in simulation, we now turn to investigate how much it helps users work with TABLE I: Survey statements to evaluate reachability, pre- dictability, reliability and trust toward robots. 1. It is easy to tell where the robot\u2019s hand can reach. 2. The robot behaves in a predictable manner. 3. I can rely on the robot to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S38",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "function properly despite its limited capability. 4. I trust the robot. robots in a user study. This study was certi\ufb01ed as exempt from IRB review per 45 CFR 46.104 category 3 by the UCLA Institutional Review Board on 9/4/2020. A. Experiment Design Participants. We recruited 202 participants (37% Female, median age 34) from Amazon Mechanical Turk. Materials. During the study, participants interact with the three robots in the simulation as described in Section IV-C. We measure how well participants can understand the robot\u2019s capability and how such understanding can help them in the collaboration, as well as their self-reported perception of the robot. To measure capability understanding, we ask users to choose positions that they think the robot can reach",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S39",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "from a number of object queries, as shown in Figure 5. We record their selections and compare them with the ground truth. For collaboration task performance , we use the accumulated reward of the team as a measure. To measure the perception of the robot , we ask participants to rate statements listed in Table I on a 7-point Likert scale labeled from \u201dstrongly agree\u201d to \u201dstrongly disagree\u201d, after they have \ufb01nished interaction with a robot. Inspired by [26], the statements shown in Table I are designed to evaluate their subjective understanding of the robot in different aspects, including reachability, predictability, reliance and trust. Calibration task. In the calibration task, the participant would be randomly assigned to an experiment group,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S40",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "and observe T demonstrations. Based on the simulations results in Fig- ure 4b1, we believe showing more demonstrations would gen- erally lead to a better calibration. Considering the limited time of participants in our online study, however, we cannot use GAO et al.: CAPABILITY CALIBRATION ON REACHABLE WORKSPACE FOR HUMAN-ROBOT COLLABORATION 7 (a) Intersection of Union between the human reachability estimation and the ground truth. A higher value indicates better estimation. (b) The human-robot team performance in the collaboration task. A higher value indicates a higher reward. (c) Users\u2019 ratings toward the Likert state- ments in table I. A higher rating indicates higher con\ufb01dence. Fig. 6: User study results. Here we report means and standard errors. * indicates statistical signi\ufb01cant",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S41",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "pairs ( p < .05). an arbitrarily large T. From simulation, we witness the most signi\ufb01cant improvement during the \ufb01rst 4 trajectories, thus we control the number of demonstrations T = 4 in practice. After seeing all demonstrations, participants are asked to estimate the robot\u2019s reachable workspace by choosing positions that they think the robot can reach from a number of object queries, as shown in Figure 5. Collaboration task. In the collaboration task, participants are asked to perform an online table clearing task together with the same robot they have just been calibrated. As discussed in Section IV-A, the task required the team to clear all four objects on the table. During each time step, the participant would pick",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S42",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "up an object \ufb01rst before the robot makes its decision. The team would get rewarded based on how fast they take all the objects. Since two of the objects cannot be reached by the robot, to get the maximum accumulated reward (+2), the participant needs to pick up objects that cannot be reached by the robot. Failure to do so would result in the team getting a lower reward (0). Experiment conditions. Like simulations, we varied types of motion users observed in the user study, i.e., the belief, static and traversal methods de\ufb01ned in Section IV-C. Design. The robot types are within-subject: participants in- teracted with all three robots. Demonstrations are between- subject: participants only saw demonstrations from one of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S43",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "the three experiment conditions when interacting with a robot. Procedure. After a brief introduction, each participant is asked to interact with three robots A, B and C in random order. The purpose is to see how robot\u2019s physical con\ufb01gurations affect capability perception. During the interaction with each robot, the participants would \ufb01rst go through a calibration task before collaborating with the same robot on the table clearing task. Hypotheses. We hypothesized the capability calibration framework bene\ufb01ts the users in the following aspects: H1: Participants going through capability calibration in the belief condition would have a better understanding of the robot\u2019s capability, compared to those in the other conditions. H2: Teams in the belief condition would perform better in the collaboration",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S44",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "tasks than those in the other conditions. H3: Participants in the belief condition would have a more positive perception of the robot, compared to those in the other conditions. B. Result and Analysis Capability understanding. We \ufb01rst analyzed the accuracy of the user\u2019s estimation of the robot\u2019s reachable workspace, by computing the intersection of union (IoU) between their re- sponses and the ground truth. We performed a Kruskal\u2013Wallis H-test of the IoUs using the type of motion independent variables. As a result, we found a signi\ufb01cant effect for the mo- tion (\u03c72(2,603) =113.52, p < .001). A post-hoc analysis with Mann-Whitney U test revealed that all three conditions are different from each other, with belief signi\ufb01cantly better than static (p",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S45",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "< .001) and traversal (p < .001). This con\ufb01rms our hypothesis H1. Figure 6a shows the accuracy of participants\u2019 reachability estimation w.r.t different robots. On average,belief performs 65% better than static and 32% better than traversal. Compared to the simulation results in Figure 4, the user study result follows relatively the same order for different conditions. Task performance. We also analyzed the collaboration task performance. A Kruskal\u2013Wallis H-test indicates that there is a statistically signi\ufb01cant effect of the accumulated rewards between conditions ( \u03c72(2,603) =22.62, p < .001). The post- hoc Mann-Whitney U test showed a signi\ufb01cant difference between belief and static (p < .001). This partially supports our hypothesis H2. We didn\u2019t observe a signi\ufb01cant difference between belief and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S46",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "traversal. Figure 6b shows the task per- formance for different robots in three conditions. The Pearson correlation coef\ufb01cient between reachability estimation and collaboration performance is r = .203 with p-value smaller than 0.001, indicating a positive correlation. This validates that calibrating perceived robot capability bene\ufb01ts the collaboration performance. Surprisingly, users in the traversal condition have a slightly higher reward when collaborating with the PR2 robot compared with those in static, even if their reachability estimation is less accurate, although the difference is not signi\ufb01cant. This is probably due to the stochastic nature of the traversal baseline and speci\ufb01c object locations in our collaboration task. Perception of robots. Finally, we analyzed participants\u2019 perception toward robots. Running a Kruskal\u2013Wallis H-test, we found",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S47",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "signi\ufb01cant effects for reachability ( \u03c72(2,603) = 20.39, p < .001) and predictability ( \u03c72(2,603) =11.30, p = .003). The post-hoc Mann-Whitney U test revealed signi\ufb01- cant difference between belief and traversal for reachability 8 IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JANUARY , 2022 (p < .001) and predictability ( p < .001), con\ufb01rming H3. Overall, users tended to prefer belief over static, and static over traversal. This is unexpected for reachability, considering the fact that users are actually better at predicting robots\u2019 reachability in traversal than in static. The Pearson correlation coef\ufb01cient between reachability rating and prediction accuracy is r = .109, indicating a weak positive correlation. Similarly, we observe a weak correlation r = .019 between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S48",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "self-reported reliance and users\u2019 actual ability to rely on the robot during collaboration. This suggests that there may be a discrepancy between the users\u2019 self-reported capability understanding and what they actually know about the robot. In summary, we found that users in the belief condition had the most accurate estimation of the robots\u2019 capability, and reported the robots in this condition as the most reliable, the most predictable, and the easiest to understand among all three conditions. Moreover, users working with the belief robots achieved a higher reward than those working with the static robots did. These objective and subjective results together suggest that our approach has an overall advantage for improving humans\u2019 understanding of robots as well as the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S49",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "quality of collaboration over the baselines. VI. C ONCLUSION We have proposed an expressive robot motion planning algorithm, REMP, which can generate informative trajectories by integrate human belief update into trajectory optimiza- tion. Our experiments show that our approach can ef\ufb01ciently calibrate a user\u2019s perception of a robot\u2019s reachability and consequently improve human-robot collaboration. In this work, we focused on the robot\u2019s spatial reachability. As reaching is one of the most basic tasks in human-robot in- teraction, we believe understanding reachability would greatly help users understand robot capacities in more complex tasks. Thus we view our work as a successful \ufb01rst step towards a more general capability calibration setting. In the future, it is possible to extend REMP for other",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S50",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "capabilities. Our current work treats predictability and reliability as separate measures from trust. Considering the multidimensional nature of trust, better instruments can be used for a more comprehensive trust measure [27]. Also, due to online experiment constraints, we only investigated the reachability calibration problem on a 2D plane. We intend to generalize our algorithm to 3D environment in future work. REFERENCES [1] T. A. Stoffregen, K. M. Gorday, Y .-Y . Sheng, and S. B. Flynn, \u201cPerceiv- ing affordances for another person\u2019s actions.\u201d Journal of Experimental Psychology: Human Perception and Performance, vol. 25, no. 1, p. 120, 1999. [2] A. Powers and S. Kiesler, \u201cThe advisor robot: tracing people\u2019s mental model from a robot\u2019s physical attributes,\u201d in Proceedings of the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S51",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "1st ACM SIGCHI/SIGART conference on Human-robot interaction, 2006, pp. 218\u2013225. [3] S. R. Fussell, S. Kiesler, L. D. Setlock, and V . Yew, \u201cHow people anthro- pomorphize robots,\u201d in 2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2008, pp. 145\u2013152. [4] S. V . Albrecht and P. Stone, \u201cReasoning about hypothetical agent behaviours and their parameters,\u201d in Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems , ser. AAMAS \u201917. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2017, p. 547\u2013555. [5] E. Cha, A. D. Dragan, and S. S. Srinivasa, \u201cPerceived robot capability,\u201d in 2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). IEEE, 2015, pp. 541\u2013548. [6] M. F.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S52",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "Jung, J. J. Lee, N. DePalma, S. O. Adalgeirsson, P. J. Hinds, and C. Breazeal, \u201cEngaging robots: easing complex human-robot teamwork using backchanneling,\u201d in Proceedings of the 2013 conference on Computer supported cooperative work, 2013, pp. 1555\u20131566. [7] F. A. Robinson, M. Velonaki, and O. Bown, \u201cSmooth operator: Tuning robot perception through arti\ufb01cial movement sound,\u201d in Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction, 2021, pp. 53\u201362. [8] S. Nikolaidis, S. Nath, A. D. Procaccia, and S. Srinivasa, \u201cGame- theoretic modeling of human adaptation in human-robot collaboration,\u201d in Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction, 2017, pp. 323\u2013331. [9] M. Chen, S. Nikolaidis, H. Soh, D. Hsu, and S. Srinivasa, \u201cPlan- ning with trust for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S53",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "human-robot collaboration,\u201d in Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 2018, pp. 307\u2013315. [10] Y . Xie, I. P. Bodala, D. C. Ong, D. Hsu, and H. Soh, \u201cRobot ca- pability and intention in trust-based decisions across tasks,\u201d in 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2019, pp. 39\u201347. [11] J. Lee, J. Fong, B. C. Kok, and H. Soh, \u201cGetting to know one another: Calibrating intent, capabilities and trust for human-robot collaboration,\u201d arXiv preprint arXiv:2008.00699, 2020. [12] G. Venture and D. Kuli \u00b4c, \u201cRobot expressive motions: a survey of generation and evaluation methods,\u201d ACM Transactions on Human- Robot Interaction (THRI), vol. 8, no. 4, pp. 1\u201317, 2019. [13] D. Sza\ufb01r, B. Mutlu, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S54",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "T. Fong, \u201cCommunication of intent in assis- tive free \ufb02yers,\u201d in Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction, 2014, pp. 358\u2013365. [14] G. Lemasurier, G. Bejerano, V . Albanese, J. Parrillo, H. A. Yanco, N. Amerson, R. Hetrick, and E. Phillips, \u201cMethods for expressing robot intent for human\u2013robot collaboration in shared workspaces,\u201d ACM Transactions on Human-Robot Interaction (THRI), vol. 10, no. 4, pp. 1\u201327, 2021. [15] A. Dragan and S. Srinivasa, \u201cGenerating legible motion,\u201d 2013. [16] F. Stulp, J. Grizou, B. Busch, and M. Lopes, \u201cFacilitating intention prediction for humans by optimizing robot motions,\u201d in 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS) . IEEE, 2015, pp. 1249\u20131255. [17] M. L. Felis, K. Mombaur, and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S55",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "A. Berthoz, \u201cAn optimal control approach to reconstruct human gait dynamics from kinematic data,\u201d in2015 IEEE- RAS 15th International Conference on Humanoid Robots (Humanoids). IEEE, 2015, pp. 1044\u20131051. [18] C. K. Liu, A. Hertzmann, and Z. Popovi \u00b4c, \u201cLearning physics-based motion style with nonlinear inverse optimization,\u201d ACM Transactions on Graphics (TOG), vol. 24, no. 3, pp. 1071\u20131081, 2005. [19] M. N. Nicolescu and M. J. Mataric, \u201cLearning and interacting in human- robot domains,\u201d IEEE Transactions on Systems, man, and Cybernetics- part A: Systems and Humans, vol. 31, no. 5, pp. 419\u2013430, 2001. [20] L. Takayama, D. Dooley, and W. Ju, \u201cExpressing thought: improving robot readability with animation principles,\u201d in Proceedings of the 6th international conference on Human-robot interaction, 2011, pp.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S56",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "69\u201376. [21] M. Kwon, S. H. Huang, and A. D. Dragan, \u201cExpressing robot incapa- bility,\u201d in Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 2018, pp. 87\u201395. [22] J. Schulman, J. Ho, A. X. Lee, I. Awwal, H. Bradlow, and P. Abbeel, \u201cFinding locally optimal, collision-free trajectories with sequential con- vex optimization.\u201d in Robotics: science and systems, vol. 9, no. 1. Citeseer, 2013, pp. 1\u201310. [23] R. Diankov, \u201cAutomated construction of robotic manipulation pro- grams,\u201d Ph.D. dissertation, USA, 2010. [24] L. P. Kaelbling and T. Lozano-P \u00b4erez, \u201cHierarchical task and motion planning in the now,\u201d in 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011, pp. 1470\u20131477. [25] O. Morgenstern and J. V on Neumann, Theory of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2103_04077v3:S57",
      "paper_id": "arxiv:2103.04077v3",
      "section": "abstract",
      "text": "games and economic behavior. Princeton university press, 1953. [26] M. Madsen and S. Gregor, \u201cMeasuring human-computer trust,\u201d in 11th australasian conference on information systems, vol. 53. Citeseer, 2000, pp. 6\u20138. [27] B. F. Malle and D. Ullman, \u201cA multidimensional conception and measure of human-robot trust,\u201d in Trust in Human-Robot Interaction. Elsevier, 2021, pp. 3\u201325.",
      "page_hint": null,
      "token_count": 54,
      "paper_year": 2021,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9550823357435655,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 8,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 5938,
        "empty": false
      },
      {
        "page": 2,
        "chars": 4507,
        "empty": false
      },
      {
        "page": 3,
        "chars": 4699,
        "empty": false
      },
      {
        "page": 4,
        "chars": 5718,
        "empty": false
      },
      {
        "page": 5,
        "chars": 3461,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4313,
        "empty": false
      },
      {
        "page": 7,
        "chars": 5463,
        "empty": false
      },
      {
        "page": 8,
        "chars": 8125,
        "empty": false
      }
    ],
    "quality_score": 0.9551,
    "quality_band": "good"
  }
}