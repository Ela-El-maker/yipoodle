{
  "paper": {
    "paper_id": "arxiv:2306.05399v2",
    "title": "Matting Anything",
    "authors": [
      "Jiachen Li",
      "Jitesh Jain",
      "Humphrey Shi"
    ],
    "year": 2023,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance. MAM offers several significant advantages over previous specialized image matting networks: (i) MAM is capable of dealing with various types of image matting, including semantic, instance, and referring image matting with only a single model; (ii) MAM leverages the feature maps from the Segment Anything Model (SAM) and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha matte through iterative refinement, which has only 2.7 million trainable parameters. (iii) By incorporating SAM, MAM simplifies the user intervention required for the interactive use of image matting from the trimap to the box, point, or text prompt. We evaluate the performance of MAM on various image matting benchmarks, and the experimental results demonstrate that MAM achieves comparable performance to the state-of-the-art specialized image matting models under different metrics on each benchmark. Overall, MAM shows superior generalization ability and can effectively handle various image matting tasks with fewer parameters, making it a practical solution for unified image matting. Our code and models are open-sourced at https://github.com/SHI-Labs/Matting-Anything.",
    "pdf_path": "data/automation/papers/arxiv_2306.05399v2.pdf",
    "url": "https://arxiv.org/pdf/2306.05399v2",
    "doi": null,
    "arxiv_id": "2306.05399v2",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 17:55:18.074137+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_2306_05399v2:S1",
      "paper_id": "arxiv:2306.05399v2",
      "section": "body",
      "text": "Matting Anything Jiachen Li1, Jitesh Jain1, Humphrey Shi1,2 1SHI Labs @ Georgia Tech & Oregon & UIUC, 2Picsart AI Research (PAIR) https://github.com/SHI-Labs/Matting-Anything Semantic Matting Model Instance Matting Model Matting Anything Model I mage I mage I mage pr ompt (bo x) (A) Semantic Matting Instance Matting Matting Anything pr ompt (point) pr ompt (text) a dog in the middle side (B) (C) Figure 1. Matting Anything Model (MAM) offers a versatile framework capable of addressing various types of image matting scenarios with a single model. Compared to previous specialized models for (A) Semantic Matting, which outputs a single alpha matte of all instances in the foreground; (B) Instance Matting, which returns alpha mattes of all human instances; (C) Matting Anything",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S2",
      "paper_id": "arxiv:2306.05399v2",
      "section": "body",
      "text": "Model can estimate the alpha matte of any target instance with user prompts as boxes, points, or text descriptions for interactive use by incorporating SAM [21]. It further reaches comparable performance to the specialized matting models on multiple benchmarks, and shows superior generalization ability with fewer parameters as a unified image matting model.",
      "page_hint": null,
      "token_count": 53,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S3",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimat- ing the alpha matte of any instance in an image with flex- ible and interactive visual or linguistic user prompt guid- ance. MAM offers several significant advantages over pre- vious specialized image matting networks: (i) MAM is ca- pable of dealing with various types of image matting, in- cluding semantic, instance, and referring image matting with only a single model; (ii) MAM leverages the feature maps from the Segment Anything Model (SAM) [21] and adopts a lightweight Mask-to-Matte (M2M) module to pre- dict the alpha matte through iterative refinement, which has only 2.7 million trainable parameters. (iii) By incorporat- ing SAM, MAM simplifies",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S4",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "the user intervention required for the interactive use of image matting from the trimap to the box, point, or text prompt. We evaluate the performance of MAM on various image matting benchmarks, and the experimental results demonstrate that MAM achieves com- parable performance to the state-of-the-art specialized im- age matting models under different metrics on each bench- mark. Overall, MAM shows superior generalization ability and can effectively handle various image matting tasks with fewer parameters, making it a practical solution for unified image matting. Our code and models are open-sourced at https://github.com/SHI-Labs/Matting-Anything. 1. Introduction Image Matting, as a long-standing computer vision task, aims to estimate the alpha matte \u03b1 given an input image I [45]. The matting target is mainly",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S5",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "around human beings or other objects at the semantic level [26,41,49]. Recent works 1 arXiv:2306.05399v2 [cs.CV] 16 Nov 2023 have extended the scope of image matting to more complex scenarios like image instance matting [42], which requires instance-aware alpha matte predictions and referring image matting [28], which extracts the alpha matte given natural language description. Previous deep learning-based image matting meth- ods [28, 29, 36, 37, 42, 47, 51, 54, 59] have been proposed to address specific image matting tasks on correspond- ing benchmarks. These methods are tailored to individual datasets and lack the flexibility to handle various image matting tasks due to their fixed model designs. This lim- itation has hindered the development of more generalized and versatile image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S6",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "matting models. As a result, there is a growing interest in developing more adaptive and efficient image matting frameworks that can handle different types of image matting tasks with a single model. Furthermore, previous image matting methods have re- lied on user-guided trimaps as auxiliary inputs to achieve accurate alpha matte predictions. Although some trimap- free methods have been proposed that use mask guidance or background images instead [39, 55], they are unable to estimate the alpha matte of the target instance based on the user request for interactive use. Therefore, it is crucial to develop a model that can achieve accurate alpha matte es- timation without relying on user-guided trimaps, while also being capable of handling simple user requests",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S7",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "in a flexi- ble and efficient manner for interactive use. Such a model would significantly enhance the user experience by reduc- ing the extra need for manual intervention. Motivated by these limitations of image matting, we pro- pose the Matting Anything Model (MAM), a versatile net- work that can estimate the alpha matte of any target in- stance with prompt-based user guidance in an image as shown in Figure 1. MAM leverages the recent Segment Anything Model (SAM) framework [21], which supports flexible prompting and outputs segmentation masks of any target instance for interactive use. Specifically, MAM takes the feature maps and mask outputs from SAM as inputs and adds a lightweight Mask-to-Matte (M2M) module to predict the alpha matte",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S8",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "of the target instance. We trained MAM on a combination of five image matting datasets that cover differ- ent classes of instances, allowing the M2M module to learn generalizable features for image matting. During training, we randomly place target instances onto background im- ages and use a pre-trained SAM to output mask predictions of the corresponding instances. The trainable M2M mod- ule then refines the mask by predicting multi-scale alpha mattes. Through an iterative refinement process based on the mask or the alpha matte, the multi-scale predictions are merged to obtain the final meticulous alpha matte. We conducted extensive evaluations of MAM on six im- age matting benchmarks, including semantic image mat- ting benchmark PPM-100 [40], AM2K [25] PM-10K [25],",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S9",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "the instance image matting benchmark RWP636 [55], HIM2K [42], and the referring image matting benchmark RefMatte-RW100 [28]. Our results demonstrate that MAM achieves performance comparable to that of state-of-the-art image matting models across all benchmarks under differ- ent evaluation metrics. The experimental results highlight the versatility and effectiveness of our proposed approach for handling various image matting tasks in an interactive and efficient manner. 2. Related Works 2.1. Image Matting Given an image I, which can be view as a combination of foreground image F and background image B with co- efficient alpha matte \u03b1, I = \u03b1F + (1\u2212 \u03b1)B (1) Image Matting is to estimate \u03b1 given only I as inputs. Traditional methods rely on a user-guided trimap,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S10",
      "paper_id": "arxiv:2306.05399v2",
      "section": "abstract",
      "text": "which explicitly annotates the absolute foreground area, absolute",
      "page_hint": null,
      "token_count": 8,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S11",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "image matting solutions use low-level features to distin- guish the transition areas by measuring the similarities be- tween foreground and background neighbors [1,2,5,8,9,11]. Recently, deep learning-based methods [27, 36, 47, 51, 54, 59] adopt neural networks to estimate the alpha matte in an end-to-end manner with trimap as auxiliary inputs. Some trimap-free methods use background image [39], mask guidance [32, 55], or segmentation data [4, 30] to make up the absence of trimap. When the image I contains multiple instances, the composition turns to I = NX i \u03b1iFi + (1\u2212 NX i \u03b1i)B (2) \u03b1i represents the alpha matte of instancei and InstMatt [42] adopt the target and reference mask as guidance to predic- tion instance-aware alpha matte prediction.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S12",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "Interactive mat- ting methods [28, 48, 52] develop specialized models that use point, boxes, or text input to estimate the alpha matte of the target instance. MatAny [53] is a concurrent work that also adopts SAM for semantic image matting. In terms of video matting, trimap-free [22\u201324, 30, 40] methods are ex- plored for real-time inference while the per-frame predic- tion quality is not comparable to image matting methods. However, these methods are designed for a certain scenario with corresponding benchmarks, which limits their poten- tial to handle various image matting tasks and benchmarks simultaneously. 2.2. Image Segmentation Image segmentation is a close research area to image matting, while it predicts the binary mask of different in- 2 Segment Anything",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S13",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "Model (SAM) Image Prompt (box/point) RB RB RB Mask-to-Matte (M2M) Module Feature Maps Frozen Weights Iterative Refinement RB Refinement Block maskPrompt (text) a girl with black/red sweater Figure 2. Matting Anything Model Architecture. The MAM architecture consists of a pre-trained SAM and an M2M module. Given an input image I, SAM generates the mask prediction for the target instance based on the box or point user prompt. The M2M module takes the concatenated inputs, including the image, mask, and feature maps, and produces multi-scale predictions \u03b1os8, \u03b1os4, and \u03b1os1. The iterative refinement process, detailed in Section 3, progressively improves the precision of the final meticulous alpha matte\u03b1, incorporating information from the multi-scale outputs. stances in the image. Similar to image",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S14",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "matting, many im- age segmentation methods are tailored for a specific im- age segmentation task, like semantic segmentation [3, 15], instance segmentation [13, 46], and panoptic segmenta- tion [20, 44]. Recent works started to explore transformer- based frameworks [6, 12, 16, 17] for unified image segmen- tation. Language-guided segmentation frameworks [50, 58] look for text supervision to segment instance-aware masks. OneFormer [16] adopts a single transformer model to learn with a joint training strategy and performs universal seg- mentation across semantic, instance and panoptic segmen- tation and outperforms specialized models. SAM [21] takes a further step recently, which supports flexible prompting from users to segment any instance in an image for inter- active use. Grounded-SAM [33] incorporates DINO with SAM",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S15",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "to add text prompt support. Foundation models like SAM offer opportunities for other areas to develop versatile frameworks to support a range of applications. 3. Matting Anything In this section, we provide an overview of the Matting Anything Model (MAM) architecture, which consists of two main components: the frozen Segment Anything Model (SAM) and the trainable Mask-to-Matte (M2M) module. We first provide a brief review of the SAM, which is de- signed to produce high-quality instance segmentation given user-guided prompts. We then introduce the M2M mod- ule, which enables the transformation of the binary masks into high-quality alpha mattes. Finally, we describe how we connect the M2M module with the SAM to gradually build the end-to-end MAM. 3.1. Segment Anything",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S16",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "Model Segment Anything is a recently proposed foundation model for segmentation. Given an image I \u2208 R3\u00d7H\u00d7W , SAM uses a ViT-based image encoder to obtain deep feature maps F \u2208 RC\u00d7 H 16 \u00d7W 16 . Then, a variety of N input prompts are encoded by the prompt encoder and sent to the mask de- coder with the feature maps. The mask decoder returns a set of mask candidates mi \u2208 R1\u00d7H\u00d7W , i\u2208 N indicated by the input prompts. With its flexible prompting mechanism, SAM allows for interactive use and is easily adaptable for downstream tasks. 3.2. Mask-to-Matte The Mask-to-Matte (M2M) module is an integral com- ponent of our Matting Anything Model (MAM) and is de- signed to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S17",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "convert instance-aware mask predictions from SAM into instance-aware alpha matte predictions efficiently and smoothly. To achieve this, we utilize the feature maps and mask predictions generated by SAM as auxiliary inputs to M2M. To improve the accuracy of our predictions, we adopt multi-scale branches for predicting the alpha matte and merge these predictions through an iterative refinement schedule. Multi-Scale Prediction: Given an input image I \u2208 R3\u00d7H\u00d7W , the pre-trained SAM model produces feature maps F \u2208 RC\u00d7 H 16 \u00d7W 16 and mask prediction m \u2208 R1\u00d7H\u00d7W on the target instance with prompt guidance. We concate- nate the rescaled image, mask, and feature maps to form 3 Task Semantic Matting Instance Matting Referring Matting Benchmark AM2K PM-10K PPM-100 HIM2K",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S18",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "RWP636 RefMatte-RW100 Metric SADall\u2193 MADall\u2193 MSEall\u2193 IMQnat mad\u2191 IMQnat mse\u2191 IMQmad\u2191 IMQmse\u2191 SADall\u2193 MSEall\u2193 Specialized Models GFM-R [25] 10.89 6.7 - - - - - - - GFM-D [25] 10.26 6.9 - - - - - - - MODNet [19] - - 4.4 - - - - - - MGMatting [55] - - - 57.98 71.12 30.64 53.16 - - InstMatt [42] - - - 70.26 81.34 51.10 73.09 - - CLIPMat-B [28] - - - - - - - 107.81 59.5 CLIPMat-L [28] - - - - - - - 85.83 47.4 Generalized Models SAM [21] 25.00 25.7 10.8 61.15 74.01 49.87 56.92 33.51 17.9 MAM 17.30 15.4 4.6 68.78 81.67 54.40 76.45 29.24 15.1 Table 1. Comparisons between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S19",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "specialized matting models and MAM on various benchmarks. \u2191 / \u2193 means higher / lower values indicate better performance for the corresponding metric. Gray text refers to models specifically designed for these benchmarks. MAM shows clear improvements over SAM and superior generalization ability as a unified image matting model. the input Fm2m \u2208 R(C+4)\u00d7H 8 \u00d7W 8 to the M2M module. M2M employs several refinement blocks [7,55], which con- tain connected self-attention layer [56], batchnorm layer, and activation layer, to generate alpha matte predictions at 1/8 resolution, denoted as \u03b1os8 \u2208 R1\u00d7H 8 \u00d7W 8 . The feature maps are then upsampled to higher resolutions to make al- pha matte predictions at 1/4 and full resolution, denoted as \u03b1os4 \u2208",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S20",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "R1\u00d7H 4 \u00d7W 4 and \u03b1os1 \u2208 R1\u00d7H\u00d7W , respectively. The multi-scale predictions enable MAM to handle objects of varying scales and provide finer-grained alpha mattes for detailed object extraction. Iterative Refinement To improve the accuracy of global and local predictions, we use an iterative refinement pro- cess. We first compute weight maps wos8, wos4, and wos1 that highlight different areas of the image during training like trimaps. These weight maps are used to compute losses for each scale of prediction, with wos8 emphasizing the en- tire image for \u03b1os8 predictions, wos4 filtering out the back- ground for \u03b1os4 predictions, and wos1 focusing only on the transition areas. During inference, we gradually merge the predictions of \u03b1os8, \u03b1os4, and \u03b1os1",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S21",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "with the mask predic- tions m from SAM to obtain the final alpha matte prediction \u03b1 \u2208 R1\u00d7H\u00d7W . 3.3. Matting Anything Model After the development of the Mask-to-Matte (M2M) module, we integrate it with the Segment Anything Model (SAM) to enable end-to-end training and inference for the Matting Anything Model (MAM). This integration allows for a comprehensive and unified framework that handles the entire matting process, from feature extraction to alpha matte prediction. Multi-Dataset Training To ensure the robustness and ver- satility of our Matting Anything Model (MAM), we adopt a multi-dataset training approach that encompasses diverse foreground instances and background images from various image matting datasets. This selection allows us to cover a wide range of instance classes",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S22",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "and background scenar- ios, enhancing the model\u2019s ability to handle different types of instances and backgrounds effectively. During the train- ing process, we create composite images by combining a foreground instance F \u2208 R3\u00d7H\u00d7W with its correspond- ing ground truth alpha matte \u03b1gt \u2208 R1\u00d7H\u00d7W and a back- ground image B \u2208 R3\u00d7H\u00d7W . The composition is per- formed using the equation I = \u03b1gtF + (1\u2212 \u03b1gt)B. We then extract the bounding box (x0, y0, x1, y1) that encap- sulates the instance of interest within the composite im- age. Then, we send the image I and the bounding box as a prompt to the pre-trained SAM, which returns the mask prediction of the instance. Then, we concatenate the image,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S23",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "mask and feature maps, and send them to the M2M mod- ule, which further returns the multi-scale alpha matte pre- dictions \u03b1os8, \u03b1os4, \u03b1os1. The loss L is computed between the multi-scale predictions and ground truth \u03b1gt as L(\u03b1gt, \u03b1os1, \u03b1os4, \u03b1os8) =\u03bbL1 L1 + \u03bbLLapLLap (3) L1 is L1 loss andLLap is Laplacian loss used in [14,30,43]. The coefficients \u03bbL1 and \u03bbLLap control the contribution of each loss term, respectively. Both loss terms are computed on multi-scale predictions as L1 = L1(\u03b1gt, \u03b1os1) +L1(\u03b1gt, \u03b1os4) +L1(\u03b1gt, \u03b1os8) (4) LLap = LLap(\u03b1gt, \u03b1os1)+LLap(\u03b1gt, \u03b1os4)+LLap(\u03b1gt, \u03b1os8) (5) Multi-Benchmark Inference During the inference phase, we conducted extensive evaluations of the Matting Any- thing Model (MAM) on multiple image matting bench- marks to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S24",
      "paper_id": "arxiv:2306.05399v2",
      "section": "background",
      "text": "assess its generality and adaptability. Given an input image I, SAM produced the initial mask prediction 4 AM2K / PM-10K",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S25",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "SHM [4] 17.81 / 16.64 6.8 / 6.9 10.2 / 9.7 12.54 / 14.54 10.26 / 8.53 LFM [57] 36.12 / 37.51 11.6 / 15.2 21.0 / 15.2 21.06 / 21.82 19.68 / 16.36 HATT [37] 28.01 / 22.66 5.5 / 3.8 16.1 / 13.1 18.29 / 15.16 13.36 / 9.32 SHMC [32] 61.50 / 57.85 27.0 / 29.1 35.6 / 34.0 37.00 / 37.28 35.23 / 23.04 GFM-R [25] 10.89 / 11.52 2.9 / 3.8 6.4 / 6.7 10.00 / 13.07 9.15 / 8.00 GFM-D [25] 10.26 / 11.89 2.9 / 4.1 5.9 / 6.9 8.82 / 12.90 8.24 / 7.80 SAM [21] 25.00 / 44.11 10.8 / 28.8 14.8 / 25.7 60.01 / 24.56 20.72 / 31.96 MAM",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S26",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "17.30 / 25.82 3.5 / 9.2 10.1 / 15.4 10.65 /14.22 15.67 / 23.99 Table 2. Results on the semantic image matting benchmark AM2K and PM-10K. Metrics with all and tri as subscript indicates the evaluation of the whole image and the transition area, separately. \u2193 means lower values indicate better performance for the metric.",
      "page_hint": null,
      "token_count": 55,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S27",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "DIM [51] 11.5 17.8 FDMPA [59] 10.1 16.0 LFM [57] 9.4 15.8 SHM [4] 7.2 15.2 HATT [37] 6.7 13.7 BSHM [32] 6.3 11.4 MODNet [19] 4.4 8.6 SAM [21] 10.8 13.8 MAM 4.6 9.9 Table 3. Results on the semantic image matting benchmark PPM- 100. m \u2208 R1\u00d7H\u00d7W , which captured the rough delineation of the instance. Subsequently, M2M contributed to the refine- ment of the alpha matte prediction by providing multi-scale predictions \u03b1os8, \u03b1os4, and \u03b1os1. Then, following the itera- tive refinements, we progressively updated the predictions by replacing the corresponding regions in the mask pre- diction m with the respective multi-scale predictions that demonstrated positive weight maps, while in some simple cases the replacement is directly done",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S28",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "upon \u03b1os8 instead of m. This iterative refinement allowed us to refine the alpha matte estimation iteratively and enhance the precision of the final prediction \u03b1 \u2208 R1\u00d7H\u00d7W . 4. Experiments We extensively evaluate the performance of MAM on six diverse image matting benchmarks. Through compre- hensive evaluations using different metrics, we compare the performance of MAM with state-of-the-art image mat- ting models on each benchmark. The results demonstrate that MAM consistently achieves comparable performance to specialized state-of-the-art models, reaffirming its versa- tility and effectiveness as a unified image matting solution. 4.1. Implementation Details Training Datasets During the training process, we ran- domly select foreground instances from several image mat- ting datasets, including Adobe Image Matting dataset [51], Distinctions-646 [54],",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S29",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "AM2K [25], Human-2K [34], and RefMatte [28], to ensure a diverse range of instance classes. For background images, we select them from two datasets: COCO [31] and BG20K [25] to provide a mix of both real- world and synthetic backgrounds. Evaluation Benchmarks To evaluate the adaptive ability of MAM, we test it on a variety of image matting bench- marks including the semantic image matting benchmarks PPM-100 [40], AM2K [25], PM-10K [25], the instance im- age matting benchmark RWP636 [55], HIM2K [42], and the referring image matting benchmark RefMatte-RW100 [28]. The box prompt is used for all benchmarks and the point prompt is only used in RefMatte-RW100. This comprehen- sive evaluation allows us to assess the generalization ca- pability of",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S30",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "MAM across various image matting tasks and benchmarks. Evaluation Metrics We evaluate the accuracy of predicted alpha matte for MAM with commonly adopted evaluation metrics. Specifically, we employ Mean Absolute Differ- ence (MAD), Sum of Absolute Difference (SAD), Mean Squared Error (MSE), Gradient (Grad), and Connectivity (Conn) [38] as corresponding evaluation metrics. We scale MAD, MSE, Grad, and Conn by 103, 103, 10\u22123, and 10\u22123, respectively. Lower values indicate better performance for these metrics. Additionally, for instance-aware matting, we utilize Instance Matting Quality (IMQ) [42], which takes recognition and matting accuracy into consideration simul- taneously. Higher values indicate better performance for the IMQ metric. Experimental Settings We trained MAM on a combina- tion of training datasets using 8 RTX A6000",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S31",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "GPUs, with a batch size of 10 images per GPU. Each image was a combination of a randomly selected foreground instance and a background image. Images were cropped to a size of 1024 \u00d7 1024 and sent to a pre-trained ViT-B based SAM [21] with a bounding box prompt of the target in- stance. The feature maps and masks output by SAM were then fed into the M2M module for alpha matte predic- 5 Model Synthetic Subset \u2191 Natural Subset \u2191",
      "page_hint": null,
      "token_count": 81,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S32",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Mask RCNN [13] 44.3 M 18.37 25.65 0.45 19.07 24.22 33.74 2.27 26.65 CascadePSP [7] 67.7 M 40.85 51.64 29.59 43.37 64.58 74.66 60.02 67.20 GCA [29] 25.2 M 37.76 51.56 38.33 39.90 45.72 61.40 44.77 48.81 SIM [41] 46.5 M 43.02 52.90 40.63 44.29 54.43 66.67 49.56 58.12 FBA [10] 34.7 M 36.01 51.44 37.86 38.81 34.81 48.32 36.29 37.23 MGMatting [55] + 29.6 M 51.67 67.08 53.03 55.38 57.98 71.12 66.53 60.86 InstMatt [42] + 29.7 M 63.59 78.14 64.50 67.71 70.26 81.34 74.90 72.60 SAM [21] 93.7 M 49.69 61.44 4.34 51.84 61.15 74.01 13.64 65.85 MAM + 2.7 M 54.15 68.01 30.47 55.40 68.78 81.67 51.79 72.62 Table 4. Results on the instance image matting benchmark",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S33",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "HIM2K. Metrics with mad, mse, grad, and conn as subscript indicates the similarity metrics for IMQ are MAD, MSE, Gradient, and Connectivity, separately. \u2191 means higher values indicate better performance for the IMQ metric. MAM shows clear improvements over SAM under different metrics with only 2.7M extra trainable parameters, much lighter compared to other mask-guided methods like MGMatting and InstMatt.",
      "page_hint": null,
      "token_count": 60,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S34",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Mask RCNN [13] 20.26 25.36 CascadePSP [7] 42.20 52.91 GCA [29] 33.87 46.47 SIM [41] 34.66 46.60 FBA [10] 35.00 47.54 MGMatting [55] 30.64 53.16 InstMatt [42] 51.10 73.09 SAM [21] 49.87 56.92 MAM 54.40 76.45 Table 5. Results on the instance matting benchmark RWP636. tion. We employed the Adam optimizer with \u03b21 = 0.5 and \u03b22 = 0.99, trained for 20,000 iterations with warm- up for the first 4,000 iterations. The weight map wos8 is always 1 at all pixels during training, whilewos4 changes to the mask guidance from SAM after the 4,000 iterations and wos1 changes to the boundary of \u03b1os4 after the 4,000 itera- tions as well. We set 3 refinement blocks for the prediction of \u03b1os8,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S35",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "3 refinement blocks for the prediction of \u03b1os4, and 2 refinement blocks for the prediction of \u03b1os1. As a result, the total trainable parameters of MAM is 2.7 million param- eters. We applied cosine learning rate decay with an initial learning rate of 0.001 during training. During inference, we used a single GPU with a batch size of 1. Each image was resized to have its longer side at 1024 pixels and its shorter side was padded to 1024 pixels before being sent to MAM for alpha matte prediction of the target instance. 4.2. Main Results Specialized vs Unified Model We present a high-level comparison between specialized image matting models and MAM on the semantic, instance, and referring image mat-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S36",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "ting benchmarks in Table 1. It shows that MAM has clear improvements over SAM on all benchmarks. Furthermore, MAM shows comparable performance to each specialized",
      "page_hint": null,
      "token_count": 25,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S37",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "MDETR [18] text 131.58 67.5 75.1 CLIPSeg [35] text 211.86 117.8 122.2 CLIPMat [28] text 107.81 59.5 62.0 SAM [21] text 122.76 67.9 69.0 MAM text 120.10 65.9 67.5 SAM [21] point 214.19 123.8 124.9 MAM point 168.82 89.6 97.7 SAM [21] box 33.51 17.9 19.0 MAM box 29.24 15.1 16.6 Table 6. Results on the referring image matting benchmark RefMatte-RW100. MAM with box prompt can reach significantly better performance than with the text prompt. image matting model and even reaches better performance on the HIM2K, RWP635, and RefMatte-RW100, which makes it a practical and feasible solution to unified image matting. Semantic Image Matting We evaluate the performance of MAM on three semantic image matting benchmarks: PPM- 100 [40], AM2K",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S38",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "[25], and PM-10K [25], as presented in Table 3 and Table 2. The iterative refinement process is based on the \u03b1os8 prediction for all three benchmarks. On the PPM-100 benchmark, MAM achieves improvements of 6.2 MSEall and 3.9 MAD all over SAM. Similarly, on the AM2K benchmark, MAM outperforms SAM with enhance- ments of 7.64 SAD all, 4.4 MSE all, 4.5 MAD all, 41.54 Gradall, and 7.59 SADtri. Instance Image Matting In Table 4 and Table 5, We eval- uate MAM on two instance image matting benchmarks: HIM2K [42] and RWP636 [55]. For HIM2K, the iterative refinement is based on prediction mask m since it contains multiple instances per image and starting from m removes false positive predictions. Compared to",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S39",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "other state-of-the- art methods on HIM2K, MAM reaches comparable perfor- mance with only 2.7 M extra trainable parameters, which 6 Model Natural Subset",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S40",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "SAM [21] 93.7 M 50.47 61.66 + Mask-Select 93.7 M 61.15 74.01 MAM Baseline 1.0 M 52.82 71.82 + Multi-Scale Prediction 2.7 M 60.11 74.74 + Iterative Refinement 2.7 M 65.44 78.93 + Multi-Dataset Training 2.7 M 68.37 81.56 Table 7. Ablation study of MAM on the HIM2K benchmark. The MAM Baseline is built upon the SAM model with the box prompt. The other strategies are gradually added to the MAM Baseline and end up with 2.7 M extra trainable parameters. is only 10% of the specialized models like MGMatting and InstMatt, which use the mask guidance from Mask RCNN. On the RWP636 benchmark, we apply the iterative refine- ment from \u03b1os8 and MAM reaches the new state-of-the-art with 54.40",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S41",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "IMQmad and 76.45 IMQmse. Referring Image Matting In Table 6, we present the eval- uation of MAM on the RefMatte-RW100 benchmark [28], a recently introduced referring image matting benchmark. While previous methods rely on text prompts for refer- ring image matting, we leverage the bounding boxes and text descriptions as the prompts for SAM. Considering the text prompt for SAM has not been released yet, we use Grounded-SAM [33] to support text prompt guidance. Re- markably, MAM achieves superior performance when uti- lizing the bounding box as the prompt for SAM, surpassing the text-based methods CLIPSeg and CLIPMat by a sig- nificant margin. Moreover, the use of bounding boxes as prompts offers user-friendly and intuitive interactions, as users find it",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S42",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "easier to provide bounding boxes compared to composing a fixed text paragraph. This observation sug- gests that the bounding box prompt is more effective for interactive image matting than the text or point prompt for referring image matting. 4.3. Ablation Study We conduct comprehensive ablation studies on the M2M module of MAM, considering that SAM remains frozen during the training process. To assess the performance of MAM, we select the real-world subset of the HIM2K benchmark. SAM on HIM2K We begin by evaluating the pre-trained ViT-B-based SAM using bounding boxes and points as prompts for the target instance. SAM with box-based prompts significantly outperforms the point-based prompts and the final mask output is selected based on the mask with the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S43",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "highest Intersection over Union (IoU) score with the bounding box. SAM demonstrates strong performance on the HIM2K benchmark, achieving 61.15 IMQ mad and 74.01 IMQmse on the natural subset. Building MAM We then construct the M2M baseline by ImageMG[51] MAMGT c Figure 3. Visualizations of alpha matte predictions from MGMat- ting and MAM. Improvements are highlighted in the red boxes. integrating the M2M module, which takes SAM\u2019s mask and feature maps, as well as the image, as inputs. This base- line, comprising 3 connected refinement blocks and pre- dicting at 1/16 resolution, yields inferior performance com- pared to SAM, as the low-resolution predictions lack fine details of the alpha matte. However, by gradually incor- porating multi-scale predictions and iterative refinement,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S44",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "as described in Section 3.2, the performance of MAM im- proves. Additionally, the adoption of multi-dataset training, as outlined in Section 3.3, further enhances performance, resulting in 68.37 IMQ mad and 81.56 IMQmse on the nat- ural subset. Subsequently, we assess MAM\u2019s performance on other benchmarks without retraining to validate its gen- erality and adaptability. 4.4. Visualization In Figure 3, we compare matting performance between MGMatting and MAM of images that contain multiple in- stances. They both leverage mask guidance from SAM. It shows that MAM is able to give more accurate alpha matte predictions with only 10% parameters compared to MGMatting under the same mask guidance. It also has fewer false positive predictions in other instances. In Fig- ure",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S45",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "4, we further provide visualizations of the mask and al- pha matte predictions from SAM and MAM. These images are selected from the semantic image matting benchmarks 7 ImageSAM[21]MAMImageSAM[21]MAM GTGT c c Figure 4. Visualizations of mask and alpha matte predictions from SAM and MAM. Improvements are highlighted in the red boxes. and contain a single instance that can be a person, animal, or transparent object. The visualizations demonstrate that MAM achieves significantly improved predictions in the transition areas without the trimap guidance, which high- lights the superior performance of MAM in refining and en- hancing the quality of alpha matte predictions. 5. Conclusion In this paper, we introduce Matting Anything Model (MAM), which uses the Segment Anything Model (SAM)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S46",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "as a guidance module with a lightweight Mask-to-Matte (M2M) module to refine the mask output into the alpha matte of the target instance. M2M is designed to handle various image matting tasks, including semantic, instance, and referring image matting, using a single model based on user prompts including points, boxes, and text. We evaluate MAM on six image matting benchmarks and demonstrate that it achieves comparable performance to the specialized state-of-the-art methods under various evaluation metrics. Our proposed model offers a more versatile and efficient solution for interactive and unified image matting. 8 References [1] Yagiz Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Design- ing effective inter-pixel information flow for natural image matting. In Proceedings of the IEEE Conference on",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S47",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Com- puter Vision and Pattern Recognition, 2017. 2 [2] Xue Bai and Guillermo Sapiro. A geodesic framework for fast interactive image and video segmentation and matting. In 2007 IEEE 11th International Conference on Computer Vision. IEEE, 2007. 2 [3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587 , 2017. 3 [4] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, and Kun Gai. Semantic human matting. In Proceedings of the 26th ACM international conference on Multimedia, 2018. 2, 5 [5] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. Knn mat- ting. IEEE transactions on pattern analysis and machine intelligence, 2013. 2 [6] Bowen Cheng, Ishan Misra, Alexander G",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S48",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Schwing, Alexan- der Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation.arXiv preprint arXiv:2112.01527, 2021. 3 [7] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. Cascadepsp: Toward class-agnostic and very high- resolution segmentation via global and local refinement. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, 2020. 4, 6 [8] Yung-Yu Chuang, Brian Curless, David H Salesin, and Richard Szeliski. A bayesian approach to digital matting. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001 . IEEE, 2001. 2 [9] Xiaoxue Feng, Xiaohui Liang, and Zili Zhang. A cluster sampling method for image matting via sparse coding. In European Conference on Computer Vision",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S49",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": ". Springer, 2016. 2 [10] Marco Forte and Franc \u00b8ois Piti\u00b4e. f, b, alpha matting. arXiv preprint arXiv:2003.07711, 2020. 6 [11] Leo Grady, Thomas Schiwietz, Shmuel Aharon, and R\u00a8udiger Westermann. Random walks for interactive alpha-matting. In Proceedings of VIIP, 2005. 2 [12] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 3 [13] Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Gir- shick. Mask r-cnn. In ICCV, 2017. 3, 6 [14] Qiqi Hou and Feng Liu. Context-aware image matting for si- multaneous foreground and alpha estimation. InICCV, 2019. 4 [15] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S50",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Wenyu Liu, and Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In TPAMI, 2020. 3 [16] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. CVPR, 2023. 3 [17] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Ji- achen Li, Steven Walton, and Humphrey Shi. Semask: Se- mantically masked transformers for semantic segmentation. arXiv preprint arXiv:2112.12782, 2021. 3 [18] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr- modulated detection for end-to-end multi-modal understand- ing. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 1780\u20131790, 2021. 6 [19] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Ryn- son",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S51",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "WH Lau. Modnet: Real-time trimap-free portrait mat- ting via objective decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1140\u20131147, 2022. 4, 5 [20] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll \u00b4ar. Panoptic segmentation. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 3 [21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White- head, Alexander C Berg, Wan-Yen Lo, et al. Segment any- thing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3, 4, 5, 6, 7 [22] Jiachen Li, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Yunchao Wei, and Humphrey Shi. Vmformer: End-to-end video matting with transformer. arXiv preprint arXiv:2208.12801, 2022.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S52",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "2 [23] Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, and Humphrey Shi. Video instance matting. arXiv preprint arXiv:2311.04212, 2023. 2 [24] Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant Navasardyan, Yunchao Wei, and Humphrey Shi. VideoMatt: A simple baseline for accessible real-time video matting. In CVPR Workshops, 2023. 2 [25] Jizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. International Journal of Computer Vision , 2022. 2, 4, 5, 6 [26] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic natural image matting. arXiv preprint arXiv:2107.07235 , 2021. 1 [27] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep image mat- ting: A comprehensive survey. ArXiv, 2023. 2 [28]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S53",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring im- age matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 22448\u2013 22457, 2023. 2, 4, 5, 6, 7 [29] Yaoyi Li and Hongtao Lu. Natural image matting via guided contextual attention. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 2, 6 [30] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. Robust high-resolution video matting with tempo- ral guidance. arXiv preprint arXiv:2108.11515, 2021. 2, 4 9 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 2014. 5 [32] Jinlin Liu,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S54",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, and Xian-sheng Hua. Boosting se- mantic human matting with coarse annotations. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2, 5 [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3, 7 [34] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. Tripartite information mining and inte- gration for image matting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 5 [35] Timo L \u00a8uddecke and Alexander",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S55",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Ecker. Image segmenta- tion using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7086\u20137096, 2022. 6 [36] GyuTae Park, SungJoon Son, JaeYoung Yoo, SeHo Kim, and Nojun Kwak. Matteformer: Transformer-based image matting via prior-tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022. 2 [37] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hi- erarchical structure aggregation for image matting. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2, 5 [38] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptu- ally motivated online benchmark for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S56",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "image matting. In 2009 IEEE Conference on Computer Vision and Pattern Recogni- tion. IEEE, 2009. 5 [39] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Back- ground matting: The world is your green screen. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2 [40] Jiayu Sun, Zhanghan Ke, Lihe Zhang, Huchuan Lu, and Rynson WH Lau. Modnet-v: Improving portrait video matting via background restoration. arXiv preprint arXiv:2109.11818, 2021. 2, 5, 6 [41] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic image matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1, 6 [42] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Human in- stance matting via",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S57",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "mutual guidance and multi-instance re- finement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 4, 5, 6 [43] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and Yu-Wing Tai. Deep video matting via spatio-temporal align- ment and aggregation. InProceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, 2021. 4 [44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 3 [45] Jue Wang and Michael F Cohen. Image and video matting: a survey. 2008. 1 [46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun- hua Shen. Solov2:",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S58",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Dynamic and fast instance segmentation. Advances in Neural information processing systems, 2020. 3 [47] Yu Wang, Yi Niu, Peiyong Duan, Jianwei Lin, and Yuanjie Zheng. Deep propagation based image matting. In IJCAI, 2018. 2 [48] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Han- qing Zhao, Weiming Zhang, and Nenghai Yu. Improved im- age matting via real-time user clicks and uncertainty estima- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition , pages 15374\u201315383, 2021. 2 [49] Bo Xu, Jiake Xie, Han Huang, Ziwen Li, Cheng Lu, Yong Tang, and Yandong Guo. Situational perception guided im- age matting. In Proceedings of the 30th ACM International Conference on Multimedia, 2022. 1 [50] Jiarui Xu, Shalini De",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S59",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, 2022. 3 [51] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, 2017. 2, 5 [52] Stephen DH Yang, Bin Wang, Weijia Li, YiQi Lin, and Con- ghui He. Unified interactive image matting. arXiv preprint arXiv:2205.08324, 2022. 2 [53] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu. Matte anything: Interactive natural image matting with seg- ment anything models. arXiv preprint arXiv:2306.04121 , 2023. 2 [54] Haichao Yu, Ning Xu, Zilong Huang,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S60",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "Yuqian Zhou, and Humphrey Shi. High-resolution deep image matting. AAAI, 2021. 2, 5 [55] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided matting via progressive refinement network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, 2021. 2, 4, 5, 6 [56] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus- tus Odena. Self-attention generative adversarial networks. In International conference on machine learning. PMLR, 2019. 4 [57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for digital matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 7469\u2013 7478, 2019.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_2306_05399v2:S61",
      "paper_id": "arxiv:2306.05399v2",
      "section": "method",
      "text": "5 10 [58] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Wang, and Fan Wang. Lmseg: Language-guided multi- dataset segmentation. arXiv preprint arXiv:2302.13495 , 2023. 3 [59] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, and Ming Tang. Fast deep matting for portrait ani- mation on mobile phone. In Proceedings of the 25th ACM international conference on Multimedia, 2017. 2, 5 11",
      "page_hint": null,
      "token_count": 65,
      "paper_year": 2023,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.9491903330940216,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 11,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 2864,
        "empty": false
      },
      {
        "page": 2,
        "chars": 5370,
        "empty": false
      },
      {
        "page": 3,
        "chars": 3771,
        "empty": false
      },
      {
        "page": 4,
        "chars": 4517,
        "empty": false
      },
      {
        "page": 5,
        "chars": 4494,
        "empty": false
      },
      {
        "page": 6,
        "chars": 4357,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3993,
        "empty": false
      },
      {
        "page": 8,
        "chars": 1243,
        "empty": false
      },
      {
        "page": 9,
        "chars": 5575,
        "empty": false
      },
      {
        "page": 10,
        "chars": 5736,
        "empty": false
      },
      {
        "page": 11,
        "chars": 398,
        "empty": false
      }
    ],
    "quality_score": 0.9492,
    "quality_band": "good"
  }
}