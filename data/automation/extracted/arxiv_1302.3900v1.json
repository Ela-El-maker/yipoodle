{
  "paper": {
    "paper_id": "arxiv:1302.3900v1",
    "title": "Robust Image Segmentation in Low Depth Of Field Images",
    "authors": [
      "Franz Graf",
      "Hans-Peter Kriegel",
      "Michael Weiler"
    ],
    "year": 2013,
    "venue": "arXiv",
    "source": "arxiv",
    "abstract": "In photography, low depth of field (DOF) is an important technique to emphasize the object of interest (OOI) within an image. Thus, low DOF images are widely used in the application area of macro, portrait or sports photography. When viewing a low DOF image, the viewer implicitly concentrates on the regions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image. Thus, a robust algorithm for the fully automatic detection of the OOI in low DOF images provides valuable information for subsequent image processing and image retrieval. In this paper we propose a robust and parameterless algorithm for the fully automatic segmentation of low DOF images. We compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand. The experiments are conducted on a real world data set with high and low DOF images.",
    "pdf_path": "data/automation/papers/arxiv_1302.3900v1.pdf",
    "url": "https://arxiv.org/pdf/1302.3900v1",
    "doi": null,
    "arxiv_id": "1302.3900v1",
    "openalex_id": null,
    "citation_count": 0,
    "is_open_access": true,
    "sync_timestamp": "2026-02-20 18:11:02.783975+00:00"
  },
  "snippets": [
    {
      "snippet_id": "Parxiv_1302_3900v1:S1",
      "paper_id": "arxiv:1302.3900v1",
      "section": "body",
      "text": "Robust Image Segmentation in Low Depth Of Field Images Franz Graf, Hans-Peter Kriegel, and Michael Weiler Ludwig-Maximilians-Universitaet Muenchen, Oettingenstr. 67, 80538 Munich, Germany [graf,kriegel,weiler]@dbs.i\ufb01.lmu.de November 27, 2024",
      "page_hint": null,
      "token_count": 27,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S2",
      "paper_id": "arxiv:1302.3900v1",
      "section": "abstract",
      "text": "In photography, low depth of \ufb01eld (DOF) is an im- portant technique to emphasize the object of interest (OOI) within an image. Thus, low DOF images are widely used in the application area of macro, portrait or sports photography. When viewing a low DOF image, the viewer implicitly concentrates on the re- gions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image. Thus, a robust algorithm for the fully automatic detection of the OOI in low DOF images provides valuable information for subsequent image processing and image retrieval. In this paper we propose a robust and parameterless",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S3",
      "paper_id": "arxiv:1302.3900v1",
      "section": "abstract",
      "text": "algorithm for the fully automatic segmentation of low DOF images. We compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand. The experiments are conducted on a real world data set with high and low DOF images.",
      "page_hint": null,
      "token_count": 53,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S4",
      "paper_id": "arxiv:1302.3900v1",
      "section": "introduction",
      "text": "In photography, low depth of \ufb01eld (DOF) is an im- portant technique to emphasize the object of interest (OOI) within an image. Low DOF images are usually characterized by a certain region which is displayed very sharp like the face of a person and blurry image regions which are signi\ufb01cantly before of behind the object of interest. Low DOF images are well known from sports, por- trait or macro photography where only a speci\ufb01c part of the image should attract most of the users\u2019 atten- tion. The OOI is thereby displayed sharp while other areas like the background appears blurred, so that the viewer automatically focuses on the sharp areas of the image. When viewing a low depth of \ufb01eld",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S5",
      "paper_id": "arxiv:1302.3900v1",
      "section": "introduction",
      "text": "image, the viewer implicitly segments the image into regions of interest and regions of less interest (usually",
      "page_hint": null,
      "token_count": 17,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S6",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "jor impact on the perception of the image, this infor- mation is a valuable feature for the subsequent image processing chain like an adaptive image compression [6] or image retrieval aspects such as the similarity of images which can be considerably in\ufb02uenced by the image\u2019s DOF. Given for example two images dis- playing a person in the sharp image region in front of di\ufb00erent, blurred backgrounds, people might judge both pictures similar even though the blurred back- ground di\ufb00ers. Although this implicit segmentation is rather easy for a human viewer of the photo, it is not an easy task for a completely unsupervised algo- rithm. This can be explained by the fact that there is usually not a sharp edge",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S7",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "which divides the sharp OOI and blurred background. Depending on the camera\u2019s setting, this transition can be very smooth so that it is hard to distinguish where the OOI ends. With the vastly growing market of consumer 1 arXiv:1302.3900v1 [cs.CV] 15 Feb 2013 DSLRs or even new small compact cameras like the Sony Cybershot which are explicitly being advertised with the ability for low DOF photos, the amount of lowDOFphotosalsoincreased. Thisgrowingamount of low DOF images may also provide new information for established search and retrieval systems if they take the OOI into account when performing the sim- ilarity search tasks. In order to pro\ufb01t from the low DOF information, search engines and feature extrac- tion algorithms need fully automatic and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S8",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "robust im- age segmentation algorithms which can separate the OOI from the rest of the image. For large search en- gines or image stock agencies, such algorithms should also be independent of the image domain, image size andthecolordepthoftheimagesothatthealgorithm performs well, no matter of the color or the toning of the photo (e.g. black-and-white, color photos, sepia photos). In this paper, we propose a robust, fully automatic and parameterless algorithm for the segmentation of low DOF images as well as an analysis of the im- pact of low DOF information on similarity search. The algorithm does not need any a priory knowledge like image domain or camera settings. The algorithm also provides meaningfull results even if the DOF is rather large",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S9",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "so that the background provides signif- icant structures. The rest of the paper is organized as follows: In Sec. 2 we review some related work and some technical background, follwed by the ex- planation of the algorithm in Sec. 3. In Sec. 4 we explain our experimental evaluation of the algorithm. In Sec. 5 we describe theinternal parameter settings and threshold values. The impact of the DOF seg- mentation to image similarity is shown in Sec. 6. Afterwards we \ufb01nish the paper with a conclusion and outlook in Sec. 7.",
      "page_hint": null,
      "token_count": 90,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S10",
      "paper_id": "arxiv:1302.3900v1",
      "section": "related_work",
      "text": "The segmentation of low DOF images has gained some interest in the research community in past years. In [12, 14] early approaches to segment low DOF images were presented. Thereby [14] is using an edge-basedalgorithmwhich\ufb01rstconvertsagray-scale image into an edge-representation which is then \ufb01l- tered. Afterwards the edges are linked to form closed boundaries. These boundaries are treated with a re- gion \ufb01lling process, generating the \ufb01nal result. [12] presents a fully automatic segmentation algorithm using block based multiresolution wavelet transfor- mations on gray scale images. Even though the pa- per lists high rates of sensitivity and speci\ufb01city on the testset, the authors also name some limitations like the dependence on very low DOF, fully focused OOI, and high image resolution",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S11",
      "paper_id": "arxiv:1302.3900v1",
      "section": "related_work",
      "text": "and quality. In [16, 15] high frequency wavelets are used to determine the segmentation of low DOF images. As stated in [9], these features have the drawback of being not too ro- bust if used alone and thus often result in errors in both focused and defocused regions if the defocused",
      "page_hint": null,
      "token_count": 51,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S12",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "cused foreground does not have very signi\ufb01cant tex- tures. In [10], localized blind deconvolution is pro- posed to determine the focus map of an image. Yet the authors do not propose a pure image segmenta- tion algorithm as the focus map is not a true seg- mentation but a measure for the amount of focus in this part of the image. Also the algorithm does not take into account any color information as it is only operating on gray scale images. The works proposed in [9, 13, 7, 8] are consecutive works for segmentation of DOF images and sequences of images [9, 13] like in movies which address a similar topic. In this pa- per, we were inspired by the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S13",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "algorithm proposed in [7] which uses morphological \ufb01ltering for the segmenta- tion. Some problems of this algorithm were given by",
      "page_hint": null,
      "token_count": 20,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S14",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "photo was taken with high ISO values. Also the al- gorithm showed some problems if spatially separated OOIs were shown in a single image. Another problem can be raised by the size of the structuring element used in the algorithm [7, Sec. IV]. We compare our algorithm with the work of [7], with [11] where single frames of videos are processed into a saliency map which is processed by morpho- logical \ufb01lters. The resulting tri-map is then used for error control and for the extraction of boundaries of the focused regions. We also compare our work to the algorithm proposed in [18], where a fuzzy segmenta- tion approach was proposed by \ufb01rst separating the image into regions using a mean",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S15",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "shift. These regions 2 are then characterized by color features and wavelet modulus maxima edge point densities. Finally, the region of interest and the background are separated by defuzzi\ufb01cation on fuzzy sets generated in the pre- vious step. Our test image dataset consists of a set of various photos and comprises several categories from high to low DOF images. 2.1 Depth of Field In optics, the DOF denotes the depth of the sharp area around the focal point of a lens seen from the photographer. Technically, each lens can only focus at a certain distance at a time. This distance builds the focal plane which is orthogonal to the photog- raphers view through the lens. Precisely, only ob- jects directly",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S16",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "on the focal plane are absolutely sharp, while objects before or behind the focal plane are dis- played unsharp. With increasing distance from the focal plane, the sharpness of the displayed object de- creases. Nevertheless, there is a certain range before and behind the focal plane where objects are recog- nized as sharp until a blur is perceived. The depth of this region is then called the DOF. As the sharpness decreases gradually with increasing distance from the focal plane, it is hard to determine an exact range for the DOF as the limits of the sharp area are only de- \ufb01ned by the perceived sharpness. Points in the defocused areas appear blurred to a certain degree. This is often",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S17",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "modeled by a Gaussian kernel G\u03c3 as in Eq. 1 where \u03c3 denotes the spread parameter which a\ufb00ects the strength of the blur. For a given imageI, the blurred representation can then be created by a convolutionG\u03c3 \u2217I. G\u03c3(x,y) = 1 2\u03c0\u03c32 exp ( \u2212x2 + y2 2\u03c32 ) (1) The e\ufb00ect of DOF is mainly determined by the choice of the camera respectively its imaging sensor size, aperture and distance to the focussed object. The larger the sensor or aperture, the smaller the DOF. Increasing the distance from the camera to the focussed object will also expand the resulting DOF. Figure 1 illustrates the geometry of DOF at a sym- metrical lens. ure ane apertu focal pla f perceived",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S18",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "sharp area Fig. 1: Figure illustrating the depth of \ufb01eld. The size of the perceived sharp area around the focal plane denotes the DOF. 2.2 Automatic segmentation of low DOF images Automatic segmentation of images is more challeng- ingthaninteractiveapproachesbecausenoadditional information of humans can be used to adapt parame- ter values for the segmentation process. However the advantages of a fully automated algorithm are obvi- ous, if the according algorithm should be deployed to a system providing lots of images where the segmen- tation should be present as fast as possible. This is for example the case in search index or photo com- munities like Flickr or Google\u2019s Picasa, where several thousand photos are uploaded each minute, even if not all",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S19",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "of them are low DOF images. The requirements to a segmentation algorithm are that it should be able to handle di\ufb00erent types (grayscale or color), orientations (landscape or por- trait) and resolutions (from small to large) of im- ages, independent of the camera settings like ISO etc. Many automatic segmentation approaches of low DOF images have some of these restrictions, as seen in [18], which only performs well on color images. Grayscale images mostly fail because the extracted color features are too few, to characterize regions and distinguish them su\ufb03cient. However, other algorithms like the one presented in [7], can only process grayscale images. In such cases, color images have to be transformed and hence their color information looses its",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S20",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "contribution to improve segmentation quality. As shown in our experimental 3",
      "page_hint": null,
      "token_count": 11,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S21",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "focused regions can cause poor segmentation results, because too many false positives are found. In this context, false positives describe the set of pixels that are de\ufb01ned as background by the underlying ground truth but classi\ufb01ed as OOI pixel by the segmentation algorithm. In the following section, we describe our algorithm that does not su\ufb00er from one of the restrictions men- tioned above. Therefore, we use a robust method for calculating the amount of sharpness of a pixel in relation to its neighbors by taking advantage of the L\u2217a\u2217b\u2217color space, which o\ufb00ers a more accurate matching between numerical and visual perception di\ufb00erences between colors. The L\u2217a\u2217b\u2217 model was favored over the well knownRGB and CMYK color spaces, as theL\u2217a\u2217b\u2217 model",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S22",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "is designed to approxi- matehumanvisionbetterthantheothercolorspaces. To accomplish the problems caused by images con- sisting of numerous less blurred pixel regions showing complex structures, we apply a density-based cluster- ing algorithm to all found sharp pixels. This enables our algorithm to distinguish between sharp pixels be- longing to the main focus region of the OOI (if these pixels belong to the largest found cluster) and noise pixels located in background structures. 3 Algorithm The proposed algorithm consists of the following \ufb01ve stages: Deviation Scoring, Score Clustering, Mask Approximation, Color Segmentation and Re- gion Scoring. Before explaining the steps of the algo- rithm in detail, we \ufb01rst want to give a brief summary of the complete algorithm. Fig. 2 illustrates the steps",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S23",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "of the algorithm. The \ufb01rst stage of the algorithm, calledDeviation Scoring, identi\ufb01es sharp pixel areas in the image. Therefore a Gaussian Blur is applied to the original image. The di\ufb00erence between the extracted edges from the original image and the blurred image is then calculated. For each pixel, this di\ufb00erence repre- sents a score value, with higher score values indicat- ing sharper pixels and lower score values indicating blurred pixels. Inthesecondstage, called Score Clustering, allpix- els with a score value above a certain threshold are clustered by using a density-based clustering algo- rithm. Thus, isolated sharp pixels are recognized as noise and only large clusters are processed further. The third stage namedMask Approximation gen- erates a nearly closed plane (containing",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S24",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "almost no holes)fromthediscretepointsofeachremaining clus- ter. This is achieved by computing the convex hull from all neighbors of all dense pixels. Any so-created polygon is then \ufb01lled and the union of these \ufb01lled regions represent the approximate mask of the main focus region. In the next two stages this approximate mask is going to be re\ufb01ned. Hence, the fourth stage, calledColor Segmentation divides the approximate mask into regions that con- tain pixels with similar color in the original image. In the \ufb01fth stage named Region Scoring, a rele- vance value is calculated for each region. This rele- vance value is directly in\ufb02uenced by the score values of the pixels surrounding the according region. The \ufb01nal segmentation mask is then created by",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S25",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "removing all regions that have a relevance value below a certain threshold. 3.1 Deviation Scoring In the \ufb01rst step, we need to identify sharp pixels as an indication for the focused objects within the im- age. The well known Canny edge detector [2] is not suitable in this case because the Canny detector op- erates on gray scale images and not on theL\u2217a\u2217b\u2217 color space. Furthermore, the Canny operator does not aim at the detection of single edge pixels but at the robust detection of lines of edges even in partly blurred areas of the image (c.f. Fig. 3b on page 6). The HOS map used in [7] is de\ufb01ned as in Eq. 2 HOS (x,y) = min ( 255,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S26",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "\u02c6m(4) (x,y) DSF ) , (2) where DSF represents a down scaling factor of100 and the forth-order moment\u02c6m(4) at (x,y) is given by \u02c6m(4) (x,y) = 1 N\u03b7 \u2211 (s,t)\u2208\u03b7(x,y) (I(s,t) \u2212\u02c6m(x,y))4 where 4 (a) Input Image (b) Deviation Scor- ing (c) Score Clustering (d) Mask Approxi- mation (e) Color Segmenta- tion (f) Region Scoring Fig. 2: Illustration of the \ufb01ve stages of our algorithm: Fig. 2a: Input image with low DOF and relatively complex background regions. Fig. 2b: Identify sharp pixels by computing the di\ufb00erence between the edges of the original image and the edges of the blurred version of the image. Fig. 2c: Generate clusters from pixels with a high appropriate score by a density-based clustering algorithm (for",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S27",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "a better visual representation we colored each found cluster and surround it with its convex hull). Fig. 2d: Filling all convex hulls from all neighbors of all dense pixels. Fig. 2e: Group pixels into regions that contain similar colored pixels in the original image. (For a better visual representation we colored each found region in a random color). Fig. 2f: Removing all color Regions with low relevancy. 5 \u02c6m is the sample mean and de\ufb01ned as in Eq. 3 \u02c6m(x,y) = 1 N\u03b7 \u2211 (s,t)\u2208\u03b7(x,y) I(s,t) . (3) Thereby\u03b7(x,y) isthesetofneighborhoodpixelswith center (x,y) and is set to size3\u00d73 where N\u03b7 denotes its cardinality. Using the HOS map also has the dis- advantage that it operates only on gray scale images. Additionally,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S28",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "the HOS map is too sensitive in case of textured background as it only produces reasonable",
      "page_hint": null,
      "token_count": 16,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S29",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "works for images with very low DOF, but as soon as the DOF is not very small, the HOS map detects too many sharp areas in the background (c.f. Fig. 3c). Thus, we propose the process of Deviation Scoring. Let I be the set of pixels of the processed image. For each pixelp(x,y) \u2208I, the mean color from the pixel\u2019s r-neighborhood is calculated by \u03b7r I(x,y) = {p(x\u2032,y\u2032) ||x\u2032\u2212x|\u2264 r\u2227|y\u2032\u2212y|\u2264 r} with r representing the L1-distance to the pixel p(x,y). The color value of p(x,y) is represented in the L\u2217a\u2217b\u2217color space and denoted by ( L\u2217 p,A\u2217 p,B\u2217 p ) . Thus, the mean neighborhood color ofp(x,y) in the L\u2217-band is determined by L\u03b7r I(x,y) = \u2211 p\u2208\u03b7r I(x,y) (",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S30",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "L\u2217 p ) \u23d0\u23d0\u23d0\u03b7r I(x,y) \u23d0\u23d0\u23d0 . The values for thea\u2217- and b\u2217-band are denoted by a\u03b7r I(x,y) and b\u03b7r I(x,y) respectively, so that the mean neighborhood color Lab\u03b7r I(x,y) of a pixelp(x,y) is de- \ufb01ned by ( L\u03b7r I(x,y) ,a\u03b7r I(x,y) ,b\u03b7r I(x,y) ) . According to the International Commission on IlluminationCIE1, the color distance\u2206E\u2217(u,v) between two color valuesu, v in theL\u2217a\u2217b\u2217color space is calculated by using the Euclidean distance: \u2206E\u2217(u,v) = \u221a (L\u2217u \u2212L\u2217v)2 + (a\u2217u \u2212a\u2217v)2 + (b\u2217u \u2212b\u2217v)2 1CIE: Commission Internationale de l\u2019eclairage, http://www.cie.co.at (a) Original image (b) Canny Edge Detection (c) Higher Order Statistics (d) Deviation Scoring Fig. 3: Comparison of edge detection techniques. 6 (a) HSV (0, 1, 1) increased in componentH. (b) HSV",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S31",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "(0, 0, 1) increased in componentS. (c) HSV (0, 0, 0) increased in componentV . Fig.4: Visualizationofthe \u2206E\u2217colordistancewithin each component of the well knownHSV color space, with colorsc0,...,c 9. Where ci of thei-th square is increased in each of the componentsH (Fig. 4a), S (Fig. 4b) andV (Fig. 4c) so that\u2206E\u2217(ci,ci+1) \u224816. For each p(x,y), the neighbor di\ufb00erence \u2206\u03b7r (x,y) is then de\ufb01ned by \u2206\u03b7r I(x,y) = min \uf8eb \uf8ed255 \u00b7 \u2206E\u2217 ( Lab\u03b7r I(x,y) ,p ) \u2206Emax , 255 \uf8f6 \uf8f8 with \u2206Emax being the maximum possible distance in the L\u2217a\u2217b\u2217color space and\u2206E\u2217(u,v) being the Eu- clidean distance of the color valuesu, vin theL\u2217a\u2217b\u2217 space. In Fig. 4 we illustrated a color distance of \u2206E\u2217= 16 , by varying",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S32",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "one of the three components of a base color de\ufb01ned in theHSV color space . In the following, all pixelsp(x,y) with a neighbor di\ufb00erencegreaterthanthethreshold \u0398score \u2208[0, 255] are called edges or edge pixels, so that the equation \u2206\u03b7r I(x,y) > \u0398score holds for each edge pixel of the image. Even though the parameter\u0398score could be set freely, we recommend a value of 50 (c.f. Tab. 1) as it showed the best result. Before calculating the score values of the edge pixels,I is convolved using a Gaussian kernel with a standard deviation\u03c3= \u0398\u03c3 to remove noise and generally soften the image. We recommend to set the value of\u0398\u03c3 to 1 10 (c.f. Tab. 1). The resulting image is then denoted byI\u2032.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S33",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "Afterwards, another image I\u2032 \u03c3 is created by con- volving I\u2032 once again by using the same Gaussian kernel. I\u2032and I\u2032 \u03c3are then used to compute the score values \u00b5 \u2208 [0,255] of the edge pixels. Therefore, the score \u00b5(x,y) for an edge pixel p(x,y) is deter- mined by the squared neighbor di\ufb00erence in the im- ages I\u2032and I\u2032 \u03c3 at the location of the according pixel: \u00b5(x,y) = min { 255, ( \u2206\u03b7r I\u2032(x,y) \u2212\u2206\u03b7r I\u2032\u03c3(x,y) )2} Due to the limitation to\u00b5(x,y) \u2264255, we are treat- ing all color changes between I\u2032(x,y) and I(x,y) equally where \u2206E > 16. This can be justi\ufb01ed by human perception, which recognizes two colorsu, v to as rather unsimilar to each other if\u2206E\u2217(u,v)",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S34",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": ">12 [3]. Thus it can be said, that a\u2206E\u2217> 16 indicates a signi\ufb01cant color change which is also a strong indi- cation for an edge. Afterwards, all edge pixels with a score value greater than the threshold\u0398score are treated as can- didates for the focused region of the image while the score values of all pixels having a score value less than \u0398score are set to0 and are thus no candidates. The resulting candidate setIscore(x,y), is de\ufb01ned by the following equation: Iscore(x,y) = { 0 \u00b5(x,y) <\u0398score \u00b5(x,y) else An illustration of the candidate set can be seen in Fig. 3d and Fig. 2b, where brighter pixels indicate a large score and black pixels indicate a score less than the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S35",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "threshold\u0398score. 3.2 Score Clustering In this step, clusters are generated from all points in Iscore in order to \ufb01nd compound regions of focused areas. Therefore, the density-based clustering algo- rithm DBSCAN [4] is used. In contrast to the K- Means [5], which partitions the image into convex clusters, DBSCAN also supports concave structures which is more desirable in this case. The following section gives a short outline of DBSCAN and then de- scribes how the necessary parameters\u03b5 and minPts are determined automatically and how DBSCAN is used in our segmentation algorithm for further pro- cessing. 7 3.2.1 DBSCAN Inthisstage, clustersaregeneratedfromall p\u2208Iscore by applying DBSCAN, which is based on the two pa- rameters \u03b5 and minPts. The main idea of this",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S36",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "clus- tering algorithm is that each point in a cluster is lo- cated in a dense neighborhood of other pixels belong- ing to the same cluster. The area in which the neigh- bors must be located is called the\u03b5-neighborhood of a point, denoted byN\u03b5(p), which is de\ufb01ned as follows: N\u03b5(p) = {q\u2208D|dist(p,q) \u2264\u03b5} (4) where D is the database of points anddist(p,q) de- scribes the distance measure (e.g. the Euclidean dis- tance) between two pointsp,q \u2208D. For each pointp of a clusterC there must exist a point q\u2208C so thatpis within the\u03b5-neighborhood of q and N\u03b5(p) includes at leastminPts points. There- fore some de\ufb01nitions are invoked which are described inthefollowing. Considering \u03b5andminPts, apointp iscalleddirectly density-reachable fromanotherpoint q, ifp\u2208N\u03b5(p) andpis a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S37",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "so-calledcore-point. A point pde\ufb01ned as acore-point if |N\u03b5(p)|\u2265 MinPts holds. If there exists a chain ofn points p1, ..., pn, such that pi+1 is directly density-reachable from pi, then pn is calleddensity-reachable from p1. Two pointsp and q are density-connected if there is a pointofrom whichpand qare both density reachable, considering \u03b5 and minPts. Now, a cluster can be de\ufb01ned as a non-empty sub- set of the DatabaseD, so that for eachp and q, the following two conditions hold: \u2022 \u2200p, q: if p \u2208C and q is density-reachable from p, then q\u2208C \u2022 \u2200p, q\u2208C : p is density-connected toq Points that do not belong to any cluster are treated as noise = {p\u2208D|\u2200i: p /\u2208Ci}, wherei = 1,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S38",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "..., k and C1, ..., Ck are the found clusters inD. 3.2.2 Determination of Parameters To provide highest \ufb02exibility with respect to the dif- ferent occurrences of the focused area, we do not ap- ply absolute values for\u03b5 and minPts, but compute them relatively to the size of the image and its score distribution. Thus, \u03b5 is calculated by\u03b5= \u221a |I|\u00b7\u0398\u03b5, with |I|denoting the total amount of pixels of the image represented byI and \u0398\u03b5 \u2208[0,1]. The second parameter minPts is determined by minPts = \uf8ef\uf8ef\uf8ef\uf8f0(\u03b5+ 1)2 |I| \uf8eb \uf8ed \u2211 (x,y)\u2208Iscore min {\u00b5(x,y) \u0398dbscan ,1 }\uf8f6 \uf8f8 \uf8fa\uf8fa\uf8fa\uf8fb, with the threshold\u0398dbscan set to255. The result of the DBSCAN clustering is a cluster set C = {c1,..., c n}, with",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S39",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "eachci \u2208C representing a subset of pixelsp(x,y) \u2208I. Due to our assumption that small isolated sharp areas are treated as noise, we de\ufb01ne the relevant score cluster set \u02c6C = { c\u2208C ||ci|\u2265 maxC 2 } \u2286C, with maxC = max{|c1|,..., |cn|}being the amount of pixels of the largest cluster. An illustration of this step can be seen in Fig. 2c on page 5 where di\ufb00erent clusters are painted in di\ufb00erent colors. 3.3 Mask Approximation The relevant score cluster set\u02c6C, as de\ufb01ned in section 3.2, is already a good reference point of the OOI\u2019s location and distribution. In general however, there exists no single contiguous area, but several individ- ual regions of interest representing the focused ob- jects. Thisstageofthealgorithmconnectsallclusters c",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S40",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "\u2208 \u02c6C to a contiguous area which represents an ap- proximate binary mask of the OOI. This is achieved through the two stepsConvex Hull Linkingand Mor- phological Filtering, that will be described in more detail below. 3.3.1 Convex Hull Linking In the convex hull linking step we \ufb01rst generate the convex hull for all points in the\u03b5-neighborhood NEps(p) of each core point p of the cluster set. Let K = {k1,...,k j}be the set of all core points from the score clusters in \u02c6C and let convex(P) be the convex hull of a point set P. Then we can de\ufb01ne the set of convex hull polygons by H = 8 {convex(Neps(k1)),...,convex (Neps(kj))}which is used to generate a contiguous area. Therefore",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S41",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "each p(u,v) \u2208I is checked, if it is located within one of the convex hull polygons ofH. If that is the case, we mark this pixel with 1, otherwise with 0. The binary approximation maskIapp is then given by Iapp(x,y) = { 1 if \u2203Hi : p(x,y) \u2208Hi 0 otherwise . Afterwards we apply the morphological \ufb01lter oper- ations closing and dilation by reconstructionto Iapp for smoothing and closing small holes. 3.3.2 Morphological Filtering Morphological \ufb01lters are based on the two primary operations dilation \u03b4H (I) and erosion \u03b5H (I) where H(i,j) \u2208 {0,1} denoting the structuring element. For a binary imageI, \u03b4H (I) and \u03b5H (I) are de\ufb01ned as in the following equations: \u03b4H (I) = {(s,t) = (u+",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S42",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "i,v + j) |(s,t) \u2208I,(i,j) \u2208H} \u03b5H (I) = {(s,t) |(s+ i,t + j) \u2208I,\u2200(i,j) \u2208H} The operation morphological closing \u03d5H, is a com- position from the two primary operations, so that \u03d5H (I) = \u03b5H (\u03b4H (I)). Thus, the input image I is initially dilated and subsequently eroded, both times with the samestructuring element H. In order to de- \ufb01ne the following operation dilation by reconstruc- tion, some more de\ufb01nitions are required. At \ufb01rst, the primary operations \u03b4H (I) and \u03b5H (I) are extended to the basic geodesic dilation \u03b4(1) (I,I\u2032) and basic geodesic erosion \u03b5(1) (I,I\u2032) of size one as in the fol- lowing equations. \u03b4(1) (I,I\u2032) (u,v) = min{\u03b4H (I) (u,v) ,I\u2032(u,v)} \u03b5(1) (I,I\u2032) (u,v) = max{\u03b5H",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S43",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "(I) (u,v) ,I\u2032(u,v)} Note that these basic geodesic operations need an additional Image I\u2032, which is called marker, where the input imageI is called mask. Thus, the result of a geodesic erosionat position(u,v) is the maximum value of theerosion \u03b5H of maskI and the value of the marker imageI\u2032(u,v) and vice versa for thegeodesic dilation. The geodesicerosion \u03b5(\u221e) and dilation \u03b4(\u221e) of in\ufb01nite size, calledreconstruction by erosion\u03d5(rec) and reconstruction by dilation\u03b3(rec), is then de\ufb01ned as follows \u03d5(rec) (I,I\u2032) = \u03b5(\u221e) (I,I\u2032) = \u03b5(1) \u25e6... \u25e6\u03b5(1) (I,I\u2032) \u03b3(rec) (I,I\u2032) = \u03b4(\u221e) (I,I\u2032) = \u03b4(1) \u25e6... \u25e6\u03b4(1) (I,I\u2032) Note that \u03d5(rec) (\u00b7,\u00b7) and \u03b3(rec) (\u00b7,\u00b7) converge and achieve stability after a certain number of iterations. Thus it is assured that these functions",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S44",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "do not need to be executed inde\ufb01nitely and so the application is guaranteed to terminate. 3.3.3 Application In our approach, we primarily apply amorphological closing operation \u03d5H (Iapp) = \u03b5H (\u03b4H (Iapp)) to the approximate mask. The dimension of the structuring elementH therefore is discussed later. Afterwards we use\u03d5rec(Iapp,\u03b4H\u2032 (Iapp)) to close holes in the approx- imate mask Iapp. The dimension of the structuring element H is h\u00d7h, where h is calculated relatively to the total pixel count|Iapp|of the imageIapp, so that h= \u221a |Iapp|\u00b7\u0398rec, with\u0398rec \u2208[0,1]. After this morphological processing, the approximate maskIapp covers the OOI quite well (c.f. Fig. 2d on page 5). In general however, it includes boundary regions that exceedthebordersoftheOOIandtendtosurroundit with a thick border. The following",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S45",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "two stages of our algorithm re\ufb01ne the mask by erasing the surrounding border regions. 3.4 Color Segmentation In this stage, the pixels from the approximate mask Iapp are divided into groups, so that each group con- tains pixels that correspond to similar colors in I. Therefore we process eachp(u,v) \u2208Iapp and itera- tively include all its neighborsnfor which the follow- ing conditions hold: n\u2208 { (s,t) \u2208\u03b71 Iapp(u,v) |Iapp(s,t) = Iapp(u,v) = 1 } \u2227 \u2206E\u2217(p,n) <\u0398dist. (5) 9 The threshold \u0398dist \u2208[0,100] is an internal param- eter, which speci\ufb01es the maximum distance between two color valuesu,v in theL\u2217a\u2217b\u2217color space. Therefore a method expand(x,y,R ) is called for each p(x,y) \u2208{(x,y) \u2208Iapp |Iapp(x,y) = 1}, which isnotyetmarkedasvisited. R= {(x,y)}herede\ufb01nes a",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S46",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "new color region formed by the pointp1 (x,y). The",
      "page_hint": null,
      "token_count": 9,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S47",
      "paper_id": "arxiv:1302.3900v1",
      "section": "method",
      "text": "all neighborsp2(x,y) of p(x,y) ful\ufb01lling Eq. 5 on the preceding page we addp2 to R and markp(u,v) as visited. Then expand(u,v,R ) is called recursively. The resulting set of regions is calledRcolor. 3.5 Region Scoring In this step, a relevance value\u00b5is calculated for each region r \u2208Rcolor. The more a region is surrounded by areas with a large score\u00b5, the larger the relevancy value gets. Low relevant regions are removed after- wards which causes an update of\u00b5in the neighboring regions and thus possibly triggers another deletion if the relevance of an updated region is not high enough after the according update. 3.5.1 Boundary Overlap The boundary overlapBOR r of a regionris a measure for the adjacency ofr to the approximate",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S48",
      "paper_id": "arxiv:1302.3900v1",
      "section": "method",
      "text": "maskIapp and is de\ufb01ned as BOR r = |{(u,v) \u2208Br |\u2203r\u2032\u2208R: (u,v) \u2208r\u2032}|, where Br is the di\ufb00erence ofr to its dilation. The mask boundary overlapMBOr of r is then de\ufb01ned as MBOr = BO Rcolorr |Br| . MBOr speci\ufb01es the ratio of the number of outline points located in other regions to the number of all outline points ofr. The score boundary overlapSBOr = BO \u02c6C r |Br| of ris a measure for the adjacency ofr to the corresponding score values\u00b5. A largeSBOr indicates, thatr has a neighborhood with large corresponding score values \u00b5. 3.5.2 Mask Relevance The mask relevance for a given regionr can then be de\ufb01ned as MRr = SBOr \u00b7MBOr. Afterwards, we eliminate all regions r",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S49",
      "paper_id": "arxiv:1302.3900v1",
      "section": "method",
      "text": "with a mask relevance value which is too low. The calculation of MRr is exe- cuted iteratively: Let MRi r denote the value MRr of a region r during the i-th iteration. One iter- ation cycle computes the corresponding \u00b5 for each region r and deletes r from the approximate mask Iapp if MRi r \u2264 \u0398rel. The precise assignment of \u0398rel and its impact on segmentation quality is dis- cussed later. Once a regionr satis\ufb01es MRi r \u2264\u0398rel at iteration i, it will be erased from Iapp, so that \u2200(x,y) \u2208r: Iapp(x,y) = 0. The calculation ofMRi r continues for i = 1 ,...m iterations and terminates as soon as there are no more regions to delete. This is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S50",
      "paper_id": "arxiv:1302.3900v1",
      "section": "method",
      "text": "the case, as soon as MRi r = MRi\u22121 r such that \u2203m\u22651 |\u2200r\u2208Rcolor : MRi r = MRi\u22121 r . 4 Experimental Results The proposed algorithm is designed to be parameter- less and thus applicable to di\ufb00erent types of images without having to adjust parameter values by hand. The quality of the resulting segmentation only de- pends on the size and resolution of the input image. In this section we discuss the quality measure for the comparison of di\ufb00erent segmentation algorithms and we show key features, such as the amount of depth of \ufb01eld, that a\ufb00ects di\ufb03culties in segmentation. Fur- ther more, we demonstrate that images with higher resolution generally lead to be better segmentation",
      "page_hint": null,
      "token_count": 116,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S51",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "loose accuracy with growing size of the processed im- age. In 4.3 we describe the internal parameters and threshold values that we determined during our de- velopment and testing phases and show their impact on the quality of the segmentation result and the per- formance. 4.1 Quality measure To determine the quality of a segmentation maskI we use thespatial distortion d\u2032(I,Ir) as proposed in 10 [7]: d\u2032(I,Ir) = \u2211 (x,y) I(x,y) \u2297Ir(x,y) \u2211 (x,y) Ir(x,y) , where \u2297is the binaryXOR operation and Ir is the manually generated reference mask that represents the ground truth. The spatial distortion denotes the occurred errors, false negatives and false positives, in relation to the size of the reference maskIr, which is equivalent to the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S52",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "sum of all true positives and false negatives. Notice that d\u2032 can grow larger than1 if more pixels are misclassi\ufb01ed than the number of fore- ground pixels in total. Let \u00ef\u00bf\u0153 be a blank mask so that \u00ef\u00bf\u0153(x,y)=0 for each pixel p(x,y). The num- ber of false negatives is now equal to the foreground mask pixel inI because no pixel in \u00ef\u00bf\u0153 is de\ufb01ned as forground. For each image I, the spatial distortion d(\u00ef\u00bf\u0153, I) = 1 . Thus we limitd\u2032to d\u2208[0,1], so that d(I,Ir) = min{1,d\u2032(I,Ir)}. 4.2 Dataset All experiments were conducted on a diverse dataset of 65 images downloaded from Flickr and created by our own. The images are from di\ufb00erent categories with strong variations in the amount",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S53",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "of depth of \ufb01eld, as well as in the fuzziness of the background. Also the selection of the images does not focus on cer- tain sceneries, topics or coloring schemes in order to avoidover\ufb01ttingtocertaintypesofimages. Inourex- periments, we compare the spatial distortion of the proposed algorithm with re-implementations of the works presented in [7], [11] and [18]. The parameters for all algorithms were optimized to achieve the best average spatial distortion over the complete test set. 4.3 Comparison A major contribution of this algorithm is that none of the parameters introduced in the previous section needs tobehand tunedfor animage asall parameters are either independent of the image or determined fully automatically. An overview of the implicit pa- rameters and their default",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S54",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "values can be seen in Ta- ble 1. Table 1: Parameters used in the algorithm. Parameter Value Description \u0398score 50 Score \u00b5 threshold ofIscore \u0398\u03f5 1 40 Spatial radius of DBSCAN \u0398\u03c3 1 10 Gaussian blur radius \u0398dist 25 Color similarity distance \u0398rec 1 3 Relative size of reconstruction \u0398rel 2 3 Minimum value of relevant regions To calculate the score imageIscore we use a Gaus- sian blur with standard deviation of\u0398\u03c3 = 1 10 . Iscore is then scaled to \ufb01t in 400 \u00d7400 pixel to improve the processing speed of the subsequent steps with- out major impact to accuracy. We set\u0398score = 50 so that a score\u00b5 must exceed 50 to be processed by thedensity-basedclusteringalgorithmDBSCANthat uses a neighborhood",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S55",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "distance\u03b5= \u0398\u03f5 \u221a |I|, where|I| is the total pixel count of imageI and minPts is cal- culated in dependence of\u03b5, as described in Sec. 3.2.2. To smooth the approximate mask we use the mor- phological operation reconstruction by dilation\u03b3rec with a structuring elementH of size \u221a |Iapprox|\u00b7\u0398rec where \u0398rec = 1 3 . Further we set the maximum dis- tance threshold of similar color values in theL\u2217a\u2217b\u2217 color space to \u0398dist = 25 . The re\ufb01nement of the approximate mask removes all regions with a mask relevance value less than\u0398rel = 2 3 . Fig. 6 compares the performance of the reference algorithms with our proposed method. It can be seen that even though the computation time of the pro-",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S56",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "posed algorithm is greater than two of the three ref- erence algorithms, it outperforms the reference algo- rithms in terms of spatial distortion in all cases. Also, our algorithm has an average spatial distortion error of 0.21 over the complete test set which is less than half compared to the best competing algorithm with anaverageof 0.51 forthemorphologicalsegmentation with region merging [7]. Our algorithm also provides the lowest minimum error of0.01 in contrast to0.02 for the fuzzy segmentation algorithm [18]. It should also be noted that in contrast to the ref- erence algorithms, the proposed algorithm shows im- proved accuracy with larger images whereas the com- petitors loose accuracy with growing size of the im- 11 Table 2: Spatial distortion and",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S57",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "run time of the proposed algorithm compared to the reference algo- rithms. Proposed [7] [18] [11] Minimum 0.01 0.05 0.02 0.07 Median 0.10 0.48 0.93 1 Average 0.21 0.51 0.84 0.81 Std.Dev. 0.21 0.26 0.25 0.31 Time 28s 9s 54.2s 2.7s 0,90 1,00 07 0 0,80 , 05 0 0,60 0,70 ror 0,40 0,50 Err 0,20 0,30 0,00 0,10 1 1 12 13 14 15 16 1 Image No. Fig. 5: Spatial distortion error values of the segmen- tation with our proposed algorithm for each of the 65 dataset images. age. The minimum, median, average and standard deviation of the spatial distortion error are listed in Tab. 2. In Fig. 5 we illustrate all spatial distortion error values for each",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S58",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "segmented image in the dataset. 4.4 Image Types For each image we can de\ufb01ne key features as in table 3. All these features a\ufb00ect the di\ufb03culty for an a accu- Table 3: Key features of an Image amount of DOF defocused regions color small homogeneous plain high complex variant rate segmentation. Images with smaller DOF, homo- geneous defocused regions and variant colors tend to produce much better segmentation results than im- ages with high DOF, complex defocused regions and plain colors. The \ufb01rst type of images is commonly used by most of the competitors\u2019 publications to en- sure a high segmentation quality of the presented al- gorithm. Figure 7 shows the minor segmentation dif- ferences of a small DOF image.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S59",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "A much more chal- lenging task is the segmentation of images with com- plex background as shown in Fig. 8. Thereby our pro- posed algorithm achieves a good segmentation result with a spatial distortion of0.21 (c.f. Fig. 8b), where the other segmentation algorithms [18], [7] and [11] fail with spatial distortion values of each larger than 0.69 (c.f. Fig. 8c, 8d and 8e). 4.5 Size of input image One of the most in\ufb02uential variables on segmentation quality is the resolution of the input image. A com- paratively high resolution is needed for a proper seg- mentation, if for example an image has just a slightly defocused background and thus shows signi\ufb01cant tex- ture. Thus we designed the algorithm to be",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S60",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "able to handle a large scope of resolutions properly with- out loss of quality. By using the image of Fig. 8a as input, a spatial distortion value of 0.65 can al- ready be achieved at the relatively small resolution of 200 \u00d7300 pixels. For this particular image, a res- olution of 240 \u00d7350 is needed to lower the spatial distortion to0.42. Fig. 9 shows the diversi\ufb01cation of average and standard deviation of the spatial distor- tion depending on the increase of image size. Because the orientation (portrait or landscape) and aspect ra- tio (2:3, 4:3, etc.) of the images in our test set varies, we rescale each image so that its longest side equals the value of the resizing operation.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S61",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "In the following we denote the term image size by the longer side of the image. As we increase the input image size from 100 pixels to 200 pixels, we can lower the average spatial distor- tion from 0.74 to 0.5 and the standard deviation of the spatial distortion from0.84 to 0.43. As the size of the images reach 600 pixels, the average and standard deviation of the spatial distortion are0.36 and 0.25, 12 (a) Original image. (b) Proposed algorithm. (c) Result of fuzzy seg- mentation [18] (d) Result of morpholog- ical segmentation [7] (e) Result of video seg- mentation [11] Fig. 6: Comparing the results of the di\ufb00erent algorithms, where the input image has complex defocused regions and small",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S62",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "DOF. (a) Original with ho- mogeneous defocused re- gions. (b) Proposed algorithm. (c) Result of fuzzy seg- mentation [18] (d) Result of morpholog- ical segmentation [7] (e) Result of video seg- mentation [11] Fig. 7: Comparing the results of the di\ufb00erent algorithms, where the input image has homogeneous defocused regions, small DOF and variant colors and thus represents an easy task for all algorithms. (a) (b) (c) (d) (e) Fig. 8: Results of di\ufb00erent segmentation algorithms applied to an image with complex background (Fig. .8a). Thespatialdistortionsoftheappliedalgorithmsare 0.21 forourproposedalgorithm(Fig.8b), 1.0 byapplying [18] (Fig. 8c),0.69 by applying [7] (Fig. 8d) and1.0 by applying [11] (Fig. 8e). 13 120000,9 Error Average Error Std. Dev. Runtime Average 100000,7 0,8 , 6000 8000 0,5 0,6",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S63",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": ", e (ms) or 4000 6000 0,3 0,4 , Runtime Erro 20000,1 0,2 , R 00 , Image Size (longest side in px) Fig. 9: Impact of the size of the input image to the er- ror rate of the segmentation measured by the spatial distortion and the average runtime per image. respectively. At the same time, the runtime increases from an average of 686 milliseconds per image at 100 pixels, to 4155 milliseconds at 300 pixels. In our ex- periments we could also show a signi\ufb01cant decrease of the average spatial distortion and the correspond- ing standard deviation up to an image size of about 600 pixels. For images larger than800 pixels, the av- erage spatial distortion and standard",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S64",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "deviation still improve, yet at a signi\ufb01cantly slower rate than in the case of smaller images. In Fig. 9 we summarize the",
      "page_hint": null,
      "token_count": 22,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S65",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "Iscore was scaled to \ufb01t in400\u00d7400 pixels in the score clustering stage, the exponential runtime of the al- gorithm stagnates after reaching image sizes\u2265400 pixels for its longest side. 4.6 Sample Segmentations Fig. 10 presents some segmentation results of di\ufb00er- ent color images. The input image is shown in the \ufb01rst column. In the second column our segmentation result is depicted. The morphological segmentation [7] is placed in the third column, fuzzy segmentation [18] in the fourth column and the video segmentation [11] in the last column. Notice, that in one case the resulting mask covers the entire image (as seen in the second row of Fig. 10 in the third column). 5 Parameter Settings In this section, we describe",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S66",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "the internal parameter settings and threshold values. The choice of the pa- rameter values is the result of an extensive evaluation and represents a recommended set of values that pro- duced the best and stable results. Thus, none of these values has to be set by the user. For each parameter we discuss the impact on the average and standard deviation of the spatial distortion error and on the performance by evaluating the segmentation on our data set. 5.1 Blur Radius In the \ufb01rst stage, called deviation scoring, the pa- rameter \u0398\u03c3 determines the size of the radius of the Gaussian convolution kernel to remove noise from the input image. The same kernel is used to re-blur the de-noisedimagetocomputethedeviationofthemean neighbor",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S67",
      "paper_id": "arxiv:1302.3900v1",
      "section": "results",
      "text": "di\ufb00erence of each pixel in the de-noised and re-blurred Image. Fig. 11 shows the distribution of the spatial distortion on the primary y-axis with the mean execution time of the algorithm in milliseconds being displayed on the secondary y-axis. If\u0398\u03c3 is set too low, too few noise is removed from the input im- age. In addition, edges of the OOI in the re-blurred image have a reduced occurrence and thus can be de- termined less e\ufb00ective in contrast to the edges of the",
      "page_hint": null,
      "token_count": 83,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S68",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "blur radius of\u0398\u03c3 = 0.775 results in a high average spatial distortion d >0.8 compared to results of a segmentation with a more convenient choice of\u0398\u03c3. We assign \u0398\u03c3 = 0 .9 as the best trade-o\ufb00 between runtime (approximately 8s per image) and an aver- age spatial distortion ofdavg = 0.26. Larger values of \u0398\u03c3 cause a very intense smoothing operation, so that the edges of the OOI loose their signi\ufb01cance. 5.2 Score Clustering Threshold The score clustering threshold \u0398score describes the minimum score value\u00b5, that each pixel has to exceed in order to be processed by the DBSCAN clustering. This parameter ensures that the clustering algorithm does not have to handle a too large number of pixels 14 Fig.",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S69",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "10: Segmentation results of di\ufb00erent color images (\ufb01rst column) by applying our proposed algorithm (second column), [7] (third column), [18] (fourth column) and [11] (\ufb01fth column). 15 8000 10000 12000 14000 16000 18000 0,6 0,8 1 1,2 untime (ms) Error Error Average Error Std. Dev. Runtime Average 0 2000 4000 6000 0 0,2 0,4 0,775 0,8 0,825 0,85 0,875 0,9 0,925 0,95 0,975 Blur Radius Ru Fig. 11: Impact of the blur radius\u0398\u03c3 on the spatial distortion error and average runtime per image. with a score value which is not signi\ufb01cant. If\u0398score is set too low, DBSCAN will consume more time with- out improving the segmentation quality signi\ufb01cantly. In cases of\u0398score < 25 the spatial distortion grows even higher because",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S70",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "too many pixels with insigni\ufb01- cant score values are considered. Otherwise if\u0398score is set too high, too many pix- els with a potentially essential score value are not processed and the remaining clusters contain too few points. This results in a smaller mask that does not cover the whole focus area. As can be seen in Fig. 12, \u0398score = 50 can be de\ufb01ned as an optimal trade-o\ufb00 between spatial distortion and runtime. 5.3 Neighborhood distance The neighborhood distance\u0398\u03b5 directly in\ufb02uences the size of the\u03b5parameter of DBSCAN, as\u03b5= \u221a |I|\u00b7\u0398\u03b5. A higher value of\u0398\u03b5 increases the spatial radius\u03b5so that a core point has a larger reachability distance. If minPts, DBSCAN\u2019s second parameter, remains unchanged, an increase in\u0398\u03b5 would result in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S71",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "a de- creasing number of larger clusters. AsminPts is de- \ufb01ned relatively to\u03b5(see Sec. 3.2.2) an increase of\u0398\u03b5 would also enlargeminPts and vice versa. Fig. 13 illustrates the di\ufb00erent clustering results when chang- ing this parameter. If\u0398\u03b5 is too low, the main focus area is split into many di\ufb00erent, mostly small clus- ters. Thus, important information inIscore would be 140000,45 Error Average Error Std. Dev. Runtime Average 12000 0,35 0,4 , 8000 10000 02 5 0,3 0,35 e (ms) or 6000 8000 01 5 0,2 0,25 untime Erro 2000 4000 00 5 0,1 0,15 Ru 0 2000 0 0,05 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 20 30 40",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S72",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "50 60 70 80 90 100 110 120 130 140 150 Score Clustering Threshold Fig. 12: Impact of the score clustering threshold \u0398score on the spatial distortion error and the aver- age runtime per image. interpreted as noise, as shown in Fig. 13b. As you can see in 13c,\u0398\u03b5 = 0.025 produced a much more reliable clustering result and covers the main fo- cus region by not including surrounding noise. If\u0398\u03b5 is set too high, too much noise surrounding the OOI is merged with the main cluster (see 13d). The in- \ufb02uence of\u0398\u03b5 on the segmentation result is shown in \ufb01gure 14. As optimum distance we set\u0398\u03b5 = 0.025, where the average and standard deviation of the spa- tial distortion",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S73",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "are 0.26 and 0.18, and the average runtime<9000 milliseconds per image is still accept- able. 5.4 Size of the structuring element The size of the structuring elementH is used in our mask approximation stage after theconvex hull link- ing step by morphological \ufb01lter operations, in order to smooth the current mask. IfH is too small, little gaps remain and prevent the followingreconstruction by dilationoperation \u03b3rec from \ufb01lling holes in the ap- proximate mask as only dark regions that are com- pletely surrounded by the white mask are treated as holes. Asthesizeof Hincreases, moregapsareclosed and the contour of the OOIs approximate mask gets more fuzzy. The operationreconstruction by dilation \u03b3rec uses a structuring elementH\u2032. The size of the structuring element is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S74",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "calculated by \u221a |I|\u00b7 \u0398rec. In 16 (a) Iscore from Lena image (b) Clustering result with \u0398\u03b5 = 0.005 (c) Clustering result with \u0398\u03b5 = 0.025 . (d) Clustering result with \u0398\u03b5 = 0.045 Fig. 13: Impact of the \u0398\u03b5 parameter on score clustering. For better visual distinction, each cluster is represented in a random color and surrounded by its convex hull. (a) (b) (c) (d) Fig. 15: Impact of the parameter\u0398rec on smoothing and \ufb01lling the approximate mask of the input image (Fig. 15a). Fig. 15b-15d show the approximate masks afterreconstruction by dilationwith small (\u0398rec = 0.1), medium (\u0398rec = 0.25) and large (\u0398rec = 0.6) structuring elements. 17 140000,9 Error Average Error Std. Dev. Runtime Average 12000 0,7",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S75",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "0,8 , 8000 10000 0,5 0,6 , e (ms) or 4000 6000 0,3 0,4 Runtim Err 2000 4000 0,1 0,2 R 00 Neighbourhood distance Fig. 14: Impact of the neighborhood distance\u0398\u03b5 on the spatial distortion error and the average runtime per image. summary it can be said that smaller sizes ofH\u2032only cause the \ufb01lling of small holes in the mask, while larger sizes ofH\u2032\ufb01ll larger holes also. Fig. 15 shows the operation \u03b3rec with small, medium and large structuring elements. For \u0398rec \u2208 [1 5 ,1 2 ] the overall segmentation result of the complete data set is not highly in\ufb02uenced by\u0398rec as shown in Fig. 16. Values outside of that range produce a higher error rate. As \u0398rec approaches1,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S76",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "the average runtime also increases rapidly from around \u224810 seconds at \u0398rec = 1 2 to >40 seconds at\u0398rec = 9 10 . 5.5 Color similarity distance The color similarity parameter \u0398dist describes the distance \u2206E\u2217(u,v) that two colorsu,v of theL\u2217a\u2217b\u2217 color space may not exceed in order to be considered as similar. Thus, the amount of\u0398dist has direct im- pact on the amount of color regions which are ex- tracted from the original images. If \u0398dist is set to a very low value, the resulting color regions are very small. This causes large relativemask relevance val- uesincaseofsmallcolorregionswhichareoverlapped byasmallareaoftheapproximationmask. Andthus, less regions are removed in the followingregion scor- ing stage. Too large values for\u0398dist imply that too many regions are",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S77",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "merged together and thus there- gion scoring will become too vague. Fig. 17 shows 450001,2 Error Average Error Std. Dev. Runtime Average 35000 40000 1 , 25000 30000 35000 0,8 (ms) r 20000 25000 0,6 untime Error 10000 15000 02 0,4 Ru 0 5000 0 0,2 0 0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,8 0,9 Structuring Element Size Fig. 16: Impact of the neighborhood distance\u0398rec to the spatial distortion error and the average runtime per image. the di\ufb00erent results of the disjoint color regions by altering the parameter \u0398dist. Fig. 17a is the pixel subset of the original image marked by the smoothed mask which is returned by themask approximation stage (Sec. 3.3) of the algorithm. If \u0398dist is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S78",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "set to a small value as in Fig. 17b, a large amount of dis- joint color regions is created compared to Fig. 17c where \u0398dist = 25 . Very few color regions are con- structed if a large value like\u0398dist = 40 is applied to the color segmentation stage. This is illustrated in Fig. 17d. Usually, this makes theregion scoring stage (Sec. 3.5) more vague. The reason for this is that in the case of a large\u0398dist it is more likely that n> 1 regions r1,...,r n with high score variation min{MRr1 ,...,MR rn}\u226a max{MRr1 ,...,MR rn} are merged together in one larger regionr = r1 \u222a ... \u222arn, where MRri, i= 1 ...n denotes the mask relevance introduced in",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S79",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "Sec. 3.5. The deletion of r would then generate more false negatives, which means that the resulting \ufb01nal mask does not cover the entire OOI. The inclusion of r would generate more false positives, so that more background is \ufb01- nally included. As illustrated in Fig. 18, an increas- ing value of\u0398dist also causes an increased processing time. For our tests, we chose\u0398dist = 25. 18 (a) (b) (c) (d) Fig. 17: Impact of the parameter\u0398dist on grouping the smoothed approximation mask into regions of similar colors of the input image (Fig. 17a). Fig. 17b-Fig. 17d show the approximate mask with small (\u0398dist = 5), medium (\u0398dist = 25) and large (\u0398dist=40) values. For better visual distinction, each region is",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S80",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "represented in a random color. 1400000,4 Error Average Error Std. Dev. Runtime Average 120000 03 0,35 , 80000 100000 0,25 0,3 (ms) r 60000 80000 01 5 0,2 untime Error 20000 400000,1 0,15 Ru 0 20000 0 0,05 0 1 02 03 04 05 06 07 08 09 0 Color Similarity Distance Fig. 18: Impact of the color similarity distance\u0398dist on the spatial distortion error and the average run- time per image. 104000,35 Error Average Error Std. Dev. Runtime Average 102000,3 , 9800 10000 0,2 0,25 e (ms) or 9400 9600 01 0,15 Runtime Erro 9200 9400 0,05 0,1 R 90000 625 650 675 700 725 750 775 800 825 850 875 900 925 950 975 0,6 0,6 0,6 0,7",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S81",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "0,7 0,7 0,7 0,8 0,8 0,8 0,8 0,9 0,9 0,9 0,9 Region Scoring Relevance Fig. 19: Impact of the region scoring parameter\u0398rel on the spatial distortion error and the average run- time per image. 5.6 Region Scoring Relevance In the region scoring stage we delete each regionr from the smoothed approximate maskIapp at thei-th iteration ifMRi r \u2264\u0398rel. Fig. 19 shows the impact of \u0398rel on the segmentation quality. Low values of\u0398rel lead to a lower number of deleted regions, leaving the \ufb01nal mask of the OOI surrounded with a thin border in most cases. Thus the total amount of false positives is increased as well as the spatial distortion. The overall in\ufb02uence of\u0398rel is rather low because it only",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S82",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "a\ufb00ects the re\ufb01nement stepRegion Scoring (see Sec. 3.5). 19 6 Impact of DOF on Similarity Measuring the similarity between two images is an essential step for content-based image retrieval [17] (CBIR). If the image database contains a signi\ufb01cant amount of low DOF images, a DOF-based segmen- tation algorithm can improve the classi\ufb01cation ac- curacy because the extraction of features can be re- stricted to the subset of pixels contained in the OOI of the images. Given for example two imagesI1 and I2 which are containing two semantically di\ufb00erent objectsO1 and O2 in their particularly focussed area in front of a comparatively similar background. In this case, the distance between imageI1 and I2 should be signi\ufb01- cantly lower than between the",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S83",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "extracted ObjectsO1 and O2, such thatd(O1,O2) \u226bd(I1,I2). Fig. 20 shows two sample images that practically display the same scene with a di\ufb00erent distance of the lens to the focal plane so that di\ufb00erent OOIs are displayed sharply. As our main focus does not lie in the evaluation of best possible feature descriptors or distance measures, we use the well known color histograms for the classi\ufb01cation of the data set. For each image we thus create a color histogramhwith 12 bins. Between the color histogramsh(I1) and h(I2) we can now measure the Minkowski-form Distance d2. For two histogramsQ and T, dp is de\ufb01ned as in Eq. 6 which corresponds to the Euclidean distance in case ofp= 2. dp(Q,T) /acute.ts1= (N\u22121\u2211 i=0",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S84",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "(Qi \u2212Ti)p )1 p (6) The distance between the color histograms of the complete images d2 (h(I1) ,h (I2)) = 334 is con- siderably lower (3.8 times) than the distance be- tween the histograms of their extracted OOIs d2 (h(O1) ,h (O2)) = 1277 (see Fig. 20c and Fig. 20d). This leads to the assumption, that a CBIR system could pro\ufb01t from an automatic segmentation ofOOIsinlowDOFimagestoimprovesearchquality. For a brief veri\ufb01cation of this hypothesis, we cre- ated a database containing 114 diverse DOF images divided into 17 classes: bird, bee, cat, coke, deer, eagle, airplane, car, fox, apple, ladybird, lion, milk, redtulp, yellowtulp and sun\ufb02ower. Let Ii,j be thej-th image of thei-th classGi. We then de\ufb01ne the inner-class distance of an",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S85",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "imageIi,j to be the distance of the imageIij to all other images Iik, k\u0338= j of the same classGi distinner(Ii,j) = 1 |Gi|\u22121 \u00b7 \u2211 J\u2208Gi/Ii,j d(h(J) ,h (Ii,j)) , where dis the distance, which is set to the Euclidean Distance d2 in our case. For a given distance mea- sure d, the average inter-class distancedistinter of an image Iij is the average distance to all other images J /\u2208Gi distinter(Iij) = 1 |J \u2208Gj\u0338=i|\u00b7 \u2211 J\u2208Gj\u0338=i d(h(J) ,h (Ii,j)) . In case of a classi\ufb01cation task, it is required, that the average inner-class distance is smaller than the inter- class distance of an image. To further improve the classi\ufb01cation, it is thus desirable to increase the dif- ference between",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S86",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "inter-class and inner-class distance. In the following experiment, we measured the inner- and inter-class distance for all classes with- out any segmentation. Afterwards we applied the proposed segmentation algorithm to the images, re- peated the experiment and computed the relative changes of the inner- and inter-class distances. In Fig. 21 we summarize the result of the experi- ment. It can be seen that the inner-class distance is decreased to an average of67% of its original value, which is signi\ufb01cantly smaller than the decrease of the inter-class distance which decreases to an aver- age of81% of its original value. Thus, the di\ufb00erence between inter-class and inner-class distance was im- proved by an average of14% throughout the data set with a maximum",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S87",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "of27% in the case of the fox class and a minimum of3% in the case of the (glass of) milk class. These di\ufb00erences between the inner-class distances and the inter-class distances are illustrated in Fig. 22. So it can be said that an CBIR task can pro\ufb01t from the DOF segmentation if the data set con- tains enough low DOF images. 20 (a) (b) (c) (d) Fig. 20: Comparing the results of the di\ufb00erent algorithms, where the comparatively similar input imagesI1, I2 (Fig. 20a/20b) have homogeneous defocused regions, small DOF and variant colors. The segmentation results O1 and O2 (Fig. 20c / 20d) show rather few similarity. 100%o inner-class inter-class 90% 100%e Ratio 70% 80% Distanc 50% 60% mage D",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S88",
      "paper_id": "arxiv:1302.3900v1",
      "section": "background",
      "text": "30% 40% plete Im 20% Comp Fig. 21: Impact of the DOF segmentation on the inner- and inter-class distance of a test dataset. 10% 15% 20% 25% 30% er-Class Decrease Ratio 0% 5% Inner-Inte Fig. 22: Amount of percent points that the inner- class distance was lowered more than the inter-class distance.",
      "page_hint": null,
      "token_count": 52,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S89",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "In this paper a new robust algorithm for the segmen- tation of low DOF images is proposed which does not need to set any parameters by hand as all neces- sary parameters are determined fully automatically or preset. Experiments are conducted on diverse sets of real world low depth of \ufb01eld images from various categories and the algorithm is compared to three ref- erence algorithms. The experiments show that the al- gorithm is more robust than the reference algorithms on all tested images and that it performs well even if the DOF is growing larger so that the background begins to show considerable texture. We also demon- strated the positive impact of low DOF segmentation to image similarity in case",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S90",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "of CBIR. In our future work, we plan to improve processing speed and accu- racy of the algorithm. Furthermore, we plan to apply the algorithm to movies and to apply an automatic detection, whether an image is low DOF or not. A Java WebStart demo of the algorithm can be tested online2. We also plan to publish the test data as far as image licensing allows to do so as well as the set including the reference masks for the ROIs. The im- plementation of thereference algorithms will be made available as ImageJ [1] plugins for download also at the demo URL. We also plan to publish the proposed algorithm as an ImageJ plug-in in the near future. 2http://www.dbs.i\ufb01.lmu.de/research/IJCV- ImageSegmentation/",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S91",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "21 Acknowledgements ThisresearchhasbeensupportedinpartbytheTHE- SEUS program in the CTC and Medico projects. They are funded by the German Federal Ministry of Economics and Technology under the grant number 01MQ 07020. The responsibility for this publication lies with the authors. We would like to thank Philipp Grossel\ufb01nger and Sirithana Tiranardvanich for their ImageJ plug-in implementations of the fuzzy segmentation [18] and video segmentation [11]. References [1] Abramo\ufb00, M., Magelhaes, P., Ram, S.: Image processing with ImageJ. Biophotonics interna- tional 11(7), 36\u201342 (2004) [2] Canny, J.: A computational approach to edge detection. Readings in computer vision: issues, problems, principles, and paradigms184 (1987) [3] Chung, R., Shimamura, Y.: Quantitative analy- sis of pictorial color image di\ufb00erence. In: TAGA, pp. 333\u2013345. TAGA; 1998 (2001) [4]",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S92",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "Ester, M., Kriegel, H., Sander, J., Xu, X.: A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proc. KDD, vol. 96, pp. 226\u2013231 (1996) [5] Hartigan, J.A.: Clustering Algorithms (1975) [6] Kavitha, S., Roomi, S., Ramaraj, N.: Lossy compression through segmentation on low depth-of-\ufb01eld images. Digital Signal Processing 19(1), 59\u201365 (2009) [7] Kim, C.: Segmenting a low-depth-of-\ufb01eld image using morphological \ufb01lters and region merging. IEEE Transactions on Image Processing14(10), 1503\u20131511 (2005) [8] Kim, C., Hwang, J.: Video object extraction for object-oriented applications. The Journal of VLSI Signal Processing29(1), 7\u201321 (2001) [9] Kim, C., Park, J., Lee, J., Hwang, J.: Fast ex- traction of objects of interest from images with low depth of \ufb01eld. ETRI journal29(3), 353\u2013362",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S93",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "(2007) [10] Kov\u00e1cs, L., Szir\u00e1nyi, T.: Focus area extraction by blind deconvolution for de\ufb01ning regions of interest. Pattern Analysis and Machine Intel- ligence, IEEE Transactions on29(6), 1080\u20131085 (2007) [11] Li, H., Ngan, K.: Unsupervized video segmenta- tion with low depth of \ufb01eld. IEEE Transactions on Circuits and Systems for Video Technology 17(12), 1742\u20131751 (2007) [12] Li, J., Wang, J., Gray, R., Wiederhold, G.: Mul- tiresolution object-of-interest detection for im- ages with low depth of \ufb01eld. In: Image Analysis and Processing, 1999. Proc. International Con- ference on, pp. 32\u201337. IEEE (2002) [13] Park, J., Kim, C.: Extracting focused object from low depth-of-\ufb01eld image sequences. In: Proceedings of SPIE, vol. 6077, pp. 578\u2013585 (2006) [14] Tsai, D., Wang, H.: Segmenting focused objects",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S94",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "in complex visual images. Pattern Recognition Letters 19(10), 929\u2013940 (1998) [15] Won, C., Pyun, K., Gray, R.: Automatic object segmentation in images with low depth of \ufb01eld. In: Image Processing. 2002. Proceedings. 2002 International Conference on, vol. 3, pp. 805\u2013808. IEEE (2002) [16] Ye, Z., Lu, C.: Unsupervised multiscale focused objects detection using hidden Markov tree. In: Proceedings of the International Conference on Computer Vision, Pattern Recognition, and Im- age Processing (IEEE Press, 2002), pp. 812\u2013815 [17] Zhang, D., Lu, G.: Evaluation of similarity mea- surement for image retrieval. In: Neural Net- works and Signal Processing, 2003. Proceedings of the 2003 International Conference on, vol. 2, pp. 928\u2013931. IEEE (2004) 22 [18] Zhang, K., Lu, H., Wang, Z., Zhao, Q.,",
      "page_hint": null,
      "token_count": 120,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    },
    {
      "snippet_id": "Parxiv_1302_3900v1:S95",
      "paper_id": "arxiv:1302.3900v1",
      "section": "conclusion",
      "text": "Duan, M.: A Fuzzy Segmentation of Salient Region of Interest in Low Depth of Field Image. Advances in Multimedia Modeling pp. 782\u2013791 23",
      "page_hint": null,
      "token_count": 23,
      "paper_year": 2013,
      "paper_venue": "arXiv",
      "citation_count": 0,
      "extraction_quality_score": 0.932323644318534,
      "extraction_quality_band": "good",
      "extraction_source": "native"
    }
  ],
  "extraction_meta": {
    "extractor": "pypdf",
    "two_column_applied": false,
    "ocr_applied": false,
    "pages_total": 23,
    "empty_pages": 0,
    "empty_page_pct": 0.0,
    "page_stats": [
      {
        "page": 1,
        "chars": 3108,
        "empty": false
      },
      {
        "page": 2,
        "chars": 4471,
        "empty": false
      },
      {
        "page": 3,
        "chars": 3631,
        "empty": false
      },
      {
        "page": 4,
        "chars": 3985,
        "empty": false
      },
      {
        "page": 5,
        "chars": 937,
        "empty": false
      },
      {
        "page": 6,
        "chars": 1793,
        "empty": false
      },
      {
        "page": 7,
        "chars": 3369,
        "empty": false
      },
      {
        "page": 8,
        "chars": 3608,
        "empty": false
      },
      {
        "page": 9,
        "chars": 3496,
        "empty": false
      },
      {
        "page": 10,
        "chars": 3557,
        "empty": false
      },
      {
        "page": 11,
        "chars": 3912,
        "empty": false
      },
      {
        "page": 12,
        "chars": 3191,
        "empty": false
      },
      {
        "page": 13,
        "chars": 1008,
        "empty": false
      },
      {
        "page": 14,
        "chars": 3609,
        "empty": false
      },
      {
        "page": 15,
        "chars": 188,
        "empty": false
      },
      {
        "page": 16,
        "chars": 3346,
        "empty": false
      },
      {
        "page": 17,
        "chars": 605,
        "empty": false
      },
      {
        "page": 18,
        "chars": 3201,
        "empty": false
      },
      {
        "page": 19,
        "chars": 1689,
        "empty": false
      },
      {
        "page": 20,
        "chars": 3916,
        "empty": false
      },
      {
        "page": 21,
        "chars": 2089,
        "empty": false
      },
      {
        "page": 22,
        "chars": 3238,
        "empty": false
      },
      {
        "page": 23,
        "chars": 180,
        "empty": false
      }
    ],
    "quality_score": 0.9323,
    "quality_band": "good"
  }
}